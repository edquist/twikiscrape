---+ Metrics Meeting, January 9, 2007
---++ Attending
   * Phillipe, Chris, Ruth, Rob, Chander, John

---++ Background
   * Meeting to discuss high level milestone to define and agree to the Operations Metrics for the first year of OSG which is due now (1st Jan actually) and which comes from the Facility Coordinator. Need to document a list of metrics, how to measure them, and how we will determine success. Included a list of some metrics in the [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=514][Project Plan]] but did not include how we will determine success. How should we get there from here without more than a week or two delay?
   * Metrics document from Miron (Jan 17): %ATTACHURL%/OSG-Metrics-V1.doc
   * Sample metrics from Grid3: http://grid.uchicago.edu/metrics/
   * Extracted metrics from a talk from &lt;noop&gt;GridPP (J. Coles): %ATTACHURL%/gridpp16_metrics.ppt 
      * Normalized CPU time by VO
      * Normalized CPU time by site and VO
      * Running jobs by VO, vs time
      * % job slots used (CPU efficiency)
      * Number of certified sites publishing accounting records to GOC (function of time)
      * Number of available job slots, vs time
      * Number of enabled VOs  %GREEN% (related would be number of enabled VOs by site) %ENDCOLOR%
      * Ranked CE&#39;s according to total CPU-hours delivered, by VO, and by failure, for a given time interval (usually measured in months).
      * Efficiency = Successful time / total time, by site, for a time interval, ranked.
      * Total hours (success and failed) by Tier2.
      * Occupancy (total, and contributed to specific processing) for a given time slice, assuming 100% availability.
      * Cummulative storage by site versus time, with VO breakdown
      * Scheduled downtimes plotted by available times, versus time (quarterly).
      * %time a site ws down in a given month, plotted for each site, broken down by month.
      * Overall SFT success percentage vs time.
      * SFT success percentage vs site, for monthly intervals.
      * Number of trouble tickets per site, per quarter.
      * Average time (in hours) to close tickets by site, per quarter.
      * Plot of number of sites at a particular middware release vs time (showing deployment trends)
      * Disk-to-disk transfer rates (in/out) by site
 
---++ Mash of notes from Rob
   * Discussion of the &quot;happiness metric&quot; suggested in various places.  Chander had a definition, =Happiness = results/expectations=
   * One of the main issues is setting user expectations - Chris has been thinking about this in his role of OSG user support. Is there a reasonable set of expectations at present? And what are the expectations?
      *  Simply, I want to submit jobs and get the results back.
      * Can I run on your site?  (lots of time wasted on things like this.)
      * An example of a worker-node not able to write into $APP, without a unique RSL.
   * We discussed the limitation of advertized OSG site attributes (even when those are available).  Lots of additional site-specific information seems neccessary at the moment.  There seems to be a need for a free-format  information service that collects such information.
   * Need to survey of happiness ratio, but its probably early to do this.
   * Going forward, we need to determine where to set expecations, given the quality of the infrastructure and effort available.
   * Ruth: and site performance should not exclude metrics for data movement and storage.
   * Currently a framework is needed that would be based on: the OSG information service (whats available), Gratia probes and reporters (what happened).  
      * *What clearly is missing is error information.*
   * Note that Gratia also can collect VO probe information.
   * VO dashboards - collecting that information: CMS and ATLAS both have dashboards. 
  
Errors discussion:
   * Need of a Global job identifier - this has been under discussion for a long time - what is the status?
   * Globus errors, eg - would like to collect site/VO statistics
   * Phillip is concentrating on batch scheduler errors.
   * Each service should have its own probe.
   * There is a job model in Gratia; it knows the state it when completed.


Most important list of metrics:
   * How many CPUs are available?
      * Rather than CPUs, one should use *cores* as an available job slot.  This information must come from a standard OSG info service. 
   * Does Monalisa measure this already? Ans seems yes.
   * Note that we need to separate policy for the raw measurement.
   * A length-of-jobs profile is needed.
   * Condor free slots.  PBS free nodes.
   * Availability of processing and storage resources:
      * Availability is measured by the OSG site functional tests and validations.  
      * Definition of availability is GREEN in VORS.  By VO.
      * VO based validation tests will give VO specific availability measurements.

Aggregation of information:
   * We have disparate sources of potential metrics data: Gratia, Monalisa, VO supplied data, VORS.
   * There are tools that work above databases.
   * Storage: availability from CEMon (need to wait for glue schema 1.3; there is a final draft; Lawrence will be working on a probe.) .  
   * Usage: there will be a sensor for dCache I/O, and an API for this. 


Use of processing and storage resources:
   * Resource usage is measured by the OSG accounting infrastructure and measures parameters such as wall clock time on a CPU, MBytes  moved to and from storage resources, MBytes of storage used. OSG resource usage is measured for accesses through grid interfaces; whether that access results in use of a local or remote resource.

System throughput:
   * Number of jobs submitted, number of jobs executed, number of files accessed,  and amount of data moved per day per user group.

Measures of shared use:
   * Fraction of jobs executed and data stored on sites that are not owned; or provide assured access to the group submitting the jobs.

System latencies:
   * The time a job spends in the wait queue, the wait  time for a request for transfer from  storage or between resources per day per user group.


Efficiency and Effectiveness:
   * Fraction of available CPU cycles that are not utilized. Error and rertry statistics for grid use. These statistics must be carefully gathered and understood to allow separation of errors due to the OSG and/or site infrastructures and the user middleware and applications.


---++ Notes from Philippe
Metrics:

Should we have a user survey?

Philippe: Yes, in particular to avoid having a big (unknown)
difference between the user perception and the providers 
perceptions.

Chris: The &#39;satisfaction&#39; of the user seems to be strongly
correctly to initial impression (aka how hard it was to get
the user job to actually run on the site).

Rob: One issue is un-inform expectation / misunderstanding
of the nature of the OSG.  Or even expectations we can not
possibly meet.

Chris/Rob: So one of the issue is a lack of a good information
system in OSG.

Rob: The gap in expectation is one important reason why I am
concerned about the touchy-feely type of metrics

Philippe: Indeed we need to use the &#39;user survey&#39; only as one
of the many metrics (and possibly it would not be upfront in
the presentation).

Chander: We need to have a clarification of what the expectation
should be.

All: There is no way (expect trying to submit a job) to know the
&#39;feature&#39; available or not on the site.

Rob: I am hoping that this time around, the OSG will have a real
Information Service (as opposed to the current mesh of tools).
We will have the GIP, (BDII and CEMON) (following the GLUE schema).
We have some VO that do not want to use the OSG information service
and will keep the information they need to match job in their own
database.  Matching all this different type of information is hard.

Rob: It might be usefull to have some set of example application
than can be used to check what can or can not be done.

All: We all agree that before we can do a survey we need to have
gathered the other technical metrics so we can constract them with
the survey the result.

Ruth: When talking about number of jobs running we also must talk
about the amount of data being transferred.

Philippe: Eventually we need to account for the usage of all OSG
service (including Portal and Globus).

Ruth: Can we have metric on the number of Cores available to the OSG.

Rob: Maybe it is the same as the number of job slot visible to the
OSG (or how many concurrent job can the best match VO run as the same
time on the site assuming nothing else is running).

Ruth: I thinking Monalisa is measuring exactly that.






-- Main.RobGardner - 11 Jan 2007


&lt;!--
These are general in nature and should be changed in the this template if the location changes:
   * Set CURRENT_TOMCAT = ==gratia09==
   * Set CURRENT_MYSQL = __gratia06__
   * Set NEW_TOMCAT = ==gratia07==
   * Set NEW_MYSQL = __ gratia07__
   * Set TOMCAT_ALIAS = ==gratia.opensciencegrid.org==
   * Set MYSQL_ALIAS = ==gratia-db01.fnal.gov==
   * Set RELEASE_TAR_REPOSITORY = /afs/fnal.gov/files/expwww/gratia/html/Files

These are specific to a release and require changing:
   * Set RELEASE_DATE = July 15, 2011
   * Set CURRENT_RELEASE = v1.08.0
   * Set RELEASE_TAG = v1-08-0
   * Set NEXT_DEVEL_VERSION = v1.09

   * Set BUILD_DIRECTORY = /home/weigand/cdcvs/gratia-%CURRENT_RELEASE%

--&gt;
---+!! Gratia Release %CURRENT_RELEASE% (%RELEASE_DATE%)%BR% 


%TOC%

%STARTINCLUDE%
---+ Overview

The main purpose of this release (%CURRENT_RELEASE%) is to enhance the database schema and to add a record of the whole Gratia ecosystem&#39;s health.

---++Main Features:

   * Introduced support for monitoring and display any potential backlog of any of the Gratia components (See [[GratiaReleaseV1dot08#Probe_Improvements][probe details]] and  [[GratiaReleaseV1dot08#Collector_Improvements][collector details]])
   * Increased the range of the dbid for the raw data (from 32 bits to 64 bits integer), virtually removing any size limitation (beside the limitation of the database engine or hardware).
   * Completed revamping of the gathering of information about the status and performance of the Collector (see [[GratiaReleaseV1dot08#Collector_Improvements][details]] below)

---++ Notices

   * First restart will be slow due to the significant database schema upgrade required.
   * The duplicate detection on SE, CE, and Subcluster was replaced at the request of HCC and OSG (due to the fact we really don&#39;t want duplicate detection here but &quot;transition detection&quot;).

---++ Report Improvements

   * Static reports upgrade to no longer produce non-truncated PDF.
   * Daily pre-generated reports are now PDF files.
   * Upgraded to BIRT 2.6.1

---++ Probe Improvements

   * Add new functionality to allow delegation of the lifetime of transient input files to Gratia so that there are deleted only in case the record is properly saved and backed-up.   This feature is now used in the Condor probe.
   * Add upload of information about the amount of data still needing to be processed or uploaded by a Probe to the Collector.
   * Add interface (GratiaCore::RegisterEstimatedServiceBacklog) to to let the probe tell the system how much work is left to do (Individual probes still need to be updated to take advantage of this feature).
   * Update GratiaCore to upload both the amount 
   * Allow delegation of lifetime of input file to the Gratia Core library for easier deletion only in case of full success ; this fixes a problem with file accumulation with the condor probe.
   * Checking on number of inodes and disk space used by the urCollector par of the PBS/LSF Probe (to prevent overflow)
   * Extra status reporting
   * Support for HTPC jobs
   * Support for CREAM (by supporting blahp.log to provide certificate information).
   * Repaired SGE probe.
   * Merged the GlideInWMS probe into the Condor probe.
   * Added support in the PBS probe for ‘array jobs’.
   * Added support for Torque stores old accounting logs in .gz files. 
   * Removed dependency on SQLAlchemy and setup tools.
   * Add support for ancient python version (2.3.4)
   * Several fixed for dcache probe.

---++ Collector Improvements
  
   * General improvement in the response time both of the administration page and of the enabling/disabling of the Collector features.
   * Housekeeping is now automatically paused if the number of record in one of the queue reach the value of &#39;max.housekeeping.nrecords&#39; in the configuration file.  The housekeeping service is restarted if the number of record in each queues goes back under &#39;min.housekeeping.nrecords&#39;.
   * Increased the range of the dbid for the raw data (from 32 bits to 64 bits integer)
      * The collector schema will be automatically updated the first time the collector is restarted, this will involve (essentially) a complete rewrite of all the data table and thus can be quire time consuming.
   * Improved XML error handling when parsing the incoming data (i.e. eliminate potential data loss in case of non standard record being uploaded)
   * New administration page listing the performance of Collector and containing: 
      * current uploading rate
      * current incoming rate
      * recovery estimate
      * recovery estimate including estimated backlog on the service side
      * housekeeping status and recovery rate
   * New administration pages detailing the current backlog content:
      * Backlog of any probe or collector sending information to the local Collector.
      * Historical information on those backlog
      * Details on which probes still has data files in the local queue
   * Improve information available in Status page, add collector-status.html page to get the state of the sub-system.  See [[https://twiki.grid.iu.edu/bin/view/Accounting/Monitoring][documentation page]] for details. 
   * Greatly improved response time and stability of the Status page.

---+++ Technical information

   * The file name used to stage the incoming data in the Collector&#39;s thread directory now encodes the name of the origin and the number of records (i.e. job#randomnumber#.#origin#.#numberOfRecords#.xml where
  the origin is the sender collector or probe with all : and / replaced with underscores.)
   * The way the Collector gathers and displays the statistics and performance information has been completely revamped.
      * The Status and !MonitorStatus administration pages have been updated to take advantage of the new tables and are now fast even under heavy load.
      * 4 tables now participates: !TableStatistics, !TableStatisticsSnapshots, !TableStatisticsHourly, !TableStatisticsDaily
         * !TableStatistics contains for each record type (JobUsage, Metric, etc.) the number of records that are currently in the database and the number of records that have been processed (even if they have been deleted since).   For each, the table not only the number of good record but also the number of duplicates and the number of ‘errorneous’ records (‘Qualifier’). The main new feature in this table is the addition of the ‘lifetime’ number in addition to the current numbers.
         * !TableStatisticsSnapshots contains a copy of the content of TableStatistics taken every five minutes (customizable) and kept for a day (default)
         * !TableStatisticsHourly contains a summary of the snapshots taken during a given hour.   For each entry (current/lifetime,RecordType,Qualifier), the table records the exact start and end time it covers, the minimum, maximum and average number of records during the period.  The default is to keep those hourly summary for one year.
         * !TableStatisticsDaily contains a summary of the snapshots taken during a given day.   For each entry (current/lifetime,RecordType,Qualifier), the table records the exact start and end time it covers, the minimum, maximum and average number of records during the period.  The default is to keep those daily summary indefinitely. 
    * A new class, !TableStatisticsManager, is introduced to manage the lifetime of the rows in the new statistics tables and to do/schedule the copying and summarizing.  2 new MYSQLl functions are also introduced for this purpose: table_statictics_hourly_summary and table_statictics_daily_summary.
   * The Collector now and displays the current and historical information about its backlog and the backlog of its providers.
      * The backlog details administration pages have been created to take advantage of the new tables
      * 4 tables now participates: !BacklogStatistics, !BacklogStatisticsSnapshots, !BacklogStatisticsHourly, !BacklogStatisticsDaily
         * !BacklogStatistics contains for each providers the number of records and xml files (and tar files) currently in their Gratia as well as an estimate of the amount of information about the service (for example number of batch jobs) the Probe still need to process.   In addition, the table records the bundle size used by the provider to upload its records and the maximum size allowed for its queue. 
         * !BacklogStatisticsSnapshots contains a copy of the content of BacklogStatistics taken every fifteen minutes (customizable) and kept for a day (default).
         * !BacklogStatisticsHourly contains a summary of the snapshots taken during a given hour.   For each entry (EntityType/Name), the table records the exact start and end time it covers, the minimum, maximum and average number of records during the period.  The default is to keep those hourly summary for one year.
         * !BacklogStatisticsDaily contains a summary of the snapshots taken during a given day.   For each entry (EntityType/Name)), the table records the exact start and end time it covers, the minimum, maximum and average number of records during the period.  The default is to keep those daily summary indefinitely. 
   * A new class, !BacklogtatisticsManager, is introduced to manage the lifetime of the rows in the new statistics tables and to do/schedule the copying and summarizing.  2 new MYSQLl functions are also introduced for this purpose: backlog_statictics_hourly_summary and backlog_statictics_daily_summary.
   * These 2 new set of features are customizable via the following configuration entry:
&lt;pre class=&quot;screen&quot;&gt;#
# Table Statistics history 
#
# frequency of the snapshots, 0 disable the history recording.
# In number of minutes between snapshots
#
tableStatistics.snapshots.wait = 5
#
# How long to keep the individual snap shots of the table statistics
#
service.lifetime.TableStatisticsSnapshots = 1 day
#
# How long to keep the hourly summary of the table statistics
#
service.lifetime.TableStatisticsHourly = 1 year
#
# How long to keep the daily summary of the table statistics
#
service.lifetime.TableStatisticsDaily = UNLIMITED
#
# input queue control - whether or not to monitor queue sizes.
#
#
# Backlog Statistics history 
#
# frequency of the snapshots, 0 disable the history recording.
# In number of minutes between snapshots
#
backlogStatistics.snapshots.wait = 15
#
# How long to keep the individual snap shots of the backlog statistics
#
service.lifetime.BacklogStatisticsSnapshots = 1 day
#
# How long to keep the hourly summary of the backlog statistics
#
service.lifetime.BacklogStatisticsHourly = 1 year
#
# How long to keep the daily summary of the backlog statistics
#
service.lifetime.BacklogStatisticsDaily = UNLIMITED
#
# input queue control - whether or not to monitor queue sizes.
#&lt;/pre&gt;


&lt;!--   -------------------------------------------- --&gt;
---+ Anticipated downtime
It is expected that this release will require the Gratia services and reporting to be unavailable beginning at:
   * Start: %RELEASE_DATE%  hh:mm CST
   * Available: %RELEASE_DATE% hh:mm CST

The changes affecting downtime  for this release are:
   1 Length of time to make a backup of the database to the backup area using =mysqlhotcopy=
   1 Installation and validation on the 6 Gratia schemas




&lt;!--   -------------------------------------------- --&gt;
---+ Collectors and Databases Affected

The following Gratia collectors and databases will be converted with this release:
%TABLE{ tableborder=&quot;1&quot; cellpadding=&quot;0&quot; cellspacing=&quot;1&quot; headerbg=&quot;#99CCCC&quot; databg=&quot;#FFFFCC, #FFFFFF&quot;}%
%EDITTABLE{  header=&quot;|*Schema*|*URL*|*Description*|&quot; format=&quot;| text, 20 | text, 20 | text, 35 | text, 15 | text, 5 | text, 15 |&quot;  changerows=&quot;on&quot; quietsave=&quot;on&quot; editbutton=&quot;Edit table&quot; }%
|*Schema*|*&lt;nop&gt;MySql port*|*Collector URL*|*Collector host*|*Size (bytes)*|*Size (rows)*|
| fermi_itb | %CURRENT_MYSQL%.fnal.gov:3320 | gratia-fermi.fnal.gov:8881 | gratia08.fnal.gov: |  868K |               1 |
| fermi_osg | %CURRENT_MYSQL%.fnal.gov:3320 | gratia-fermi.fnal.gov:8880 | gratia08.fnal.gov: |  5.0G |   2,554,484 |
| gratia | %CURRENT_MYSQL%.fnal.gov:3320 | gratia.opensciencegrid.org:8880 | gratia09.fnal.gov |  60.0G |  30,226,750 |
| gratia_itb | %CURRENT_MYSQL%.fnal.gov:3320 | gratia.opensciencegrid.org:8881 | gratia09.fnal.gov 3.5 |  9.2G |   2,796,812 |
| gratia_osg_integration | %CURRENT_MYSQL%.fnal.gov:3320 | gratia.opensciencegrid.org:8885 | gratia09.fnal.gov |  3.5G |      905,703 |
| gratia_qcd  | %CURRENT_MYSQL%.fnal.gov:3320 | gratia-fermi.fnal.gov:8883 | gratia08.fnal.gov: |  2.2G |      817,080 |

%BR%
The following Gratia collectors and databases will __NOT__  be converted with this release.  These repositories contained specialized reports that have not as yet been upgraded to the new Birt V2.2 software. __However__, they will be taken out-of-service while the other databases are being updated.%BR%

%TABLE{ tableborder=&quot;1&quot; cellpadding=&quot;0&quot; cellspacing=&quot;1&quot; headerbg=&quot;#99CCCC&quot; databg=&quot;#FFFFCC, #FFFFFF&quot;}%
%EDITTABLE{  header=&quot;|*Schema*|*URL*|*Description*|&quot; format=&quot;| text, 20 | text, 20 | text, 35 | text, 15 | text, 5 |  text, 15 |&quot;  changerows=&quot;on&quot; quietsave=&quot;on&quot; editbutton=&quot;Edit table&quot; }%
|*Schema*|*&lt;nop&gt;MySql port*|*Collector URL*|*Collector host*|*Size (bytes)*|*Size (rows)*|
| gratia_psacct | %CURRENT_MYSQL%.fnal.gov:3320 | gratia08.fnal.gov:8882 | gratia08.fnal.gov: |  6.6G |     4,144,690 |
| gratia_osg_daily | %CURRENT_MYSQL%.fnal.gov:3320 | gratia.opensciencegrid.org:8884 | gratia09.fnal.gov |  65.0M |    56,300     |



&lt;!-- BUILD DISTRIBUTION -------------------- --&gt;
---+ Build the %CURRENT_RELEASE% for distribution
&lt;ol&gt;
&lt;li&gt;Make sure your build area contains all _committed_ changes.
&lt;ul&gt;&lt;li&gt;svn status&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;In _gratia/build-scripts/Makefile_ , change the _version_default_ to:
&lt;ul&gt;&lt;li&gt;version_default = %NEXT_DEVEL_VERSION%&lt;/li&gt;
       &lt;li&gt;commit the change&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;Tag the release (for all committed changes):
&lt;ul&gt;&lt;li&gt;svn cp https://gratia.svn.sourceforge.net/svnroot/gratia/trunk https://gratia.svn.sourceforge.net/svnroot/gratia/tags/%RELEASE_TAG%&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt; _Into a *new* area_, export the tagged release:
&lt;ul&gt;&lt;li&gt;svn export https://gratia.svn.sourceforge.net/svnroot/gratia/tags/%RELEASE_TAG% gratia-%CURRENT_RELEASE%&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;Build it for the release (this insures that tar files are produced for VDT):
&lt;ul&gt;&lt;li&gt;cd gratia-%CURRENT_RELEASE%/build-scripts&lt;/li&gt;
&lt;li&gt;source setup-jdk15.sh&lt;/li&gt;
&lt;li&gt;make release&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;Copy the built tar files to the release area:
&lt;ul&gt;&lt;li&gt;scp ../target/*_%CURRENT_RELEASE%.tar   flxi07.fnal.gov:%RELEASE_TAR_REPOSITORY%/&lt;/li&gt;&lt;/ul&gt; &lt;/li&gt;

&lt;li&gt;Update the version number on the [[InstallationGuideVDT][services release TWiki page]]:&lt;/li&gt;
&lt;ul&gt;&lt;li&gt;Edit and update the TWiki variable _&lt;nop&gt;ReleaseVersion_.&lt;/li&gt;&lt;/ul&gt;
&lt;/ol&gt;
 

&lt;!-- ----------------- SHUTDOWN AND DATABASE BACKUP ------------- --&gt;
---+ Shutdown and database backup.

---++ On the tomcat/collector nodes
&lt;ol&gt;
&lt;li&gt; __comment__ out the __root__ user cron entry for the static reports:

On __gratia08__ :
&lt;pre class=&quot;screen&quot;&gt;
42 0 * * * &#39;/data/tomcat-fermi_itb/gratia/staticReports.py&#39; &#39;/data/tomcat-fermi_itb&#39; &#39;http://gratia-fermi.fnal.gov:8881/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-qcd/gratia/staticReports.py&#39; &#39;/data/tomcat-qcd&#39; &#39;http://gratia-fermi.fnal.gov:8883/gratia-reporting/&#39; &gt;/dev/null 
42 0 * * * &#39;/data/tomcat-fermi_osg/gratia/staticReports.py&#39; &#39;/data/tomcat-fermi_osg&#39; &#39;http://gratia-fermi.fnal.gov:8880/gratia-reporting/&#39; 
&lt;/pre&gt;

On __gratia09__ :
&lt;pre class=&quot;screen&quot;&gt;
42 0 * * * &#39;/data/tomcat-osg_integration/gratia/staticReports.py&#39; &#39;/data/tomcat-osg_integration&#39; &#39;http://gratia.opensciencegrid.org:8885/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-itb/gratia/staticReports.py&#39; &#39;/data/tomcat-itb&#39; &#39;http://gratia.opensciencegrid.org:8881/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-gratia/gratia/staticReports.py&#39; &#39;/data/tomcat-gratia&#39; &#39;http://gratia.opensciencegrid.org:8880/gratia-reporting/&#39; 
&lt;/pre&gt;
&lt;/li&gt;

    &lt;li&gt; Disable _init.d_ services  as __root__ user:

On __gratia08__:
   * chkconfig tomcat-ps off
   * chkconfig tomcat-qcd   off
   * chkconfig tomcat-fermi_osg off
   * chkconfig tomcat-fermi_itb  off

On __gratia09__ :
   * chkconfig tomcat-gratia  off
   * chkconfig tomcat-osg_daily   off
   * chkconfig tomcat-osg_integration off
   * chkconfig tomcat-itb    off

&lt;/li&gt;
        

     &lt;li&gt; For __ALL__ of the Gratia collectors (  __including those &lt;u&gt;not&lt;/u&gt; being upgraded__  ) ,  __stop__ the Gratia update services:
   * In your browser,  connect to the Gratia administrative services url for each of the databases.
   * Select the _System / Administration_ menu option in the left menu
   * Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Stop Update Services_ link.

__Note:__ Effective with Gratia v0.31, this step is not necessary.  For any pre-v0.31 installation, this step is still necessary.
 
      &lt;/li&gt;

     &lt;li&gt; __Stop__ the tomcat init.d service for__ALL__ Gratia collectors.

On __gratia08__:
   * service tomcat-fermi_itb  stop
   * service tomcat-fermi_osg stop
   * service tomcat-ps  stop
   * service tomcat-qcd stop

On __gratia09__ :
   * service tomcat-gratia  stop
   * service tomcat-itb  stop
   * service tomcat-osg_daily  stop
   * service tomcat-osg_integration stop

&lt;/li&gt;

&lt;li&gt;This is optional, but probably a good idea to save off the logs under each tomcat instance and empty the _log_ directory.  It will facilitate catching any errors that may occur (of course there won&#39;t be any, but a good idea anyway).
&lt;b&gt;This can be performed while the database backups are being performed.&lt;/b&gt;

On __gratia09__ :
   * date=`date &#39;+&lt;nop&gt;%Y&lt;nop&gt;%m&lt;nop&gt;%d&#39;`
   * cd /data/tomcat-osg_integration/logs
   * tar zcf  /data/gratia_tomcat_logs_backups/tomcat-osg_integration.$date.tgz     * 
   * rm -f *
   * 
   * cd     /data/tomcat-itb/logs/
   * tar zcf /data/gratia_tomcat_logs_backups/tomcat-itb.$date.tgz   *
   * rm -f *
   * 
   * cd  /data/tomcat-gratia/logs/
   * tar zcf /data/gratia_tomcat_logs_backups/tomcat-gratia.$date.tgz  *
   * rm -f *

On __gratia08__ :
   * date=`date &#39;+&lt;nop&gt;%Y&lt;nop&gt;%m&lt;nop&gt;%d&#39;`
   * cd        /data/tomcat-fermi_itb/logs/
   * tar zcf /data/gratia_tomcat_logs_backups/tomcat-fermi_itb.$date.tgz   *
   * rm -f *
   * 
   * cd  /data/tomcat-qcd/logs/
   * tar zcf /data/gratia_tomcat_logs_backups/tomcat-qcd.$date.tgz     *
   * rm -f *
   * 
   * cd     /data/tomcat-fermi_osg/logs/
   * tar zcf /data/gratia_tomcat_logs_backups/tomcat-fermi_osg.$date.tgz  *
   * rm -f *

&lt;/li&gt;

&lt;/ol&gt;

---++ On the !MySql server node (  %CURRENT_MYSQL% )
  &lt;ol type=&quot;1&quot;&gt;
    &lt;li&gt; __comment__ out the cron entries for:

__gratia__ user cron jobs.
&lt;pre class=&quot;screen&quot;&gt;
0 0 1-15 * *   dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=previous --update
30 01 * * *   dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
&lt;/pre&gt;

__root__ user cron entry.
&lt;pre class=&quot;screen&quot;&gt;
43 2 * * * /usr/local/bin/mysqlhotcopy_cron.sh &amp;gt; /var/log/mysqlhotcopy.out 2&amp;gt; &amp;1
&lt;/pre&gt;
&lt;/li&gt;


&lt;li&gt;On the !MySql server node ( %CURRENT_MYSQL% ):

Take a back up the database instances (this will include all schema) using part of the _msqlhotcopy_cron_ script from the command line as _root_ user.
%BR%Backup will only being done to  _/backup/mysqldb_ on %CURRENT_MYSQL% in order to reduce the downtime.
   * mysqlhotcopy -p lisp01 --addtodest fermi_itb /backup/mysqldb       
   * mysqlhotcopy -p lisp01 --addtodest gratia_osg_daily /backup/mysqldb    
   * mysqlhotcopy -p lisp01 --addtodest gratia_qcd /backup/mysqldb     
   * mysqlhotcopy -p lisp01 --addtodest gratia_osg_integration /backup/mysqldb  
   * mysqlhotcopy -p lisp01 --addtodest fermi_osg /backup/mysqldb   
   * mysqlhotcopy -p lisp01 --addtodest gratia_psacct /backup/mysqldb   
   * mysqlhotcopy -p lisp01 --addtodest gratia_itb /backup/mysqldb   
   * mysqlhotcopy -p lisp01 --addtodest gratia /backup/mysqldb      


Backup times:
%TABLE{ tableborder=&quot;1&quot; cellpadding=&quot;0&quot; cellspacing=&quot;1&quot; headerbg=&quot;#99CCCC&quot; databg=&quot;#FFFFCC, #FFFFFF&quot;}%
%EDITTABLE{  header=&quot;|*Schema*|*Expected Duration*|*Actual Duration*|&quot; format=&quot;| text, 20 | text, 20 | text, 20 |&quot;  changerows=&quot;on&quot; quietsave=&quot;on&quot; editbutton=&quot;Edit table&quot; }%
|*Schema*|*Expected Duration*|*Actual Duration*|
| fermi_itb | 1 sec |  |
| gratia_osg_daily | 6 secs  |        |
| gratia_qcd  | 64 secs |    |
| gratia_osg_integration | 99 secs  |       |
| fermi_osg | 132 secs |  |
| gratia_psacct | 179 secs  |      |
| gratia_itb | 461 secs (8 min)  |    |
| gratia | 2900 secs (48+ min)   |  |

     &lt;/li&gt;



&lt;/ol&gt;


&lt;!-- ----------------- UPGRADE AND IMPLEMENTATION ------------- --&gt;
---+ Upgrade and implementation
The __upgrades should be single-threaded__ , that is, performed for each database schema one at a time.  

We will perform these upgrades based on the size of the individual database schema, in ascending order.

---++ On the tomcat/collector nodes
&lt;ol&gt;
  &lt;li&gt;Install the new software on a Gratia tomcat instance:
%BR% pswd=xxx
%BR% source=%BUILD_DIRECTORY%
%BR% pgm=%BUILD_DIRECTORY%/common/configuration/update-gratia-local 
%BR% On __gratia09:__
   * $pgm  -d $pswd -S $source  osg_integration
   * $pgm  -d $pswd -S $source  itb
   * $pgm  -d $pswd -S $source   gratia
%BR% On __gratia08:__
   * $pgm  -d $pswd -S $source  fermi_itb
   * $pgm  -d $pswd -S $source qcd
   * $pgm  -d $pswd -S $source   fermi_osg

&lt;/li&gt;


&lt;li&gt;Start the gratia tomcat services:
   * service &amp;lt;tomcat service&amp;gt; start
   * When the tomcat service initializes, it will detect any schema changes have been effected and a conversion process will begin. 
   * _tail_ the  _catalina.out_ log. When the conversion process completes the log will show the following message:%BR%
       &quot;INFO: Server startup in _xxx_ms&quot;

&lt;li&gt;Then, _start_ the Gratia _update_ services for the database schema just upgraded.%BR%
   * In your browser, connect to the Gratia administrative services url for each of the databases.%BR%
   * Select the _System / Administration_ menu option in the left menu%BR%
   * Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Start Update Services_ link.
   * As __each__ collector/tomcat host update service is started, monitor the tomcat logs files for any errors.
   * Bring up the gratia _administration_ web interface and verify that the collectors are processing the data.
   * Bring up the gratia _reporting_ web interface and verify that the reports look reasonable while still _tail&#39;ing_ the log files.

&lt;li&gt;If all looks good, _stop_ the Gratia _update_ services for the database schema just upgraded.


&lt;li&gt; Run the static reports cron as _root_ and verify these reports are generated:
   * ./gratia-reports/reports-static/UsageByVOByDate-ranked.pdf
   * ./gratia-reports/reports-static/UsageBySiteByDate-ranked.pdf
   * ./gratia-reports/reports-static/WeeklyUsageByVO-ranked.pdf

On __gratia08__ :
&lt;pre class=&quot;screen&quot;&gt;
42 0 * * * &#39;/data/tomcat-fermi_itb/gratia/staticReports.py&#39; &#39;/data/tomcat-fermi_itb&#39; &#39;http://gratia-fermi.fnal.gov:8881/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-qcd/gratia/staticReports.py&#39; &#39;/data/tomcat-qcd&#39; &#39;http://gratia-fermi.fnal.gov:8883/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-fermi_osg/gratia/staticReports.py&#39; &#39;/data/tomcat-fermi_osg&#39; &#39;http://gratia-fermi.fnal.gov:8880/gratia-reporting/&#39; 
&lt;/pre&gt;

On __gratia09__ :
&lt;pre class=&quot;screen&quot;&gt;
42 0 * * * &#39;/data/tomcat-osg_integration/gratia/staticReports.py&#39; &#39;/data/tomcat-osg_integration&#39; &#39;http://gratia.opensciencegrid.org:8885/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-itb/gratia/staticReports.py&#39; &#39;/data/tomcat-itb&#39; &#39;http://gratia.opensciencegrid.org:8881/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-gratia/gratia/staticReports.py&#39; &#39;/data/tomcat-gratia&#39; &#39;http://gratia.opensciencegrid.org:8880/gratia-reporting/&#39; 
&lt;/pre&gt;
&lt;/li&gt;

&lt;li&gt;If satisifed, __stop__ the tomcat service for that tomcat database schema so you can proceed to the next one.
   * service &amp;lt;tomcat service&amp;gt; stop
&lt;/li&gt;
&lt;/ol&gt;


&lt;!-- ----------------- ACTIVATING THE NEW RELEASE ------------- --&gt;
---+ Activating the new release

---++ On the tomcat/collector nodes
  &lt;ol&gt;
        &lt;li&gt; __start__ the tomcat service for the Gratia collectors.%BR%

On __gratia08:__
   * service tomcat-fermi_itb  start
   * service tomcat-fermi_osg start
   * service tomcat-ps start
   * service tomcat-qcd start

On __gratia09:__
   * service tomcat-gratia start  
   * service tomcat-itb start
   * service tomcat-osg_daily start
   * service tomcat-osg_integration start
&lt;/li&gt;

   &lt;li&gt; As __each__ collector/tomcat host service is started, monitor the tomcat logs files for any errors.

&lt;/li&gt;

   &lt;li&gt; Bring up the gratia administrative web interface and verify that the collectors are processing the data.%BR%
     If they are not processing any data,  __verify__ the Gratia update services are active.
&lt;ul&gt;&lt;li&gt;In your browser,  connect to the Gratia administrative services url for each of the databases.&lt;/li&gt;
       &lt;li&gt;Select the _System / Administration_ menu option in the left menu&lt;/li&gt;
       &lt;li&gt;Then scroll down to the _Starting/Stopping Database Update Services_ section and and view the status.&lt;/li&gt;
&lt;/ul&gt;
      &lt;/li&gt;

    &lt;li&gt; Enable _init.d_ services for all tomcats as __root__ user:

On __gratia08:__
   * chkconfig tomcat-ps on
   * chkconfig tomcat-qcd   on
   * chkconfig tomcat-fermi_osg on
   * chkconfig tomcat-fermi_itb  on

On __gratia09:__
   * chkconfig tomcat-gratia  on
   * chkconfig tomcat-osg_daily   on
   * chkconfig tomcat-osg_integration on
   * chkconfig tomcat-itb    on
&lt;/li&gt;

   &lt;li&gt; __Uncomment/verify__   the __root__ user cron entry for the static reports:

On __gratia08__ :
&lt;pre class=&quot;screen&quot;&gt;
42 0 * * * &#39;/data/tomcat-fermi_itb/gratia/staticReports.py&#39; &#39;/data/tomcat-fermi_itb&#39; &#39;http://gratia-fermi.fnal.gov:8881/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-qcd/gratia/staticReports.py&#39; &#39;/data/tomcat-qcd&#39; &#39;http://gratia-fermi.fnal.gov:8883/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-fermi_osg/gratia/staticReports.py&#39; &#39;/data/tomcat-fermi_osg&#39; &#39;http://gratia-fermi.fnal.gov:8880/gratia-reporting/&#39; 
&lt;/pre&gt;

On __gratia09__ :
&lt;pre class=&quot;screen&quot;&gt;
42 0 * * * &#39;/data/tomcat-osg_integration/gratia/staticReports.py&#39; &#39;/data/tomcat-osg_integration&#39; &#39;http://gratia.opensciencegrid.org:8885/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-itb/gratia/staticReports.py&#39; &#39;/data/tomcat-itb&#39; &#39;http://gratia.opensciencegrid.org:8881/gratia-reporting/&#39; 
42 0 * * * &#39;/data/tomcat-gratia/gratia/staticReports.py&#39; &#39;/data/tomcat-gratia&#39; &#39;http://gratia.opensciencegrid.org:8880/gratia-reporting/&#39; 
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

---++ On the !MySql server node ( %CURRENT_MYSQL% )
&lt;ol&gt;
&lt;li&gt; __Uncomment__   the __root__ user cron job(s) for gratia backups
&lt;pre class=&quot;screen&quot;&gt;
43 2 * * * /usr/local/bin/mysqlhotcopy_cron.sh &gt; /var/log/mysqlhotcopy.out 2&gt;&amp;1
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt; __Uncomment__  the __gratia__ user cron job(s).
&lt;pre class=&quot;screen&quot;&gt;
0 0 1-15 * *   dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=previous --update
30 01 * * *   dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;



&lt;!-- ----------------- POST MORTEM  ------------- --&gt;
---+ Post-mortem
At this time, this will appear to be random notes.  After the conversion, they may be organized.

&lt;ol&gt;

&lt;/ol&gt;


%STOPINCLUDE%

&lt;!-- MAJOR UPDATES
For significant updates to the topic, consider adding your &#39;signature&#39; (beneath this editing box) !--&gt;
---++!! Major updates
&lt;!--Future editors should add their signatures beneath yours!--&gt;

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> AdminSetupMPI &lt; Sandbox &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Sandbox/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Sandbox/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Sandbox/AdminSetupMPI?_T=16 Feb 2017" type="application/x-wiki" title="edit AdminSetupMPI" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><meta name="robots" content="noindex,nofollow"> 
<base href="https://twiki.opensciencegrid.org/bin/view/Sandbox/AdminSetupMPI"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#DDDDDD;
	}
	.patternBookView {
		border-color:#DDDDDD;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">



</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <h1><a name="Admin_Setup_MPI"></a>  <strong><noop>Admin Setup MPI</strong> </h1>
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Adding_site_specific_MPI_attribu"> Adding site specific MPI attributes</a>
</li> <li> <a href="?cover=print#Add_JobManager_specific_bits"> Add JobManager-specific bits</a>
</li> <li> <a href="?cover=print#PBS_and_Modules_Purdue"> PBS and Modules (Purdue)</a> <ul>
<li> <a href="?cover=print#Enable_the_handle_attribute"> Enable the "handle" attribute</a>
</li> <li> <a href="?cover=print#Make_appropriate_changes_to_the"> Make appropriate changes to the Globus JobManager</a>
</li></ul> 
</li> <li> <a href="?cover=print#PBS_and_Modules_NERSC"> PBS and Modules (NERSC)</a> <ul>
<li> <a href="?cover=print#NERSC_Franklin_Cray_Environment"> NERSC-Franklin (Cray Environment)</a>
</li> <li> <a href="?cover=print#NERSC_Jacquard_and_NERSC_Davinci"> NERSC-Jacquard and NERSC-Davinci (SLES 10 environment)</a>
</li> <li> <a href="?cover=print#Globus_Jobmanager_Changes"> Globus Jobmanager Changes</a>
</li></ul> 
</li></ul> 
</div>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Adding_site_specific_MPI_attribu"></a> Adding site specific MPI attributes </span></h2>
Follow the instructions on the <a href="/bin/view/ReleaseDocumentation/GenericInformationProviders" class="twikiLink">Generic Information Providers</a> page to add Glue attributes. The following is an example of how to add an MPICH version along with the module information.
<p />
In etc/add-attributes.conf:
<pre class=screen>
# MPICH Intel
dn: GlueSoftwareLocalID=MPICH_1.2.7p1_intel, GlueSubClusterUniqueID=lepton.rcac.
purdue.edu, GlueClusterUniqueID=lepton.rcac.purdue.edu,mds-vo-name=local,o=grid
objectClass: GlueClusterTop
objectClass: GlueSoftware
objectClass: GlueKey
objectClass: GlueSchemaVersion
GlueHostApplicationSoftwareRunTimeEnvironment: MPICH_1.2.7p1_intel
GlueSoftwareLocalID: MPICH_1.2.7p1_intel
GlueSoftwareName: MPICH
GlueSoftwareVersion: 1.2.7.p1_intel
GlueSoftwareInstalledRoot: /apps/steele/mpich-1.2.7p1/64/p4-intel-10.1.015
GlueSoftwareModuleName: mpich-intel
GlueSoftwareEnvironmentSetup: module load mpich-intel
GlueChunkKey: GlueSubClusterUniqueID=lepton.rcac.purdue.edu
GlueSchemaVersionMajor: 1
GlueSchemaVersionMinor: 3
</pre>
<p />
in etc/alter-attributes.conf:
<pre class=screen>
GlueHostApplicationSoftwareRunTimeEnvironment: MPICH_1.2.7p1_gcc
</pre>
<p />
There are a few points to make here. First, the <span class="twikiNewLink">SoftwareInstalledRoot<a href="/bin/edit/Sandbox/SoftwareInstalledRoot?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="SoftwareInstalledRoot (this topic does not yet exist; you can create it)">?</a></span> tells the user the location of the binaries and libraries for each MPI version. The <span class="twikiNewLink">SoftwareEnvironmentSetup<a href="/bin/edit/Sandbox/SoftwareEnvironmentSetup?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="SoftwareEnvironmentSetup (this topic does not yet exist; you can create it)">?</a></span> attribute tells the user what module or softenv command to use in order to utilize the MPI version in their job. Finally, the <span class="twikiNewLink">SoftwareName<a href="/bin/edit/Sandbox/SoftwareName?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="SoftwareName (this topic does not yet exist; you can create it)">?</a></span> and <span class="twikiNewLink">SoftwareVersion<a href="/bin/edit/Sandbox/SoftwareVersion?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="SoftwareVersion (this topic does not yet exist; you can create it)">?</a></span> attributes make the MPI version easier to find using an ldapsearch.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Add_JobManager_specific_bits"></a> Add <span class="twikiNewLink">JobManager<a href="/bin/edit/Sandbox/JobManager?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="JobManager (this topic does not yet exist; you can create it)">?</a></span>-specific bits </span></h2>
The hardest part of getting MPI jobs running is setting up the <span class="twikiNewLink">JobManager<a href="/bin/edit/Sandbox/JobManager?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="JobManager (this topic does not yet exist; you can create it)">?</a></span> to deal with different MPI versions. In a general sense, there are only two things that need to be done: <ul>
<li> Enable the "handle" attribute in the appropriate .rvf file
</li> <li> Add an MPI section to your <span class="twikiNewLink">JobManager<a href="/bin/edit/Sandbox/JobManager?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="JobManager (this topic does not yet exist; you can create it)">?</a></span> <scheduler>.pm file
</li></ul> 
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="PBS_and_Modules_Purdue"></a> PBS and Modules (Purdue) </span></h2>
Purdue uses a combination of the PBS scheduler and the <a href="http://modules.sourceforge.net/" target="_top">modules</a> environment management software. Most schedulers and environment managers should follow a similar pattern.
<p />
<h3><a name="Enable_the_handle_attribute"></a> Enable the "handle" attribute </h3>
The first thing to do is to change the appropriate .rvf file in Globus. For PBS, this is in $GLOBUS_LOCATION/share/globus_gram_job_manager/pbs.rvf
<p />
Add the following lines to enable the "handle" RSL attribute:
<pre class=screen>
Attribute: handle
Description: "Defines the module that should be loaded"
ValidWhen: GLOBUS_GRAM_JOB_SUBMIT
</pre>
<p />
<h3><a name="Make_appropriate_changes_to_the"></a> Make appropriate changes to the Globus <span class="twikiNewLink">JobManager<a href="/bin/edit/Sandbox/JobManager?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="JobManager (this topic does not yet exist; you can create it)">?</a></span> </h3>
Next, the appropriate <span class="twikiNewLink">JobManager<a href="/bin/edit/Sandbox/JobManager?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="JobManager (this topic does not yet exist; you can create it)">?</a></span> file needs to be modified to do something with our new handle attribute. For PBS, this file is located in $GLOBUS_LOCATION/lib/perl/Globus/GRAM/JobManager/pbs.pm
<p />
The first change to make is to get the handle name from the job description. To do this, add the following line in the sub submit{} stanza:
<pre class=screen>
    my $handle = $description->handle();
</pre>
<p />
The next change is the stanza to take care of the PBS job script:
<pre class=screen>
        if ($description->jobtype() eq "mpi")
        {
            my $this_count = ($description->totalprocesses() > 0) ?
                $description->totalprocesses() : $description->count();
            my $machinefilearg = ($cluster) ? ' -machinefile $PBS_NODEFILE' : '';

            if ($mpisoftenv)
            {
                print JOB 'which mpiexec >/dev/null 2>&1' . "\n";
                print JOB 'if [ $? == 0 ]; then' . "\n";
                print JOB "  mpiexec $machinefilearg -n " . $this_count;
                print JOB " $cmd_script_name < " .  $description->stdin() . "\n";
                print JOB 'else' . "\n";
                print JOB '  which mpirun >/dev/null 2>&1' . "\n";
                print JOB '  if [ $? == 0 ]; then' . "\n";
                print JOB "    mpirun -np " . $this_count . $machinefilearg;
                print JOB " $cmd_script_name < " .  $description->stdin() . "\n";
                print JOB '  else' . "\n";
            }
            else
            {
                print JOB ". /etc/profile.d/modules.sh\n";
                # ahoward Thu Aug 21 14:05:57 EDT 2008  
                # Check to see if user specified a module to load...
                if ($description->handle() ne '')
                {
                    print JOB "module load $handle\n";
                }
                # ... otherwise load a default module
                else
                {
                    print JOB " module load mpich-1.2.7p1-intel64/9.1.045\n";
                }
        
                if ($handle =~ m/mpich2/)
                {
                    print JOB "$mpirun -np " . $this_count;
                }
                else        
                {
                    print JOB "$mpirun -np " . $this_count . $machinefilearg;
                }
                
                #print JOB " " .  $description->executable() . " < " .  $description->stdin() . "\n";
                print JOB " " .  $description->executable() . " $args < " . $description->stdin() . "\n";
                #print JOB " $cmd_script_name < " .  $description->stdin() . "\n";

            }
            if ($mpisoftenv)
            {
                print JOB '  fi' . "\n";
                print JOB 'fi' . "\n";
            }

        }
</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="PBS_and_Modules_NERSC"></a> PBS and Modules (NERSC) </span></h2>
<h3><a name="NERSC_Franklin_Cray_Environment"></a> NERSC-Franklin (Cray Environment) </h3>
The interactive login environment on the NERSC-Franklin Cray XT4 CE (based on SLES 10) is fundamentally different from the Worker node environment (Compute Node Linux). The implications of this are: <ul>
<li> All MPI codes must be cross compiled so that they can run on the worker nodes
</li> <li> Worker nodes do not have support for dynamically loaded libraries. This means that the MPI job executable must be launched directly, and cannot be contained in a wrapper script
</li> <li> Users do not invoke "modules" in their jobs because of the above constraint.
</li> <li> Because of the complexities of cross-compilation, it is recommended to login directly via ssh and compile any codes directly on the interactive nodes. More details can be found here: <a href="http://www.nersc.gov/nusers/systems/franklin/programming/" target="_top">http://www.nersc.gov/nusers/systems/franklin/programming/</a>
</li></ul> 
<p />
Once codes have been compiled for the worker node environment, users may launch MPI jobs using grid interfaces, with the caveat that MPI jobs must <strong>not</strong> be wrapped in a shell script. 
<p />
<h3><a name="NERSC_Jacquard_and_NERSC_Davinci"></a> NERSC-Jacquard and NERSC-Davinci (SLES 10 environment) </h3>
MPI Modules are automatically sourced for jobs on the NERSC-Jacquard and NERSC-Davinci systems. To enable this we add the following lines to $VDT_LOCATION/vdt/etc/vdt-local-setup.sh{.csh}
<pre class=screen>
# vi $VDT_LOCATION/vdt/etc/vdt-local-setup.sh
source /etc/profile.d/modules.sh
module load mvapich path

# vi $VDT_LOCATION/vdt/etc/vdt-local-setup.csh
source /etc/profile.d/modules.csh
module load mvapich path
</pre>
<p />
To source other modules (in this example: module_name) in your job, simply add the following line to your job script:
<pre class=screen>
module load module_name
</pre>
<p />
<h3><a name="Globus_Jobmanager_Changes"></a> Globus Jobmanager Changes </h3>
<p />
Here is a diff of the NERSC PBS jobmanager from the original PBS jobmanager (for non Cray systems NERSC-Jacquard and NERSC-Davinci)
<p />
<pre class='screen'>
$ diff pbs.pm pbs.pm.orig 
24,25c24
<     # Change CPU per node - SPC, NERSC
<     $cpu_per_node = 2;
---
>     $cpu_per_node = 1;
72,78d70
< 
<         # For non-mpi jobs, change cluster variable to bypass bogus ssh - SPC, NERSC
<         if($description->jobtype !~ /^(mpi)$/)
<         {
<            $cluster = 0;
<         }
< 
271d262
<         # Add ppn parameter - SPC, NERSC
273c264
<         myceil($description->count() / $cpu_per_node), ":ppn=$cpu_per_node", "\n";
---
>         myceil($description->count() / $cpu_per_node), "\n";
512,514c503
<             # NODEFILE semantics not supported at NERSC - SPC, NERSC
<             # my $machinefilearg = ($cluster) ? ' -machinefile $PBS_NODEFILE' : '';
<             my $machinefilearg = '';
---
>             my $machinefilearg = ($cluster) ? ' -machinefile $PBS_NODEFILE' : '';
</pre>
<p />
The Cray systems have additional MPI changes due to an idiosyncratic job launcher (aprun instead of mpirun), lack of dynamic library support and specialized PBS variables.
Here is the diff for the NERSC-Franklin Cray XT4 system:
<pre class='screen'>
19,20c19
<     # Change to aprun - SPC, NERSC
<     $mpirun = '/usr/bin/aprun';

---
>     $mpirun = 'no';
72,77d70
< 
<         # For non-mpi jobs, change cluster variable to bypass bogus ssh - SPC, NERSC
<         if($description->jobtype !~ /^(mpi)$/)
<         {
<            $cluster = 0;
<         }
262,265c255
< 
<       # Change -l nodes to -l mppwidth - SPC, NERSC
<       print JOB '#PBS -l mppwidth=', $description->nodes(), "\n";
---
>       print JOB '#PBS -l nodes=', $description->nodes(), "\n";
269,271c259
<       # Change -l nodes to -l mppwidth - SPC, NERSC
<       print JOB '#PBS -l mppwidth=', $description->host_count(), "\n";
---
>       print JOB '#PBS -l nodes=', $description->host_count(), "\n";
275,284c263,264
<       # Change -l nodes to -l mppwidth - SPC - NERSC
<       print JOB '#PBS -l mppwidth=', $description->count(), "\n";
< 
<       # May want to tweak mppnppn later - SPC - NERSC
<       # print JOB '#PBS -l mppnppn=', $cpu_per_node, "\n";
<       # myceil($description->count() / $cpu_per_node), "\n";
< 
< 
---
>         print JOB '#PBS -l nodes=',
>         myceil($description->count() / $cpu_per_node), "\n";
523,525c503
<           #  NODEFILE semantics not supported at NERSC - SPC - NERSC
<           my $machinefilearg = '';
---
>             my $machinefilearg = ($cluster) ? ' -machinefile $PBS_NODEFILE' : '';
547,551c525,526
<               # NERSC aprun mods - SPC - NERSC
<               # directly launch executable on worker node, since scripts are not supported
<               print JOB "$mpirun -n " . $this_count . $machinefilearg . " ";
<               print JOB $description->executable()," $args < " .  $description->stdin() . "\n";
---
>               print JOB "$mpirun -np " . $this_count . $machinefilearg;
>               print JOB " $cmd_script_name < " .  $description->stdin() . "\n";
</pre>
<p />
-- <span class="twikiNewLink">ShreyasCholia<a href="/bin/edit/Main/ShreyasCholia?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="ShreyasCholia (this topic does not yet exist; you can create it)">?</a></span> - 12 Feb 2009
<p />
-- <a href="/bin/view/Main/AndrewHoward" class="twikiLink">AndrewHoward</a> - 06 Oct 2008</div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Sandbox<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>AdminSetupMPI</span> <br />    
Topic revision: r5 - 13 Feb 2009 - 00:27:12 - <span class="twikiNewLink">ShreyasCholia<a href="/bin/edit/Main/ShreyasCholia?topicparent=Sandbox.AdminSetupMPI" rel="nofollow" title="ShreyasCholia (this topic does not yet exist; you can create it)">?</a></span>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
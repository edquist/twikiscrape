%LINKCSS%

%STARTINCLUDE%
&lt;img src=&quot;/pub/Sandbox/ForrestChristianSandbox/natskintest_patternskin_low.gif&quot; alt=&quot;natskintest_patternskin_low.gif&quot; style=&quot;border:2px; width:25%; height:25%;&quot; onLoad=&quot;setPref(&#39;LOGINNAME&#39;, &#39;Timmy&#39;); setPref(&#39;LOGINHOST&#39;, &#39;this.is.my.host.be&#39;); var doShow = getPref(&#39;LOGINNAME&#39;); var doShow = getPref(&#39;LOGINHOST&#39;);&quot;/&gt;

---
%INCLUDE{ &quot;ForrestChristianSandbox&quot; }%


%LOGINHOST%

%LOGINHOSTSHORT%

   * Set STARTNUM = 1
   * Set COOKIE = %HTTP{&quot;Cookie&quot;}%

   * Set NAMESTART = %CALC{&quot;$EVAL($FIND(LOGINNAME, %COOKIE%, %STARTNUM%) + 11)&quot;}%
%CALC{&quot;$SET(NAMESTART, $EVAL($FIND(LOGINNAME, %COOKIE%, %STARTNUM%) + 11))&quot;}%
   * Set NAMEEND = %CALC{&quot;$EVAL($FIND(7C, %COOKIE%, $GET(NAMESTART)) - 2)&quot;}%
%CALC{&quot;$SET(NAMEEND, $EVAL($FIND(7C, %COOKIE%, $GET(NAMESTART)) - 2))&quot;}%

%CALC{&quot;$GET(NAMEEND)&quot;}% and %CALC{&quot;$GET(NAMESTART)&quot;}%

   * Set LOGINNAME2 = %CALC{&quot;$REPLACE($REPLACE(%COOKIE%, 1, $GET(NAMESTART),), $GET(NAMEEND), 1000,)&quot;}%
%CALC{&quot;$SET(LOGINNAME2, $REPLACE($REPLACE(%COOKIE%, 1, $GET(NAMESTART),), $GET(NAMEEND), 1000,))&quot;}%
   * Set LOGINNAME = %CALC{&quot;$REPLACE($GET(LOGINNAME2), $GET(NAMEEND), 4000,)&quot;}%


   * Set COOKIE_LESS_ENDING = &lt;pre&gt;%CALC{&quot;$LISTJOIN($n, $TRANSLATE($REPLACE($TRANSLATE($SUBSTITUTE($SUBSTITUTE($REPLACE($REPLACE(%COOKIE%, $FIND(;, %COOKIE%), 1000,), 1, 10,), $NOP(%7C), #@@@* Set ), $NOP(%3D), @=@), @, $sp), 1, 1,), #, $comma))&quot;}%&lt;/pre&gt;

---

%CALC{&quot;$TRANSLATE($REPLACE($TRANSLATE($SUBSTITUTE($SUBSTITUTE($REPLACE($REPLACE(%COOKIE%, $FIND(;, %COOKIE%), 1000,), 1, 10,), $NOP(%7C), #), $NOP(%3D), @=@), @, $sp), 1, 1,), #, $comma)&quot;}%

---

| %COOKIE_LESS_ENDING% | %LOGINNAME% | %LOGINHOST% |
%STOPINCLUDE%



%TOC{ &quot;CEInstallGuide&quot; web=&quot;ReleaseDocumentation&quot; depth=&quot;2&quot; %}%

Steven Timm wrote:
&gt; On Wed, 11 Apr 2007, Frank Wuerthwein wrote:
&gt;
&gt;&gt; If you do NFS-lite then you can&#39;t stream because the stdout is 
&gt;&gt; transfered upon exit.
&gt;
&gt; What about the case where NFS-lite is not involved?  Is it supposed
&gt; to work then?
&gt;
&gt;
Streaming IO is disabled to protect both the CE and the submitter. The 
CE dies because streaming stdio means no gridmonitor even when using 
condor-g (no streaming is a pre-requisite), the submitter dies because 
it does not take that many streaming IO jobs to kill a submitter 
although it can typically take more streaming than the CE can take 
unrestricted jobmanagers.

NFS-Lite only removes the streaming from the WN to the CE, the leg of 
the journey that Grid monitor does not protect.

&gt;&gt;
&gt;&gt; From an architectural standpoint, it is absurd to stream out 
&gt;&gt; everything in sight only because occasionally
&gt;&gt; you want to look at it.
&gt;
&gt; In this case the user is looking at each output and running a fairly
&gt; small number of simultaneous jobs (25), on average half of which would
&gt; have to be prematurely killed.  I am sure this user will learn with time
&gt; how to do some of the functions you describe below.
&gt;
Unfortunately streaming with the grid middleware is an all or nothing 
situation where if streaming is allowed then the system will fail. 
However that does not mean that the ability to get live stdio, if done 
reasonably, cannot be a service a VO or submitter site offers to users. 
Most OSG sites that I am aware of offer unrestricted egress of network 
traffic. A VO or submitter could submit a companion job that acts as a 
stdio redirector and streams the stdio to a central system managed by 
the VO or submitter site. In simple terms a job syslog that logs 
remotely.  Since the stdio would be streaming through a side channel 
that only uses network capacity at the site even heavy logging should 
have a negligible impact on the site, and it is much easier for a site 
to throttle network traffic using various QoS technologies than it is to 
scale up disk subsystems to handle hundreds or even thousands of stdio 
streams. This also puts the burden of supporting stdio streaming on the 
VO or submitter site, not the OSG site and their infrastructure.

Hopefully it would also help to improve the error handling mechanisms 
employed with the job submitted.

As Frank said the better solution for this user is to wrap his program 
in an alarm loop where unless certain conditions are met the parent 
(wrapper) will kill the child (program) and then report in a 
non-streaming mechanism that the job was killed a why.  That is the 
programmer should learn to write better error handling into their job.

------
&gt;&gt; Steven Timm wrote:
&gt;
&gt; I am well aware of what UCSD has done and why it has been done.  I am
&gt; not disputing the technical merits of banning streaming on a wide scale.
&gt; My question is one of policy--is there anything in OSG policy that
&gt; bans it outright? I don&#39;t think so, and of technical possibility--
&gt; can it be done on the one or two job scale? there&#39;s nothing
&gt; in the docs that say no.
&gt;
&gt; There is a new and potentially large VO that could potentially join 
&gt; OSG if the early tests we are doing at Fermilab prove successful.  
&gt; Obviously
&gt; we will not turn them loose on the OSG while they are still streaming,
&gt; but without the diagnostics from these tests they may not make it there.
&gt; It is in the interest of all on OSG to help them out.
&gt;
Streaming is something I would expect falls under site AUP, not the OSG. 
We do not specifically ban it, after all I do not care if a user streams 
IO, as long as they use some other mechanism besides condor-g/GRAM to do 
it. We have no philosophical problem with streaming IO, the problem is 
that the grid middle ware is simply incapable of handling it at all but 
the most trivial scale.

If you want to make a special case of this user you can have them turn 
on streaming at the submitter via the condor-g submit script and make 
sure the cluster is not using NFS Lite because that causes condor to 
turn off streaming between the starter and shadow. Grid monitor will not 
start and you will get streaming IO into the submitters standard IO 
files.  Keep in mind however too much STDIO can cause serious problems 
here, even in the GRAM software itself.

On the topic of getting new VOs added to the OSG, it is never too early 
to encourage new VOs to approach the OSG with the best possible strategy 
for successful job submission while placing the least impact on the 
target sites. At the risk of sounding pedantic the best way to avoid bad 
habits is to not start them.  UCSD has helped more than one new VO get 
onto the OSG. Our efforts start with best practices. If at all possible 
we try to encourage testing of user jobs without special arrangements at 
our site and using submission technologies that place the least burden 
on the CE where possible. The exception to standard procedure is that we 
will provide higher priorities and slot quotas to a VO to enable them to 
get the slots they need to actually do their testing. Aside from that we 
try to present as a typical OSG site to the VO.

Back to what I was saying in my last email about side channels for 
stdio. The minimum level of support for this would be as a site admin to 
offer the VO a place to write their logs that is not out the GRAM. At 
UCSD this has been done traditionally in two ways. One is to have them 
write IO to OSG_DATA and grab it with tools like uberftp. The other is 
to have an NFS mount that is not published to the OSG, but which a job 
can write to and which is mounted in such a way that a user can run a 
tail -f on it.  We do this for local CMS users typically due to the 
massive amount of logging that software is capable of in a way that 
avoids putting a burden on the grid infrastructure. At most they crumble 
an NFS server the that has no effect on the cluster as a whole. We have 
only ever made this exception for local CMS users at UCSD.

Terrence

------

If you use the Condor-G grid monitor, you cannot use streaming.

http://www.cs.wisc.edu/condor/manual/v6.8/5_3Grid_Universe.html#sec:Condor-G-Documentation/Release3.GridMonitor

&gt;By default, standard output and standard error are streamed back to 
&gt;the submitting machine while the job is running. Streamed I/O 
&gt;requires the jobmanager. As a result, the Grid Monitor cannot 
&gt;replace the jobmanager for jobs that use streaming. If possible, 
&gt;disable streaming for all jobs; this is accomplished by placing the 
&gt;following lines in each job&#39;s submit description file:
&gt;
&gt;stream_output = False
&gt;stream_error  = False

Just to clarify why this is: the Condor-G grid monitor turns of the 
job managers for individual jobs for most of the time while the job 
is running. When the job manager isn&#39;t running, it isn&#39;t querying the 
batch system and causing high load.

Streaming I/O requires the job manager to be running. So the two are 
incompatible.
-- Alain Roy

---++ Generic OSG VO Guide
This process assumes that you have:
   1. [[ReleaseDocumentation.CertificateUserGet][Obtained and maintained a valid client certificate]] from an OSG-trusted Certificate Authority.
   1. Registered successfully in an OSG-affiliated [[Trash.DocumentationVOWhatIs][Virtual Organization]].
   1. Ported an existing application to parallelization or created a new parallel application for running on the grids.
   1. Access to a working interface to the OSG with the appropriate [[ReleaseDocumentation.0_4_1.ClientInstallationGuide][client software installed]].
   1. Documented the IO interface requirements for your application.
   1. Documented the environment requirements for your application.
   1. [[ForrestChristianCanIRun][Have determined that you can run your application on this site.]]




---++ Log in and create a proxy certificate.
%INCLUDE{ &quot;Education.LoggingInToTheGrid&quot; pattern=&quot;^.*(&lt;pre.*?pre&gt;).*&quot; }%


^.*&lt;\!--tzdate:date--&gt;(.*?)&lt;\!--/tzdate:date--&gt;.*

-------------

%INCLUDE{ &quot;Documentation.ToolsBottomMatter&quot; }%

                                                    
-- Main.ForrestChristian 2006 nov 20



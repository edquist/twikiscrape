---+ Hadoop Distributed File System

The Hadoop Distributed File System (!HDFS) is a highly scalable, very reliable distributed file system developed by the Apache project as a part of the Hadoop data processing system.  The primary contributor (and largest user) is Yahoo.  !HDFS is based on the design of the Google File System.  !HDFS&#39;s strengths is in its ability to use commodity hard drives in worker nodes; it can turn a large amount of semi-reliable hardware into a system which is very reliable.

To find out more information about HDFS, [[http://hadoop.apache.org/hdfs/][visit its home page]].  If you are thinking about installing Hadoop, we also recommend reading the [[http://hadoop.apache.org/common/docs/current/hdfs_design.html][HDFS architecture page]].

This page covers the OSG&#39;s usage of Hadoop, and includes instructions for installing a grid-enabled HDFS system.

---++ Information for Site Admins

---+++ Preparation

If you plan on installing a Hadoop SE on the OSG, we recommend [[HadoopUnderstanding][starting off with the planning document]].

Just *curious about HDFS?*  The planning document includes a [[HadoopUnderstanding#Minimal_Installation_0_50TB_WAN][section on a minimal install]].  For the HDFS core components, this would require at least 3 nodes (1 namenode and 2 datanodes), but this will not give you full functionality.  It will take 5 nodes to enable all the components.

---+++ Installation
Once you have read the planning document and feel you understand the general architecture, follow these guides (in order).

Major components:
   * [[HadoopInstallation][Hadoop and FUSE]].  This guide covers installation of the core HDFS components, including the FUSE-based mounts.  Once completed, you will be able to store files and interact with the file system locally.
      * [[Hadoop20Installation][Hadoop and FUSE]].  This guide covers installation of the next version of Hadoop, 0.20.
   * [[HadoopGridFTP][GridFTP]].  This guide covers installation of the HDFS-aware !GridFTP server.  Once completed, you should be able to copy files in and out of HDFS through the grid-standard WAN protocol, !GridFTP.
      * [[Hadoop20GridFTP][GridFTP]].  This guide covers installation of the GridFTP server compatible with the next version of Hadoop, 0.20.
   * [[HadoopSRM][SRM]].  This covers the installation of a !BeStMan SRM server on top of HDFS.  Once completed, you should be able to interact with HDFS via SRM, a grid-standard webservices protocol for doing metadata operations remotely.
   * [[HadoopXrootd][Xrootd server]].  (Experimental).  Using Xrootd to export your data over the WAN and allow quick and secure ROOT access to files.

Minor components:
   * [[HadoopGratia][Gratia Probe]].  The Gratia probe instruments the !GridFTP servers running on HDFS; it uses their log files to send records of all completed transfers to a central server.  Once completed, you should see transfers at your site show up in the central OSG accounting.
   * [[HadoopStorageReports][Hadoop Chronicle Storage Reports]].  These are gratia-based storage reports that give you a daily and historical view of the status of your Hadoop cluster.
   * [[HadoopApache][Apache integration]]. Configuring the Apache web server to serve up files from HDFS

---+++ Validation

These guides provide simple tests you can perform to see if your install is functioning.
   * [[HadoopCoreValidation][Hadoop Core + FUSE]].
   * [[HadoopGridftpValidation][Hadoop / GridFTP interface]].
   * [[HadoopSRMValidation][SRM verification]].
   * [[HadoopGratiaValidation][Gratia validation]].

---+++ Operations and Troubleshooting
HDFS, while relatively easy to administrate, is not completely headache free!  The pages below offer tips and tricks for operating and maintaining HDFS.
   * [[HadoopOperations][Operations]].  How to operate a stable HDFS instance.  Keep this page handy.
   * [[HadoopDebug][Troubleshooting]].
   * [[HadoopUpgrade][Upgrades]].  Currently written for the transition from 0.19 to 0.20.
   * [[HadoopRecovery][Recovery]].
   * [[HadoopMonitoring][Monitoring]].
   * [[HadoopPhedex][Phedex Agent Tips]].  CMS-specific tips to getting their transfer application working optimally with HDFS.

---++ Get Involved!  Contact Us!

   $ Mailing list: [[mailto:osg-hadoop@fnal.gov][osg-hadoop@fnal.gov]]
   $ Chat: =uscms-t2@conference.fnal.gov= (Jabber Multi-User Chat)

---++ Information for developers 

   * Development
      * [[Trash.StorageHadoopTodo][TODO]]
      * [[HadoopRelease][Building a release]]
      * [[ReleaseDocumentation.Hadoop20Install][Hadoop 0.20.0 / CDH3 upgrade]] Transitional instructions for building the RPMs for the HDFS 0.20.0 upgrade 
      * [[HadoopRepos][Repositories]] Links to relevant code repositories
   * Talks and workshops
      * [[HdfsWorkshop][HDFS Workshop]] (UCSD, 2009.03.11)
      * [[CmsSeRequirements][CMS-specific requirements tracking]]
      * [[http://indico.cern.ch/conferenceDisplay.py?confId=67969][USCMS Review of HDFS as a Storage Technology]]

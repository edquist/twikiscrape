---+ Introduction to the Mass Storage System 
%STARTINCLUDE%
%EDITTHIS%

The mass storage system at Fermilab has three major components: 
   1. _Enstore_, the principal mass storage component; Enstore provides access to data on tape or other storage media both local to a user&amp;#8217;s machine and over networks. 
   1. A _namespace implemented by PNFS_ which presents the storage media library contents as though the data files existed in a hierarchical UNIX file system. 
   1. _dCache_, a data file caching system; dCache is implemented as a front-end to Enstore. 

---++ About Enstore 
Enstore is the mass storage system implemented at Fermilab as the primary data store for large data sets. Its design was inspired by the mass storage architecture at DESY, and it originates from discussions with the DESY designers. Enstore is designed to provide high fault tolerance and availability sufficient for the RunII data acquisition needs, as well as easy administration and monitoring. It uses a client-server architecture which provides a generic interface for users and allows for hardware and software components that can be replaced and/or expanded. 

Enstore has two major kinds of software components: 

   1. Enstore servers, which are software modules that have specific functions, e.g., maintain database of data files, maintain database of storage volumes, maintain configuration, look for error conditions and sound alarms, communicate user requests down the chain to the tape robots, and so on. See Chapter 8: Overview of the Enstore Servers. 
   1. encp, a program for copying files directly to and from the mass storage system. [[#CopyingFilesWithEncp][Copying Files with Encp]]. Enstore can be used directly only from on-site machines. Off-site users are restricted to accessing Enstore via dCache, and in fact on-site users are encouraged to go through dCache as well. 

Enstore supports both automated and manual storage media libraries. It allows for a larger number of storage volumes than slots. It also allows for simultaneous access to multiple volumes through automated media libraries. There is no preset upper limit to the size of a data file in the enstore system; the actual size is limited by the physical resources. The lower limit on the file size is zero. The upper limit on the number of files that can be stored on a single volume is about 5000. 

Enstore allows users to search and list contents of media volumes as easily as they search native file systems. The stored files appear to the user as though they exist in a mounted UNIX directory. The mounted directory is actually a distributed virtual file system in PNFS namespace containing metadata for each stored file. Enstore eliminates the need to know volume names or other details about the actual file storage. 

Users typically access Enstore via the dCache caching system. The protocols supported by dCache include dccp, gridftp (globus-url-copy), kerberized ftp and weakly-authenticated ftp (these are described in Chapter 5: Using the dCache to Copy Files to/from Enstore). On-site users may bypass dCache and use the encp program, the Enstore copy command roughly modeled on UNIX&amp;#8217;s cp, to copy files directly to and from storage media. 

There are several installed Enstore systems at Fermilab. Currently these include CDFEN for CDF !RunII, D0EN for D0 !RunII, and STKEN for all other Fermilab users. [[http://hppc.fnal.gov/enstore/][Web-based monitoring for the Enstore systems]] is available. Currently, all storage libraries are tape libraries. The Computing Division operates and maintains the tape robots, slots, and other tape equipment, but for the present, experiments provide and manage their own volumes. 

---++ About the PNFS Namespace 
PNFS is a virtual file system package that implements the Enstore namespace. It was written at DESY. PNFS is mounted like NFS, but it is a virtual file system only. It maintains file grouping and structure information via a set of tags in each directory. The encp program communicates this information between PNFS and the Enstore servers when it uploads or downloads a data file. 

   * %ICON{&quot;info&quot;}% *%GREEN%NOTE:%ENDCOLOR%* There are some exceptions; arrangements for PNFS mounting have been made for some experiments whose systems are managed by the Computing Division, e.g., soudan.org for Minos. 

PNFS can only be mounted on machines that are physically at the lab1. When a user copies a data file from disk to the Enstore system, he or she specifies its destination in terms of a PNFS directory. The data file gets copied to a storage volume (selected according to the tags of the specified PNFS directory) and a corresponding metadata entry is created in the PNFS directory. This entry takes the name given in the encp command line or in the protocol-specific dCache command. It contains metadata about the data file, including information about the file transfer, the data storage volume on which the data file resides, the file&amp;#8217;s location on the volume, and so on. 

To browse file entries in the Enstore system, on-site users can mount their experiment&amp;#8217;s PNFS storage area on their own computers, and interact with it using standard non-I/O UNIX operating system utilities (see section 4.1 UNIX Commands You can Use in PNFS Space). Normal UNIX permissions and administered export points are used for preventing unauthorized access to the name space. 

---++ About dCache  
The dCache was originally designed as a front-end for a set of Hierarchical Storage Managers (HSMs), namely Enstore, EuroGate and DESY&amp;#8217;s OSM. (It has since been further developed and can be implemented stand-alone. We do not address the stand-alone functionality in this manual.) When used as a front-end to an HSM, dCache can be viewed as an intermediate &amp;#8220;relay station&amp;#8221; between client applications and the HSM (Enstore, in our case). Client systems communicate with dCache via any of a number of protocols, listed in 1.3.3 Protocols for Communicating with dCache. DCache communicates with Enstore (in a manner transparent to the user) via a high-speed ethernet connection. The dCache decouples the potentially slow network transfer (to and from client machines) from the fast storage media I/O in order to keep Enstore from bogging down. 

Data files uploaded to the dCache from a user&amp;#8217;s machine are stored on highly reliable RAID disks pending transfer to Enstore. Files already written to storage media that get downloaded to the dCache from Enstore are stored on ordinary disks. 

The dCache is installed at Fermilab on a server machine on which the /pnfs root area is mounted. Since PNFS namespace can only be mounted on machines in the fnal.gov domain, off-site users may only access Enstore via the dCache. On-site users are strongly encouraged to go through the dCache as well. We discuss dCache in more depth in Chapter 5: Using the dCache to Copy Files to/from Enstore. 

[[http://www-dcache.desy.de][More general information about dCache.]] 


---+++ Advantages 
The principal advantages of using the dCache are: 

   * Optimized usage of existing tape drives due to transfer rate adaption. 
   * Possible usage of slower and cheaper drive technology without overall performance reduction. 
   * Optimized usage of the robot systems by coordinated read and write requests. 
   * Better usage of network bandwidth by exploring the best location for the data. 
   * No explicit staging required to access the data. 
   * Ability to do posix-like IO reads and writes to data files instead of transferring entire files. 
   * Working ROOT interfaces. 
   * Tapeless data methods, raw data to reconstruction to analysis to users. 
   * Written to tape as &#39;by-product&#39;; no tape delays. 
   * Pnfs does not have to be mounted for access to the data. 
   * Same access to storage system, on and off site. Strong authentication, both gss and gsi to the data. Native and ftp access to the data. 
   * The access methods for data would be uniform, independent of data&amp;#8217;s media location. 
   * Even without the back-end HSM (e.g., Enstore), the dCache system could be seen as a huge data store with a unique namespace and standardized access methods. Care will be taken that valuable data resides on safe disks as long as no HSM copy exists. Back-end storage to the HSM can be done regularly (policy based) or by manual intervention only. 
   * A joint DESY-FNAL effort makes the use of manpower more efficient and guarantees continued support and maintenance of the developed software. 

---+++ Protocols for Communicating with dCache 
Whenever an application needs to talk to the dCache, it has to choose an appropriate door into the system. There are a number of different dCache doors through which users/applications can send requests to Enstore. Doors are protocol converters from the dCache point of view, and they are responsible for strong authentication, as necessary. One door may be for Kerberized ftp read/write access, another for dcap (dCache native C API), gridftp, weakly authenticated ftp read-only access, and so on. Each experiment determines which door(s) its experimenters may use, and communicates this information to the Enstore administrators who manage the doors&amp;#8217; configurations. Most doors are for native transfers, and are local. See Chapter 5: Using the dCache to Copy Files to/from Enstore for more information. 

---+ Overview of the Enstore Servers

In this chapter we describe the software modules that act as Enstore servers and the libraries with which they interact. The servers include: 

   * File Clerk (FC) 
   * Volume Clerk (VC) 
   * Library Manager (LM) 
   * Mover (MV) 
   * Media Changer (MC) 
   * Configuration Server (CS) 

All of the above-listed servers must be running in order for data reads and writes to succeed. 

The Enstore monitoring framework includes:

   * Inquisitor 
   * Alarm Server (AS) 
   * Log Server (LS) 
   * Event Relay (ER) 
   * Monitor Server 
   * Accounting Server 
   * Drivestat Server 
   * Information Server 

Typically, data transfer can still take place even if any of these monitoring systems is down. 


---++ File Clerk 
The File Clerk (FC) is a server that tracks files in the system. It manages a database of metadata for each data file in the Enstore system. The metadata includes the file&amp;#8217;s name, its unique identifier (the bit file ID, or bfid, that the 

FC itself assigns to each new file ), the volume on which it resides, and so on. You can get information on specific files using the enstore info command; see section 9.1 enstore info. 


---++ Volume Clerk 
The Volume Clerk (VC) is a server that stores and administers storage volume (tape) information. You can get information on specific volumes using the enstore info command; see section 9.1 enstore info.


---++ Library Manager 
A Library in Enstore is comprised of both the physical media and a robot arm used to mount the media in attached drives. An Enstore library is typically called a robot. A library/robot interfaces to software that controls the robot arm (the Media Changer, see section 8.5). Each library can contain a variety of media types and employ different types of media drives.

A Virtual Library (VL) is a subset of an Enstore library. It can contain one and only one type of media. A Library Manager (LM) is a server which is bound to a single Virtual Library (VL), and controls what happens within that VL. We speak of bound &amp;#8220;LM-VL pairs&amp;#8221;. An LM receives requests for file reads and writes from the user, stores these unassigned requests in a queue, prioritizes them, and dispatches the requests to a Mover for actual data transfer to and from its VL. 

There may be many LM-VL pairs in an Enstore system. There may be more than one LM-VL pair for each media type, but not vice-versa. For example, given an STK Powderhorn library holding 20, 60 and 200 GB media, Enstore would need to divide it into at least three LM-VL pairs. 

You can get information on specific library managers using the enstore library command; see section 9.2 enstore library. 


---++ Mover 
A Mover (MV) is a process responsible for efficient data transfer between the encp process and a single, assigned media drive in a library (robot). The Mover receives instructions from a Library Manager (LM) on how to satisfy the users&amp;#8217; requests. The Mover sends instructions to the Media Changer (MC) (described in section 9.5) that services the Mover&#39;s assigned drive in order to get the proper volume mounted. 

A mover can be configured to serve multiple LMs. Allowing flexible LM assignment has two benefits: 

   * First, since a virtual library (an LM-VL pair) handles only one type of media, a drive which can handle multiple types of media (e.g., different capacity media) can be shared by multiple LM-VL pairs without a static partitioning of the system. 
   * Secondly, suppose user groups A and B want to share the capacity of a VL, in which half the tapes belong to group A and the other half to group B. You want to guarantee that groups A and B each get one third of the tape drives, and that the last third is shared. To do this, your administrator can configure the Movers to partition resources in the VL, and assign an LM to each type of use. 

   $ %ICON{&quot;tip&quot;}% *%GREEN%NOTE%ENDCOLOR%*: The media types governed by the LMs must be supported by the Mover&amp;#8217;s assigned drive. 
   
---++ Media Changer 
The Media Changer (MC) mounts and dismounts the media into and out of drives according to requests from the Movers. One MC can serve multiple drives and thus multiple VLs (the image in section 8.4 Mover shows an MC associated with only one drive). When the drives are in the robot, the MC is the interface to the robotic software. 


---++ Configuration Server 
The Configuration Server (CS) maintains and distributes the information about Enstore system configuration, such as the location and parameters of each Enstore component and/or server. At startup, each server asks the CS for the information it needs (e.g., the location of any other server with which it must communicate). New configurations can be loaded into the CS without disturbing the current running system. 


---++ Inquisitor
The Inquisitor monitors the Enstore servers, obtains information from them, and creates reports at regular intervals that can be viewed on the web under http://hppc.fnal.gov/enstore/. See section 8.10 Event Relay for an illustration of an Inquisitor task. The reports created by the Inquisitor include Enstore Server Status (section 10.4), Encp History (section 10.10), Enstore Configuration (section 10.11), and Enstore Log Files (section 10.13). 

If the Inquisitor goes down, the System-At-A-Glance web page (described in section 10.3) indicates this by a red ball next to Inquisitor. In this case, data can still be transferred via encp, however, the information on the reports mentioned above doesn&amp;#8217;t continue to update! 


---++ Alarm Server 
The Alarm Server (AS) maintains a record of alarms raised by other servers, and creates a report available online and described in section 10.12 Enstore Active Alarms. Since Enstore attempts error recovery whenever possible, it is expected that raised alarms will need human intervention to correct the problem. The AS compares each newly raised alarm with the previously raised ones (it raises a counter) in order to prevent raising the same alarm more than once. Alarm output can be configured to be sent in email messages, to a web page, and so on for notification. 


---++ Log Server 
The Log Server (LS) receives messages from other processes and logs them into formatted log files available online and described in section 10.13 Enstore Log Files. These messages are transactional records. Log files are labeled by date. Every night at midnight, the currently opened log file gets closed and another one is opened. Logs are backed up to tape. 


---++ Event Relay
The Event Relay (ER) is a server that forwards messages based on subscription. All the Enstore servers send messages to the ER. Any server may &amp;#8220;subscribe&amp;#8221; to the ER in order to have messages of particular types forwarded to it. 

For example, the ER periodically receives &amp;#8220;I&amp;#8217;m alive&amp;#8221; messages (called heartbeats) from the other servers in the system. The Inquisitor (section 8.7) subscribes to the heartbeat messages, so the ER forwards these messages to it. This is illustrated in the image below: 

If the ER goes down (indicated by a red ball next to Event Relay on the System-At-A-Glance web page described in section 10.3), the information on EnstoreServerStatus and the other web pages described in [[EnstoreMonitoringEnstoreOnTheWeb][Monitoring Enstore on the Web]] may not be accurate. 

---++ Monitor Server 
The Monitor Server (MS) is available for investigating network-related problems. It attempts to mimic the communication between encp, the corresponding library manager, and the mover. To initiate a test of this kind, you must use the enstore monitor command, see section 9.3 enstore monitor. 


---++ Accounting Server
The accounting server is a front-end to a Postgres SQL database, and at this time is not intended for use by end users. It maintains statistical information on a running system. Such information is not essential to operations, however it can be used by admins to analyze the performance and utilization of the system for purposes of troubleshooting and future planning. 


---++ Drivestat Server 
Drivestat server maintains statistical information of the drives. This is used to update the &amp;#8220;ingest rate&amp;#8221; plots. As with the accounting server, such information is not essential to operations, however it can be used to analyze the performance and utilization of the system for purposes of troubleshooting and future planning. 


---++ Info Server 
The Information Server is a read-only server that maintains detailed file and volume information. You can access this information for particular files and volumes using the enstore info command and its various options (see section 9.1 enstore info). 


%STOPINCLUDE%

%BOTTOMMATTER%


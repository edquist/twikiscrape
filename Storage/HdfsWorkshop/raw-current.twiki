---+ HdfsWorkshop, March 11-13, UCSD


---++ Logistics

Information concerning travel and accommodations can be found at the webpage for the recent
[[http://hepuser.ucsd.edu/twiki2/bin/view/UCSDTier2/SandCweekApril2009][Software and Computing]] meeting at UCSD.
   * UCSD will host a hands-on workshop focused on integration of hdfs (the Hadoop file system) at the USCMS Tier2s from March 11 to March 13, 2009.
   * Most attendees will be arriving around noon on Wednesday.  Most will be at the Sheraton La Jolla.

Remote attendees can connect to the &quot;Hadoop Workshop&quot; EVO meeting.

---++ Agenda

All times PDT.

   * Meet Wednesday noon at Mayer Hall 5517 (Terrence&#39;s office). 
      * start with lunch, in case either Will or Brian are delayed with their flights.
      * Followed by the TODO as it is below.

   * Wednesday: Overview
      * Introductory remarks from Ken Bloom (30 min)
      * Reality check: Time to ask upfront questions and let sites define any goals they have for the meeting. (30 min)
         * Small roundtable of &quot;what we want to do with Hadoop (and why)&quot; from each site.
      * Detailed outline of Hadoop components (logfiles, what&#39;s installed where, etc). (1 hr)
         * Demonstration and outline of the Nebraska system

   * Thursday: Practical
      * Rocks integration (0930 - 1000)
         * RPMs of hadoop, fuse packages
         * Rocks kickstart xml fragments
         * Hands-on help integrating Hadoop with existing Rocks clusters
         * Todo: build hadoop rpm from source
         * Todo: integration bestman/gridftp (RPMS + kickstart xml)
      * Installation
         * Help folks install small evaluation testbeds.
         * Get all the important components going (HDFS, FUSE, GridFTP, SRM)
         * Demonstrate file transfers and CMSSW usage
      * Management and transition notes from UNL (1 hr)
      * Packaging and documentation (30 min)
      * Upgrade (30 min)
      * Monitoring
         * JMX.  Make sure everyone can run JMX console.
         * Ganglia.  Make sure everyone&#39;s HDFS cluster gets integrated into Ganglia
         * Nagios.  Share Nebraska probes and offer advice from Nerbaska.
         * Monalisa. Share custom monalisa+ganglia module from Caltech

   * Friday: Verification
      * SE tests and configuration
      * Demonstrate reliability of HDFS in various circumstances: (30 min)
         * restart of namenode
         * restoration from checkpoint/backup
         * client behavior during namenode restart
         * log atomicity during namenode failure (=kill -9=)
      * Test Scalability / Inter-site PhEDEx transfers?  (1 hr)
      * Plan a &quot;storage challenge&quot; to be held before data taking?
      * Develop best practices:
         * Replication policy
         * Service layout
         * Monitoring
         * Namenode checkpointing/backup policy
         * Logging (?)

---++ Action Items
   * Provide packaging for the VDT

---++ Coordination

   * Jabber/XMPP: uscms-t2@conference.fnal.gov (multi-user chat)

---++ Other Resources

   * [[http://t2.unl.edu/documentation/hadoop][UNL documentation]]


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> HadoopPhedex &lt; Storage &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Storage/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Storage/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Storage/HadoopPhedex?_T=16 Feb 2017" type="application/x-wiki" title="edit HadoopPhedex" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/bin/view/Storage/WebRss" />    
<base href="https://twiki.opensciencegrid.org/bin/view/Storage/HadoopPhedex"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#0000FF;
	}
	.patternBookView {
		border-color:#0000FF;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">



</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <h1><a name="Hadoop_Phedex"></a>  <strong><noop>Hadoop Phedex</strong> </h1>
<p />
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Use_the_FUSE_mount_on_the_phedex"> Use the FUSE mount on the phedex node</a>
</li> <li> <a href="?cover=print#Create_leading_directories_in_Fi"> Create leading directories in FileDownloadDelete</a>
</li> <li> <a href="?cover=print#Delete_load_test_files_after_tra"> Delete load test files after transferring</a>
</li> <li> <a href="?cover=print#Example_FileDownloadDeleteHadoop"> Example FileDownloadDeleteHadoop</a>
</li> <li> <a href="?cover=print#Example_FileDownloadVerifyHadoop"> Example FileDownloadVerifyHadoop</a>
</li> <li> <a href="?cover=print#Example_agent_config_for_Load_Te"> Example agent config for Load Tests</a>
</li></ul> 
</div>
<p />
<p />
The SRM interface to Hadoop is through the Bestman SRM server.  This SRM server has some slight differences in semantics compared with the dCache SRM server.  Most importantly, the Bestman SRM server does not create the parent directories for file copy operations.  The dCache SRM server violates this part of the specification.
<p />
The remainder of this document describes some suggested modifications to your phedex agents to work better with Hadoop and Bestman.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Use_the_FUSE_mount_on_the_phedex"></a> Use the FUSE mount on the phedex node </span></h2>
<p />
Each invocation of the srm client tools launches a new JVM.  When many file operations occur at the same time (copy, delete, verify) then this can quickly bring a phedex server to its knees. Many sites mount the dCache pnfs filesystem on the phedex node to avoid the need to run <code>srmrm</code> to delete files.  Similarly with Hadoop, if you mount the HDFS filesystem with fuse on the phedex server, you can delete files with <code>rm</code> command instead of <code>srmrm</code>.  Even if you don't use the fuse mount on the phedex server, you can still use the Hadoop CLI to interact with the Hadoop filesystem.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Create_leading_directories_in_Fi"></a> Create leading directories in <span class="twikiNewLink">FileDownloadDelete<a href="/bin/edit/Storage/FileDownloadDelete?topicparent=Storage.HadoopPhedex" rel="nofollow" title="FileDownloadDelete (this topic does not yet exist; you can create it)">?</a></span> </span></h2>
<p />
Since Bestman SRM does not create the leading directories for file transfers, the phedex agents must do this.  The <code>FileDownload</code> agent can be configured to invoke the <code>FileDownloadDelete</code> script before transferring a file.  It is here that you will want to create the leading directories.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Delete_load_test_files_after_tra"></a> Delete load test files after transferring </span></h2>
<p />
This isn't specific to Hadoop, but is listed here because more than one site has reported this problem.  If <span class="twikiNewLink">PhEDEx<a href="/bin/edit/Storage/PhEDEx?topicparent=Storage.HadoopPhedex" rel="nofollow" title="PhEDEx (this topic does not yet exist; you can create it)">?</a></span> load test files are not cleaned up after the transfer is finished, then you will quickly fill up HDFS with junk files.  In the <span class="twikiNewLink">PhEDEx<a href="/bin/edit/Storage/PhEDEx?topicparent=Storage.HadoopPhedex" rel="nofollow" title="PhEDEx (this topic does not yet exist; you can create it)">?</a></span> config for the load test agents, you can pass a <code>-d</code> flag to the verify script.  The default <span class="twikiNewLink">FileDownloadVerify<a href="/bin/edit/Storage/FileDownloadVerify?topicparent=Storage.HadoopPhedex" rel="nofollow" title="FileDownloadVerify (this topic does not yet exist; you can create it)">?</a></span> agent should check for the existence of this flag and delete the file after performing the verification.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Example_FileDownloadDeleteHadoop"></a> Example <span class="twikiNewLink">FileDownloadDeleteHadoop<a href="/bin/edit/Storage/FileDownloadDeleteHadoop?topicparent=Storage.HadoopPhedex" rel="nofollow" title="FileDownloadDeleteHadoop (this topic does not yet exist; you can create it)">?</a></span> </span></h2>
<p />
<pre>
#!/bin/sh

# Make sure you use java 1.6 if you are invoking the Hadoop cli.
JAVA&#95;HOME&#61;/usr/java/default
PATH&#61;$JAVA&#95;HOME/bin:$PATH

# Pick up arguments
reason&#61;&#34;$1&#34; fullsrmpath&#61;&#34;$2&#34;

# message
echo &#34;Removing $fullsrmpath&#34;

# Rewrite PFN to form that&#39;s useful to our commands
pfn&#61;`echo $fullsrmpath &#124; sed &#39;s&#124;srm://.&#42;/srm/v2/server?SFN&#61;&#124;&#124;&#39;`
echo &#34;&#42;&#42;&#42;Extracted pfn: $pfn&#34;
hadoop&#95;path&#61;`echo $pfn &#124; sed -e &#34;s&#124;/mnt/hadoop&#124;&#124;&#34;`
echo &#34;&#42;&#42;&#42;Extracted hadoop path: $hadoop&#95;path&#34;

# Handle removal.  We remove destination both before transfer and
# after a failed transfer.
case $reason in
  pre )
        # You have the choice of using the rm command, or the Hadoop cli
        # to remove the file.

        echo &#34;Executing: hadoop fs -rm $hadoop&#95;path&#34;
        hadoop fs -rm $hadoop&#95;path
        #echo &#34;Executing: rm -f $pfn&#34;
        #rm -f $pfn


        # Create the leading directories that are not automatically created
        # by the Bestman SRM server.
        dir&#61;`dirname $hadoop&#95;path`
        echo &#34;Creating parent directories with &#39;hadoop fs -mkdir $dir&#39;&#34;
        hadoop fs -mkdir $dir
        #dir&#61;`dirname $pfn`
        #echo &#34;Creating parent directories with &#39;mkdir -p $dir&#39;&#34;
        #hadoop fs -mkdir $dir
        ;;
  post )  
        echo &#34;Executing: hadoop fs -rm $hadoop&#95;path&#34;
        hadoop fs -rm $hadoop&#95;path
        #echo &#34;Executing: rm -f $pfn&#34;
        #rm -f $pfn
        ;;
  &#42; ) echo &#34;unrecognised reason to remove $pfn: $reason&#34; 1&#62;&#38;2; exit 1
        ;;
esac

exit 0

</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Example_FileDownloadVerifyHadoop"></a> Example <span class="twikiNewLink">FileDownloadVerifyHadoop<a href="/bin/edit/Storage/FileDownloadVerifyHadoop?topicparent=Storage.HadoopPhedex" rel="nofollow" title="FileDownloadVerifyHadoop (this topic does not yet exist; you can create it)">?</a></span> </span></h2>
<p />
<pre>
#!/bin/sh

# Make sure you use java 1.6 if you are invoking the Hadoop cli.
JAVA&#95;HOME&#61;/usr/java/default
PATH&#61;$JAVA&#95;HOME/bin:$PATH

# Process command line arguments
do&#95;checksum&#61;false do&#95;delete&#61;false do&#95;force&#61;false
while &#91; $# -ge 1 ]; do
  case $1 in
    -c ) do&#95;checksum&#61;true; shift ;;
    -d ) do&#95;delete&#61;true; shift ;;
    -f ) do&#95;force&#61;true; shift ;;
    -&#42; ) echo &#34;unrecognised option $1&#34; 1&#62;&#38;2; exit 5 ;;
    &#42;  ) break ;;
  esac
done

# Pick up arguments
status&#61;&#34;$1&#34; fullsrmpath&#61;&#34;$2&#34; size&#61;&#34;$3&#34; checksum&#61;&#34;$4&#34;
validity&#61;0

# Rewrite PFN to a form that&#39;s useful to our commands
pfn&#61;`echo $fullsrmpath &#124; sed &#39;s&#124;srm://.&#42;/srm/v2/server?SFN&#61;&#124;&#124;&#39;`
echo &#34;&#42;&#42;&#42;Extracted pfn: $pfn&#34;
hadoop&#95;path&#61;`echo $pfn &#124; sed -e &#34;s&#124;/mnt/hadoop&#124;&#124;&#34;`
echo &#34;&#42;&#42;&#42;Extracted hadoop path: $hadoop&#95;path&#34;

# Check file size and mark file invalid on mismatch.
if &#91; $validity &#61; 0 ]; then
# Take your pick.  You can either use the Hadoop CLI to get the file size
# or use the fuse mount.
  disksize&#61;`hadoop fs -ls &#34;$hadoop&#95;path&#34; &#124; tail -1 &#124; awk &#39;{print $5}&#39;`
#  disksize&#61;`ls -l &#34;$pfn&#34; &#124; awk &#39;{print $5}&#39;`
  if &#91; &#34;$?&#34; !&#61; 0 ] ; then
      echo &#34;Failed to get file size&#34;
      validity&#61;1
  else
      &#91; X&#34;$disksize&#34; !&#61; X&#34;$size&#34; ] &#38;&#38; echo &#34;size mismatch disk&#61;$disksize db&#61;$siz
e guid&#61;$guid pfn&#61;$pfn&#34; &#38;&#38; validity&#61;2
      &#91; X&#34;$disksize&#34; &#61;&#61; X&#34;$size&#34; ] &#38;&#38; echo &#34;Success!!!  pfn&#61;$pfn&#34;
  fi
fi

echo &#34;&#42;&#42;&#42;disksize: $disksize ----- &#42;&#42;&#42;size: $size   for $pfn&#34;

# If file deletion was requested, delete the file.
$do&#95;delete &#38;&#38; echo &#34;deleting $pfn&#34;
#$do&#95;delete &#38;&#38; rm -f &#34;$pfn&#34; 2&#62;/dev/null
$do&#95;delete &#38;&#38; hadoop fs -rm &#34;$hadoop&#95;path&#34; 2&#62;/dev/null

# If we are forcing true return value, lie about it all
$do&#95;force &#38;&#38; validity&#61;0

# Return file validity
exit $validity

</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Example_agent_config_for_Load_Te"></a> Example agent config for Load Tests </span></h2>
<p />
<pre>
### AGENT LABEL&#61;download-debug-ucsd PROGRAM&#61;Toolkit/Transfer/FileDownload
 -db              ${PHEDEX&#95;DBPARAM}
 -nodes           ${PHEDEX&#95;NODE}
 -delete          ${PHEDEX&#95;CONF}/FileDownloadDeleteHadoop
 -validate        ${PHEDEX&#95;CONF}/FileDownloadVerifyHadoop,-d
 -accept          &#39;T2&#95;US&#95;UCSD&#39;
 -backend         SRM
 -protocols       srmv2,srm
 -command         srmcp,-pushmode&#61;true,-debug&#61;true,-retry&#95;num&#61;2,-protocols&#61;gsiftp,-srm&#95;protocol&#95;version&#61;2
 -batch-files     2
 -jobs            5
 -timeout         10800
</pre>
<p />
<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 <code><b>===============</b></code>
<p />
 Thank you for claiming ownership for this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Storage/FirstLast?topicparent=Storage.HadoopPhedex" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local OWNER = <span class="twikiNewLink">DouglasStrain<a href="/bin/edit/Storage/DouglasStrain?topicparent=Storage.HadoopPhedex" rel="nofollow" title="DouglasStrain (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (<span class="twikiNewLink">ComputeElement<a href="/bin/edit/Storage/ComputeElement?topicparent=Storage.HadoopPhedex" rel="nofollow" title="ComputeElement (this topic does not yet exist; you can create it)">?</a></span>|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3) <ul>
<li> Local DOC_AREA       = Storage
</li></ul> 
<p />
 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (<span class="twikiNewLink">EndUser<a href="/bin/edit/Storage/EndUser?topicparent=Storage.HadoopPhedex" rel="nofollow" title="EndUser (this topic does not yet exist; you can create it)">?</a></span>|Student|Developer|SysAdmin|VOManager) <ul>
<li> Local DOC_ROLE       = <span class="twikiNewLink">SysAdmin<a href="/bin/edit/Storage/SysAdmin?topicparent=Storage.HadoopPhedex" rel="nofollow" title="SysAdmin (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge) <ul>
<li> Local DOC_TYPE       = Troubleshooting  Please define if this document in general needs to be reviewed before release ( 1 | 0 )
</li> <li> Local INCLUDE_REVIEW = 1
</li></ul> 
<p />
 Please define if this document in general needs to be tested before release ( 1 | 0 ) <ul>
<li> Local INCLUDE_TEST   = 0
</li></ul> 
<p />
 change to 1 once the document is ready to be reviewed and back to 0 if that is not the case <ul>
<li> Local REVIEW_READY   = 1
</li></ul> 
<p />
 change to 1 once the document is ready to be tested and back to 0 if that is not the case <ul>
<li> Local TEST_READY     = 0
</li></ul> 
<p />
 change to 1 only if the document has passed the review and the test (if applicable) and is ready for release <ul>
<li> Local RELEASE_READY  = 1
</li></ul> 
<p />
<p />
 DEAR DOCUMENT REVIEWER
 <code><b>==================</b></code>
<p />
 Thank for reviewing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Storage/FirstLast?topicparent=Storage.HadoopPhedex" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local REVIEWER       = <span class="twikiNewLink">NehaSharma<a href="/bin/edit/Storage/NehaSharma?topicparent=Storage.HadoopPhedex" rel="nofollow" title="NehaSharma (this topic does not yet exist; you can create it)">?</a></span> Please define the review status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 )
</li> <li> Local REVIEW_PASSED  = 2
</li></ul> 
<p />
<p />
 DEAR DOCUMENT TESTER
 <code><b>================</b></code>
<p />
 Thank for testing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Storage/FirstLast?topicparent=Storage.HadoopPhedex" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local TESTER         = <span class="twikiNewLink">NehaSharma<a href="/bin/edit/Storage/NehaSharma?topicparent=Storage.HadoopPhedex" rel="nofollow" title="NehaSharma (this topic does not yet exist; you can create it)">?</a></span> Please define the test status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 )
</li> <li> Local TEST_PASSED    = 0
</li></ul> 
############################################################################################################
--></div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Storage<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span><a href="/bin/view/Storage/WebHome" class="twikiCurrentWebHomeLink twikiLink">WebHome</a> &gt; <a href="/bin/view/Storage/Hadoop" class="twikiLink">Hadoop</a><span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>HadoopPhedex</span> <br />    
Topic revision: r12 - 06 Dec 2016 - 18:13:14 - <a href="/bin/view/Main/KyleGross" class="twikiLink">KyleGross</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
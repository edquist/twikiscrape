---+!! *&lt;noop&gt;%SPACEOUT{ &quot;%TOPIC%&quot; }%*
%TOC%

---+ Hadoop Deployments at CMS Tier2 Sites

*Purpose*: The purpose of this page is to document the technical details of Hadoop deployments at various sites.

---++ Wisconsin
---+++ Networking
Our HDFS machines are connected by single gigabit Ethernet to one of four stacks of Cisco 3750G or -E switches. The stackes have redundant 10 gigabit fiber Etherchannel uplinks or interconnects.
---+++ Machine Types
We have a variety of machines in our cluster, ranging from dedicated 4U systems with 24 or 36 SATA disks to (old) 1U systems with 3 IDE disks. The 1U systems also run Condor and both the 1U and 4U systems may also run gridftp and xrootd.
---+++ Disk Layout
On all of our machines, including the dedicated 4Us, we expose each disk as a single datadir to HDFS. That means that our 4Us have lots of small RAID0 volumes (one for each disk). All of our disks are mounted under /data and automatically tested and added or removed from the HDFS configuration by a script run from cron. Each disk has a single ext3 partition.
---++ UCSD
---+++ Networking
All hadoop nodes are currently connected via 1Gbps links into a Cisco 6509. We also have deployed a 10Gbps Copper module in our Cisco 6509 as we may move to a sub switch in certain racks and deploy 10Gbps cards in those nodes.
---+++ Machine Types
All nodes have CPU and Storage. This allows us to save space and lower costs for things like Motherboards and chassis since we only purchase that once. Current nodes use an Intel chassis, an SSD for the OS, 12x 3TB disks in a JBOD, two 6-core processors, and 48GB of memory.
---+++ Disk Layout
We deploy our disks in a JBOD format and allow Hadoop to perform node level replication. This allows us to lose 1 disk in a node and only lose data on that single disk instead of the whole array. As well we are already mirroring so any additional RAID just consumes disks we could use for storage.
---++ Cern
---+++ Machine Types and Disk Layout
We deploy dedicated storage nodes of which we have three types. All are 4U boxes with either 24x 750GB disks, 24x 2TB disks or 36x 3TB disks. All nodes run Solaris (we have a variation of Solaris 10, OpenSolaris, OpenIndiana, but basically it&#39;s all the same). All nodes have 7+1 or 8+1 RAIDZ volumes that are exposed as data dirs to hadoop and we run most of the hadoop as replication factor = 1 except for crucial files like /store/user, /store/unmerged that are unique to us. The storage amounts to 15TB / 35TB 80TB per node depending on the config. All nodes have 1-2 hot global hot spares.
---+++ Networking
The interconnect right now is 1Gbit shared with worker nodes, but some of the storage nodes have 2-4 links in aggregation. I&#39;m currently planning a tender to move the whole interconnect of our Tier 2 to 10G with possibly trunked 10G links going to the storage nodes (especially the ones running 80TB of stuff per node).
---++ Purdue
Details coming soon.


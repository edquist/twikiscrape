<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> HadoopUnderstanding &lt; Storage &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Storage/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Storage/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Storage/HadoopUnderstanding?_T=16 Feb 2017" type="application/x-wiki" title="edit HadoopUnderstanding" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/bin/view/Storage/WebRss" />    
<base href="https://twiki.opensciencegrid.org/bin/view/Storage/HadoopUnderstanding"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#0000FF;
	}
	.patternBookView {
		border-color:#0000FF;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">



</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <h1><a name="Hadoop_Understanding"></a>  <strong><noop>Hadoop Understanding</strong> </h1>
<p />
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Understanding_and_Planning_Your"> Understanding and Planning Your Hadoop Install</a> <ul>
<li> <a href="?cover=print#Hadoop_Introduction"> Hadoop Introduction</a>
</li> <li> <a href="?cover=print#Hadoop_SE_Components"> Hadoop SE Components</a>
</li> <li> <a href="?cover=print#Recommended_Hardware"> Recommended Hardware</a>
</li> <li> <a href="?cover=print#Minimal_Installation_0_50TB_WAN"> Minimal Installation (0-50TB, WAN transfers up to 1Gbps)</a>
</li> <li> <a href="?cover=print#Medium_Installation_50_150TB_WAN"> Medium Installation (50-150TB, WAN transfers up to 2 Gbps)</a>
</li> <li> <a href="?cover=print#Large_Installation_150TB_WAN_tra"> Large Installation (&gt;150TB, WAN transfers over 2 Gbps)</a>
</li></ul> 
</li> <li> <a href="?cover=print#Hadoop_Security"> Hadoop Security</a>
</li> <li> <a href="?cover=print#Comments"> Comments</a>
</li></ul> 
</div>
<p />
Please note that this page has moved to: <a href="/bin/view/Documentation/HadoopUnderstanding" class="twikiLink">HadoopUnderstanding</a>.  Please use that page for newer information.
<p />
<h1><a name="Understanding_and_Planning_Your"></a> Understanding and Planning Your Hadoop Install </h1>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Hadoop_Introduction"></a> Hadoop Introduction </span></h2>
<p />
Hadoop is a data processing framework.  It is an open-source <a href="http://hadoop.apache.org" target="_top">Apache Foundation project</a>, and the main contributor is Yahoo!  The framework has two main parts - job scheduling and a distributed file system, the Hadoop Distributed File System (HDFS).  We currently utilize HDFS as a general-purpose file system.  For this document, we'll use the words "Hadoop" and "HDFS" interchangeably, but it's nice to know the distinction.
<p />
We recommend starting with <a href="http://hadoop.apache.org/hdfs/docs/current/hdfs_design.html" target="_top">HDFS architecture document</a>.
<p />
Please do read through this. We will assume you have read this, or at least the important architectural portions.  The file system is block-oriented; each file is broken up into 64 MB or 128 MB chunks (user configurable).  These chunks are stored on data nodes and served up from there; the central namenode keeps track of the block locations, the namespace information, and block placement policies.  HDFS provides POSIX-like semantics; it provides fully random-access reads and non-random-access writes.  Currently, fsync and appends (after the file has been initially closed) are experimental and not available to OSG-based installs.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Hadoop_SE_Components"></a> Hadoop SE Components </span></h2>
<p />
We broadly break down the server components of the Hadoop SE into three categories: HDFS core, Grid extensions, and HDFS auxiliary.  The components in each of these categories are outlined below:
<p /> <ul>
<li> HDFS Core: <ul>
<li> Namenode: The core metadata server of Hadoop.  This is the most critical piece of the system, and there can only be one of these.  This stores both the file system image and the file system journal.  The namenode keeps all of the filesystem layout information (files, blocks, directories, permissions, etc) and the block locations.  The filesystem layout is persisted on disk and the block locations are kept solely in memory.  When a client opens a file, the namenode tells the client the locations of all the blocks in the file; the client then no longer needs to communicate with the namenode for data transfer.
</li> <li> Datanode: This node stores copies of the blocks in HDFS.  They communicate with the namenode to perform "housekeeping" such as creating new replicas, transferring blocks between datanodes, and deleting excess blocks.  They also communicate with the clients to transfer data.  To reach the best scalability, there should be as many datanodes as possible.
</li></ul> 
</li> <li> Grid extensions <ul>
<li> <a href="https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallOSGBestmanSE" target="_top">BeStMan SRM</a>: A generic SRM server that can be run on top of any POSIX-like filesystem.  This is run in "gateway" mode, which limits the amount of the SRM protocol implemented.  To date, this has been sufficient to LHC VOs.
</li> <li> Globus GridFTP: The standard GridFTP from Globus.  We use a plug-in module (using the Globus Direct Storage Interface) that allows the GridFTP process to use the HDFS C-bindings directly.
</li> <li> Gratia probe: <a href="/bin/view/Accounting/WebHome" class="twikiLink">Gratia</a> is an accounting system that records batch system and transfer records to a database.  The records are collected by a client program called a "probe" which runs on the GridFTP server.  It parses the GridFTP server's log files and creates transfer records.
</li> <li> Xrootd server plugin: Xrootd is an extremely flexible and powerful data server popular in the high energy physics community.  There exists a HDFS plugin for Xrootd; integrating with Xrootd provides a means to export HDFS securely outside the local cluster, as another Xrootd plugin provides GSI-based authentication and authorization.
</li></ul> 
</li> <li> HDFS auxiliary: <ul>
<li> Secondary Namenode: Perhaps more aptly called a "checkpoint server".  This server downloads the file system image and journal from the namenode, merges the two together, and uploads the new file system image up to the namenode.  This is done on a different server in order to reduce the memory footprint of the namenode.
</li> <li> Hadoop Balancer: This is a script (unlike the others, which are daemons) that runs on the namenode.  It requests transfers of random blocks between the datanodes.  This works until all datanodes have approximately the same percentage of free space.  Well-balanced datanodes are necessary for having a healthy cluster.
</li></ul> 
</li></ul> 
<p />
In addition to the server components, there are two client components:
<p /> <ul>
<li> FUSE: This allows HDFS to be mounted as a filesystem on the worker nodes.  FUSE is a Linux kernel module that allows kernel I/O calls to be translated into a call to a userspace program.  In this case, a program called fuse_dfs translates the POSIX calls into HDFS C-binding calls.
</li> <li> Hadoop Command Line Client: This command line client exposes a lot of the Unix-like calls without mounting FUSE, plus access to the non-POSIX calls (such as setting quotas and file replication levels).  For example, "hadoop fs -ls /" is equivalent to "ls /mnt/hadoop" if /mnt/hadoop is the mount point of HDFS.
</li></ul> 
<p />
<a name="HadoopStorageRecommendedHardware"></a>
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Recommended_Hardware"></a> Recommended Hardware </span></h2>
<p /> <ul>
<li> Namenode: We recommend at least 8GB of RAM (minimum is 2GB RAM), preferably 16GB or more.  A rough rule of thumb is 1GB per 100TB of raw disk space; the actual requirements is around 1GB per million objects (files, directories, and blocks).  The CPU requirements are any modern multi-core server CPU.  Typically, the namenode will only use 2-5% of your CPU. <ul>
<li> As this is a single point of failure, the <strong>most important</strong> requirement is reliable hardware rather than high performance hardware.  We suggest a node with redundant power supplies and at least 2 hard drives.
</li></ul> 
</li> <li> Secondary namenode: This node needs the same amount of RAM as the namenode for merging namespaces.  It does not need to be high performance or high reliability.
</li> <li> Datanode: Each datanode should plan to dedicate 200-500MB of RAM to HDFS.  A general rule of thumb is to dedicate 1 CPU to HDFS per 5TB of disk capacity under heavily load; clusters with moderate load (i.e., mostly sequential workflows) will need less.  At idle, HDFS will consume almost no CPU.
</li></ul> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Minimal_Installation_0_50TB_WAN"></a> Minimal Installation (0-50TB, WAN transfers up to 1Gbps) </span></h2>
<p />
The minimal installation would involve 5 nodes:
<p /> <ol>
<li> hadoop-name: The namenode for the Hadoop system.  Must be on private NAT.
</li> <li> hadoop-name2: This will run the HDFS secondary namenode.  Must also be on private NAT.
</li> <li> hadoop-data1, hadoop-data2: Two HDFS datanodes.  They will hold data for the system, so they should have sizable hard drives.  These must be on the private NAT.  As the Hadoop installation grows to many terabytes, this will be the only class of nodes one adds.
</li> <li> hadoop-grid: Runs the BeStMan SRM and Globus GridFTP server.  Must have a public interface and a private interface.
</li></ol> 
<p />
If desired, hadoop-name and hadoop-name2 may be virtualized.  Prior to installation, DNS / host name resolution <strong>must</strong> work.  That is, you should be able to resolve all the hadoop servers either through DNS or /etc/hosts.  Because of the grid software, hadoop-grid <strong>must</strong> have reverse DNS working.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Medium_Installation_50_150TB_WAN"></a> Medium Installation (50-150TB, WAN transfers up to 2 Gbps) </span></h2>
<p />
For a medium install, make the following changes over the minimal install: <ol>
<li> Run BeStMan on a separate machine, hadoop-srm.  This host may be virtualized.
</li> <li> Run multiple GridFTP servers (2 should be fine); if possible, use 10 Gbps cards for these hosts.
</li> <li> Add many more HDFS datanodes.  This usually means starting to add 1 or 2 TB hard drives to some worker nodes.
</li></ol> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Large_Installation_150TB_WAN_tra"></a> Large Installation (&gt;150TB, WAN transfers over 2 Gbps) </span></h2>
<p />
For a large installation, make the following changes: <ol>
<li> Run more GridFTP servers; plan for 800 Mbps per host with 1 Gbps card in order to have excess capacity.
</li> <li> Purchase machines suitable for HDFS datanodes.  It is possible to get a 2U box with 8-12 hard drives; for many projects, this will provide a more suitable CPU to disk ratio than the 1U boxes with 2 hard drives.
</li></ol> 
<p />
<h1><a name="Hadoop_Security"></a> Hadoop Security </h1>
<p />
HDFS has unix-like user/group authorization, but no strict authentication.  HDFS should only be exposed to a secure internal network which only non-malicious users are able to access.  For users with access to the local cluster, it is not difficult at all to bypass authentication.
<p />
<a href="http://www.cloudera.com/blog/2009/08/14/hadoop-default-ports-quick-reference/" target="_top">The default ports are listed here</a>.
<p />
There are some ways to improve security of your cluster: <ul>
<li> Keep the namenode behind a firewall.  One possibility is to run hadoop entirely on the private subnet of a cluster.
</li> <li> Use firewalls to protect the HDFS ports (default for the datanode is 50010 and 50075; for the namenode, 50070 and 9000).
</li> <li> For clusters utilizing FUSE, one can block outgoing connections to the HDFS ports except for user root.  This means that only root-owned processes (such as FUSE-DFS) will be able to access Hadoop. <ul>
<li> This is sufficient for grid environments, but does not protect one in the case where the attacker has physical access to the network switch.
</li></ul> 
</li> <li> There exists another option, currently untested.  It is possible to limit all HDFS socket connections to SSL-based sockets.  Using this to only allow known hosts to connect to HDFS and only allowing FUSE-DFS to connect on those known hosts, one might be able to satisfy even fairly stringent security folks (but not paranoid ones).
</li></ul> 
<p />
There are three options to export your data outside your cluster: <ul>
<li> Globus GridFTP / SRM.  This is covered in these web pages.
</li> <li> Xrootd.  Documentation is nascent.
</li> <li> Apache HTTP (authenticated via HTTPS).  This is in use at Caltech, but there isn't any documentation available.
</li></ul> 
<p />
<h1><a name="Comments"></a> Comments </h1>
<form method="post" action="https://twiki.opensciencegrid.org/bin/save/Storage/HadoopUnderstanding" enctype="multipart/form-data" name="tableappend0" id="tableappend0">
<p />
<div class="commentPlugin commentPluginPromptBox">
<table><tr valign="middle"><td><textarea  rows="3" cols="70" name="comment" wrap="soft" onfocus="if(this.value=='')this.value=''" onblur="if(this.value=='')this.value=''"></textarea></td><td><input  type="submit" value="Add comment" /></td></tr></table>
</div><!--/commentPlugin-->
<p />
<input type="hidden" name="comment_action" value="save"  />
<input type="hidden" name="comment_type" value="tableappend"  />
<input type="hidden" name="comment_index" value="0"  /></form>
<p />
<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 <code><b>===============</b></code>
<p />
 Thank you for claiming ownership for this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Storage/FirstLast?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local OWNER = <span class="twikiNewLink">DouglasStrain<a href="/bin/edit/Storage/DouglasStrain?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="DouglasStrain (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (<span class="twikiNewLink">ComputeElement<a href="/bin/edit/Storage/ComputeElement?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="ComputeElement (this topic does not yet exist; you can create it)">?</a></span>|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3) <ul>
<li> Local DOC_AREA       = Storage
</li></ul> 
<p />
 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (<span class="twikiNewLink">EndUser<a href="/bin/edit/Storage/EndUser?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="EndUser (this topic does not yet exist; you can create it)">?</a></span>|Student|Developer|SysAdmin|VOManager) <ul>
<li> Local DOC_ROLE       = <span class="twikiNewLink">SysAdmin<a href="/bin/edit/Storage/SysAdmin?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="SysAdmin (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge) <ul>
<li> Local DOC_TYPE       = Installation  Please define if this document in general needs to be reviewed before release ( 1 | 0 )
</li> <li> Local INCLUDE_REVIEW = 1
</li></ul> 
<p />
 Please define if this document in general needs to be tested before release ( 1 | 0 ) <ul>
<li> Local INCLUDE_TEST   = 0
</li></ul> 
<p />
 change to 1 once the document is ready to be reviewed and back to 0 if that is not the case <ul>
<li> Local REVIEW_READY   = 1
</li></ul> 
<p />
 change to 1 once the document is ready to be tested and back to 0 if that is not the case <ul>
<li> Local TEST_READY     = 0
</li></ul> 
<p />
 change to 1 only if the document has passed the review and the test (if applicable) and is ready for release <ul>
<li> Local RELEASE_READY  = 0
</li></ul> 
<p />
<p />
 DEAR DOCUMENT REVIEWER
 <code><b>==================</b></code>
<p />
 Thank for reviewing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Storage/FirstLast?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local REVIEWER       = <span class="twikiNewLink">NehaSharma<a href="/bin/edit/Storage/NehaSharma?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="NehaSharma (this topic does not yet exist; you can create it)">?</a></span> Please define the review status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 )
</li> <li> Local REVIEW_PASSED  = 2
</li></ul> 
<p />
<p />
 DEAR DOCUMENT TESTER
 <code><b>================</b></code>
<p />
 Thank for testing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Storage/FirstLast?topicparent=Storage.HadoopUnderstanding" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local TESTER         =  Please define the test status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 )
</li> <li> Local TEST_PASSED    = 0
</li></ul> 
############################################################################################################
--></div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Storage<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span><a href="/bin/view/Storage/WebHome" class="twikiCurrentWebHomeLink twikiLink">WebHome</a> &gt; <a href="/bin/view/Storage/Hadoop" class="twikiLink">Hadoop</a><span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>HadoopUnderstanding</span> <br />    
Topic revision: r20 - 06 Dec 2016 - 18:13:14 - <a href="/bin/view/Main/KyleGross" class="twikiLink">KyleGross</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
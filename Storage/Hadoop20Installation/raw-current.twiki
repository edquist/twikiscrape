---+!! *&lt;noop&gt;%SPACEOUT{ &quot;%TOPIC%&quot; }%*


%RED%
WARNING! This page is for an older version of Hadoop.
For newer versions, please visit [[Documentation/Release3.InstallHadoopSE][Hadoop Release 3 Installation]]
%ENDCOLOR%


%DOC_STATUS_TABLE%
%TOC%

---+ Installing HDFS


%RED%
WARNING! This page is for an older version of Hadoop.
For newer versions, please visit [[Documentation/Release3.InstallHadoopSE][Hadoop Release 3 Installation]]
%ENDCOLOR%


This guide covers installation of the HDFS core components, along with the FUSE mounts.  The current version of HDFS covered in this guide is 0.20.2.

Once done with this guide, you should have Hadoop installed, configured, and working.  You should be able to navigate the file system through the FUSE mount point.  The next two guides cover the installation of grid components.

%INCLUDE{&quot;Documentation/DocumentationTeam/DocConventions&quot; section=&quot;Header&quot;}%
%INCLUDE{&quot;Documentation/DocumentationTeam/DocConventions&quot; section=&quot;CommandLine&quot;}%

---++ General Prerequisites

%STARTSECTION{&quot;Prereqs&quot;}%

Hadoop will run anywhere that Java is supported (including Solaris).  However, these instructions are for !RedHat 5 derivants (including Scientific Linux) because of the RPM based installation.

The HDFS prerequisites are:
   * Minimum of 1 headnode (the namenode), although 2 recommended (the namenode and the secondary namenode)
   * At least one node which will hold data, preferably at least 2.  Most sites will have 20 to 200 datanodes.
   * The namenode and secondary name node are *not* datanodes.
   * Working Yum and RPM installation on every system.
   * =fuse= kernel module and =fuse-libs=.
   * Java RPM.  If java isn&#39;t already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency.  Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.

*Compatibility Note* Note that versions of !OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux !OpenAFS client, make sure you&#39;re running at least !OpenAFS 1.4.7.

*Note*: The rpm/yum installation will create a &#39;hadoop&#39; system account and group  (uid,gid &lt; 500) on the host system for running the datanode services.  If you would like to control the uid/gid that is used, then you should create the &#39;hadoop&#39; user and group manually before installing the rpms.

%ENDSECTION{&quot;Prereqs&quot;}%

---++ RPM installation

---+++ Quick Install

Quickstart for the impatient.  Follow the following steps on your namenode, secondary namenode, and all the data nodes.

&lt;pre class=&quot;rootscreen&quot;&gt;
rpm -ivh http://vdt.cs.wisc.edu/hadoop/osg-hadoop-20-3.el5.noarch.rpm
yum install hadoop-0.20-osg
vi /etc/sysconfig/hadoop # Edit appropriately (see below)
service hadoop-firstboot start
chkconfig hadoop on
service hadoop start
&lt;/pre&gt;

---+++ Full Install

%STARTSECTION{&quot;Prep&quot;}%

The Hadoop init script assumes that you are not running multiple hadoop services (datanode, namenode, secondary namenode) on the same host.

%ENDSECTION{&quot;Prep&quot;}%

The FUSE interface to HDFS requires the =fuse= kernel module.  The stock RHEL kernels include the =fuse= kernel module as of RHEL5.4. %STARTSECTION{&quot;Fusemod&quot;}% If you are running a custom kernel, then be sure to enable the =fuse= module with =CONFIG_FUSE_FS=m= in your kernel config.  Building and installing a =fuse= kernel module for your custom kernel is beyond the scope of this document.

*Note:* If you cannot find a =fuse= kernel module to match your kernel, ATRPMs has a [[http://people.atrpms.net/~pcavalcanti/LCG_kernel_modules.html][guide for using their RPM spec files]] in order to generate a module.  That page mostly works, although sections are a bit out dated.  Contact the osg-hadoop@opensciencegrid.org list if you need help.

%ENDSECTION{&quot;Fusemod&quot;}%

---+++ Installing with yum

A yum repository for installing and upgrading Hadoop is hosted at the VDT.

To configure your local installation for the yum repository, you should install the osg-hadoop package with the following command:

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% rpm -ihv http://vdt.cs.wisc.edu/hadoop/osg-hadoop-20-3.el5.noarch.rpm
&lt;/pre&gt;

After installing the yum configuration package, you can install the hadoop core with:

%STARTSECTION{&quot;Install&quot;}%

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% yum install hadoop-0.20-osg
&lt;/pre&gt;

%ENDSECTION{&quot;Install&quot;}%

---+ Configuring HDFS

---++ HDFS Directory Locations

%STARTSECTION{&quot;Config&quot;}%

The Hadoop RPMs install files into the standard system locations.  The following table highlights some of the more interesting locations, and documents whether you might ever want to edit them.

| File Type | Location | Needs editing? |
| Log files | =/var/log/hadoop/*= | No |
| PID files | =/var/run/hadoop/*.pid= | No |
| init scripts | =/etc/init.d/hadoop=, =/etc/init.d/hadoop-firstboot= | No |
| init script config file | =/etc/sysconfig/hadoop= | Yes |
| runtime config files | =/etc/hadoop/conf/*= | Maybe |
| System binaries | =/usr/bin/hadoop= | No |
| JARs | =/usr/lib/hadoop/*= | No |

---%SHIFT%++ Edit /etc/sysconfig/hadoop

The most common site configuration settings can be changed in =/etc/sysconfig/hadoop=.  In most cases, this file will be identical on the namenode and datanodes.  The configuration settings are documented in the file itself, but we document some of the most commonly edited ones in the table below:

| Option Name | Needs editing? | Suggested value |
| HADOOP_NAMENODE | Yes | The host name of your namenode; should match the output &#39;hostname -s&#39; on the namenode server |
| HADOOP_NAMEPORT | Yes | 9000 |
| HADOOP_SECONDARY_NAMENODE | Yes | The host name of the secondary namenode; should match the output of &#39;hostname -s&#39; |
| HADOOP_CHECKPOINT_DIRS | Yes | Comma-separated (*important:* no spaces between commas!) list of directories to store checkpoints on.  The safest configuration is to store 2 checkpoints locally on 2 block devices and 1 checkpoint on a NFS server.  At least 1 checkpoint directory is required.  |
| HADOOP_CHECKPOINT_PERIOD | Yes | The time, in seconds, between checkpoints.  600 is suggested for small sites |
| HADOOP_REPLICATION_DEFAULT | Yes | Default number of replications.  Suggested: 2 |
| HADOOP_REPLICATION_MIN | Yes | Minimum number of replications; below this, an error will be thrown.  Suggested: 1 or 2. |
| HADOOP_REPLICATION_MAX | Yes | Maximum number of replications.  Suggested: 512 |
| HADOOP_GANGLIA_ADDRESS | Maybe | Hostname or IP of your Ganglia gmetad.  If left empty then hadoop will try to extract the ganglia metad address from /etc/gmond.conf.  If you aren&#39;t using Ganglia just leave it blank. |
| HADOOP_DATADIR | Yes | The base directory where HDFS temp and management data will be written.  On datanodes this is usually the parent of the first data partition. It is safe to leave this empty for client-only installations.|
| HADOOP_DATA | Yes | A comma-separated list of directories (no spaces!) where the HDFS data blocks will be stored.  The first one is typicall the same as $HADOOP_DATADIR/data.  It is safe to leave this empty for client-only installations. |
| HADOOP_USER | Maybe | The username that the hadoop datanode daemons will run under.  Suggested: hadoop |
| HADOOP_NAMENODE_HEAP | Maybe | The Java heap size for the namenode; bigger is better, but the node shouldn&#39;t swap.  Minimum: 2048m.  Suggested: 8192m |
| HADOOP_MIN_DATANODE_SIZE | Maybe | A value in GB; if the data directory is smaller than this size, HDFS will refuse to start.  Safeguards against starting the datanode daemon on non-datanodes.  Suggested: 300 (this value will vary widely with your datanode size). Set to zero or an empty string to bypass this check. |

After making changes to the file, you must run:

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% service hadoop-firstboot start
&lt;/pre&gt;

This propagates the changes to the hadoop configuration files in =/etc/hadoop= and must be run every time you make changes to =/etc/sysconfig/hadoop=.

*NOTE:* If you just installed Hadoop for the first time, you must log in/out of your shell or source /etc/profile.d/hadoop.sh before your you try playing with the command line tools.

*Upgrade note:* Configuration files will be saved with a =.rpmsave= extension if you ever update your hadoop rpms with rpm or yum.  *Make sure to copy your settings from =/etc/sysconfig/hadoop.rpmsave= to =/etc/sysconfig/hadoop= if you ever update your hadoop rpms.*  Any manual changes to the hadoop configuration files in =/etc/hadoop/= should be preserved during an upgrade, but may be overwritten when running =hadoop-firstboot=.

---%SHIFT%+++ Side topic: Multiple data directories on a datanode.

Hadoop has the ability to store data in multiple directories on a datanode.  This can be useful if you have multiple drives on your datanode and don&#39;t want to run them in a raid array, or if you have multiple large storage volumes mounted on your datanode.  To configure a datanode to use multiple directories, you need to enter each directory in the =HADOOP_DATA= setting in =/etc/sysconfig/hadoop= as a comma-separated list of directories (no spaces!) and then run =service hadoop-firstboot start=.  Here is an example of a datanode with 4 storage directories:

&lt;pre class=&quot;file&quot;&gt;
HADOOP_DATA=/data1/hadoop/data,/data2/hadoop/data,/data3/hadoop/data,/data4/hadoop/data
&lt;/pre&gt;

%ENDSECTION{&quot;Config&quot;}%

---++ Running Hadoop

%STARTSECTION{&quot;Running&quot;}%

The Hadoop rpms install a startup script in =/etc/init.d/hadoop=.  The same command is used to start hadoop services on a datanode, namenode, or secondary namenode:

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% service hadoop start
&lt;/pre&gt;

You will also want to configure hadoop to start at boot time with:

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT%  chkconfig hadoop on
&lt;/pre&gt;

---%SHIFT%+++ Side topic: Client-only installation

Sometimes it is handy to configure a node to be a client, that is, a system that has access to hadoop but will not serve as a datanode or namenode.  The installation and configuration for such a node is the same as above, except that you do not need to start any hadoop services with =/etc/init.d/hadoop=.  It is still necessary to modify =/etc/sysconfig/hadoop=, but it is not necessary to specify any datanode directories in =HADOOP_DATA=.

%ENDSECTION{&quot;Running&quot;}%

---++ Mounting FUSE at boot time

After you have installed the =fuse-libs= rpms as well as the =fuse= kernel module (preferably via the =fuse= RPM), you can %STARTSECTION{&quot;Fuse&quot;}% mount FUSE by adding the following line to =/etc/fstab= (Be sure to change the =/mnt/hadoop= mount point and =namenode.host= to match your local configuration.  To match the help documents, we recommend using =/mnt/hadoop= as your mountpoint):

&lt;pre class=&quot;file&quot;&gt;
hdfs# %RED%/mnt/hadoop%ENDCOLOR% fuse server=%RED%namenode.host%ENDCOLOR%,port=9000,rdbuffer=131072,allow_other 0 0
&lt;/pre&gt;

Alternatively this can be taken care of automatically when running =hadoop-firstboot= if in your =/etc/sysconfig/hadoop= file you set the following line:

&lt;pre class=&quot;file&quot;&gt;
HADOOP_UPDATE_FSTAB=1
&lt;/pre&gt;

Once your =/etc/fstab= is updated, to mount FUSE run:

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% mount /mnt/hadoop
&lt;/pre&gt;

When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:

&lt;pre class=&quot;rootscreen&quot;&gt;
# mount /mnt/hadoop
port=32767,server=(
fuse-dfs didn&#39;t recognize /mnt/hadoop,-2
fuse-dfs ignoring option allow_other
&lt;/pre&gt;

%ENDSECTION{&quot;Fuse&quot;}%

%STARTSECTION{&quot;Fusedebug&quot;}%

To start the FUSE mount in debug mode, you can run the FUSE mount command by hand:

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT%  /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server=%RED%namenode.host%ENDCOLOR%,port=9000,rdbuffer=131072,allow_other -d
&lt;/pre&gt;

Debug output will be printed to stderr, which you will probably want to redirect to a file.  Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.

%ENDSECTION{&quot;Fusedebug&quot;}%

---+ Next steps

Congratulations!  At this point, you should have a working Hadoop installation.  Please proceed to the validation steps or the next guide, [[Hadoop20GridFTP][Hadoop and GridFTP]].

&lt;!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = JeffDost

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|General|Trash/Trash/Integration|Monitoring|Operations|Security|Storage|Trash/Tier3|User|VO)
   * Local DOC_AREA       =  Storage

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Developer|Documenter|Scientist|Student|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (HowTo|Installation|Knowledge|Navigation|Planning|Training|Troubleshooting)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = 
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
--&gt;

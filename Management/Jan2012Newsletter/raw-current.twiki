 
*OSG Newsletter, January 2012*
 

*New Staff in OSG*

We are pleased to announce the addition of two new OSG staff, Timothy Mortensen and Yaling Zheng. 

Both Timothy and Yaling will be replacing other staff who are moving into broader contributions across both the core project and our companion satellites. 
At the University of California, San Diego, Timothy Mortensen is now part of the Overlay Factory operations team. Timothy has a bachelor&#39;s degree in computer science from UCSD. 

At the University of Nebraska Yaling Zheng is part of the user support and technology investigation areas. Initially, Yaling will be helping with the evaluation of the iRODS technologies - which spans both areas. Yaling has a doctorate in computer science from the University of Nebraska. 

Yaling and Timothy will both also be helping with the &quot;Any Data Any Where Any Time (AAA)&quot; satellite project. AAA is deploying a federated data system for the CMS experiment, with the long term goal of eventually making similar technologies available for other communities. 

Please join me in welcoming Yaling and Timothy to the OSG team!

_~ Ruth Pordes_ 

*Feedback Wanted on Upcoming OSG Software Plans*

Now that we&#39;ve done the initial release of the OSG Software as RPMs, we would like to make sure we have goals for the next six to 12 months that meet the community&#39;s needs. We have a lot of ideas of what might be needed in the OSG Software, but most of them are not yet scheduled because we need your input. 

You can see the current list of goals.

You can also see our list of scheduled software updates.

We are very interested in hearing about your upcoming software needs. Is there software you would like to see added? Support for other platforms (e.g. Scientific Linux 6)? Anything else? If you can review the above lists and share feedback (including timelines) with the OSG Software Team, we would appreciate it. 

Please send your feedback directly to Alain Roy. 

_~ Alain Roy_ 

*OSG Documentation Update*

Starting with OSG Software Release3 (aka the RPM release), the documentation team has joined with the software team to produce the updated documentation. Since this release required changes to the majority of installation procedures, we took the opportunity to review all the official documentation. This included moving and, where necessary, updating those documents that would be relevant to the new release. 

Another change in approach has been using short, concentrated team efforts on documentation in the form of Doc Fests. We held a three day Doc Fest in October 2011 and a two day fest in January 2012. These fests were focused on quickly deciding on our approach and standards and then actually doing the work. A core group of five to seven people were physically present and a few others called in at scheduled times. 

This new process has proven to be quite effective. Since the start of the project, over 300 documents have been reviewed, updated as needed, and copied or moved to the Documentation Web and the documentation for Release3. Approximately 10 new documents were created. Better yet, about 150 documents were eliminated as they were specific to a previous project or release or were no longer relevant. As a side benefit, we eliminated the Troubleshooting Web, which was mostly out-of-date, by moving the useful documents into Release3. 

Over the next month, as the remaining documents are resolved, we hope you will be pleased with the results. To get a preview of where we are headed, check out the Storage Navigation Page.

Thanks are owed to primary contributors Alain Roy, Marco Mambelli, Doug Strain, Marko Slyz, Anand Padmanabhan, Suchandra Thapa, Steve Timm, and Jim Weichel, as well as the other community members who helped with this project.

_~ Jim Weichel_ 

*From the Executive Director*

I hope everyone had a very good holiday and break. 

Activities this month include discussions of technology futures and the future software releases. Please do not hesitate to contact Alain Roy with your requests and ideas for the OSG Virtual Data Toolkit. Likewise, please do not hesitate to contact Gabriele Garzoglio with your general requests and the needs of your communities. 

Our technology activities include moving forward with the services we can offer on the campus for local computing and sharing of computational services, virtualization, resource provisioning, and the management of shared data storage. While the blueprint meetings themselves are by invitation only, the information page and email list are public and you are welcome to join the discussions there. 

There are three report drafts in the document repository (see below) as a result of some of the recent technology work. As these are drafts, feedback is welcome - please send any comments and questions to Brian Bockelman. 

Our security team has also been very busy dealing with somewhat more than the usual amount of alarms and incidents. Please do reach out to the security team and/or Mine Altunay for any questions, training, or help that you can possibly think of! We encourage and expect you to be pro-active on this one and we will be there to help. 

_~ Ruth Pordes_ 

*ATLAS and CMS Thumbnails*

Reminder:
Don&#39;t forget to register for the upcoming OSG All Hands!

*Featured Site: Bellarmine ATLAS-T3* 

Site name: Bellarmine ATLAS Tier-3

Location: Bellarmine University 

Owner: Bellarmine University and University of Oklahoma High Energy Physics Group 

Cores: 384 with 375TB of disk storage space. 

Used by: ATLAS, D0, and DOSAR 

What is unusual about your site? 
Kentucky, where we are located, is an EPSCoR state. Through our Trash/Tier3 center, which is the only OSG site in the state, OSG is helping to close the “Digital Divide” gap in grid computing. As an EPSCoR state, we face some unusual challenges. For instance, we only have a 100 Mbps bandwidth connection dedicated to the cluster. Despite our challenges, we are able to participate in the ATLAS collaboration under the University of Oklahoma. Since we are a primarily undergraduate institution, this opens up new opportunities for undergraduates to gain invaluable research experience. 

_~ Akhtar Mahmood_

*SBGrid&#39;s successful start to 2012*

The first week of 2012 was successful for the Structural Biology Grid (SBGrid) virtual organization (VO). 

Structural biologists utilized 343,000 hours of computing on the OSG, with sustained periods of over 6,000 concurrent jobs derived predominantly from opportunistic resources. This underlines the robustness of the GlideinWMS mechanism which has been deployed by the SBGrid team after the extensive discussion that took place at the 2010 All-Hands-Meeting at Fermilab. 

The computations were submitted through the SBGrid Science Portal, developed by the Sliz Laboratory at Harvard, and supported macromolecular structure determination efforts by research groups in the US, Asia, and Europe. 

SBGrid also recently benefited from the addition of the 5,000-core Orchestra cluster at Harvard Medical School (HMS) as an OSG site that accepts SBGrid jobs. Trash/Trash/Integration of the Orchestra resources with OSG was a priority for Chris Botka, the new director of research computing at HMS. 

We&#39;d like to thank the OSG community for sharing their technical expertise and resources with the structural biology community, as well as long-term community members Peter Doherty and Ian Stokes-Rees, who carried out the technical implementation and testing. Without Chris, Peter, and Ian&#39;s hard work, and the support of the larger community, our successful New Year start would not have been possible.

_~ Piotr Sliz_ 

*Glidein WMS Front-End Training a Success*

Virtual organization (VO) administrators gathered last week at the University of California-San Diego for a school on how to run the VO Overlay Job Management Services: the VO &quot;Front end&quot; and GlideinWMS submit hosts. 

It was a week of in-depth lectures, talks from VOs about real-world experiences, and discussion. Trainers and supporters included the OSG operations staff from UCSD and Indiana University, GlideinWMS team project members, and Condor project contributors. Nearly 20 attendees came from campus IT departments, VOs in earthquake and nano-fabrication engineering, a multi-campus regional IT organization, and a non-LHC high energy physics experiment. These organizations serve scientists across mathematics, physics, chemistry, biology, and engineering. 

Materials from the class, the agenda, and further information can be seen on the school&#39;s website. 

_~Igor Sfiligoi_ 

*OSG Certificate Authority Status*

We are continuing to evaluate the prospective commercial certificate authority (CA) to serve as the OSG Certificate Authority. 
To date, our testing has been focused on the suitability of the CA from a technical and policy standpoint to satisfy OSG&#39;s certificate needs. No major issues have been revealed. Beginning in January, we will test the use of these certificates for the Large Hadron Collider and other community applications - covering data, job, database, and other services that these VOs regularly use. 

In early December 2011, ESnet contacted its DOEGrids CA users to alert them to the transition of its services to the OSG. We are in contact with a few of the communities DOEGrids currently serves which are interested in our plans. If you have not been in touch with the OSG and would like more information, please don&#39;t hesitate to contact Ruth Pordes or Von Welch. 

The commercial CA pilot will conclude in mid-February with a recommendation to the Executive Team; the recommendations will include a preliminary plan for implementation. At the end of February ESnet, the OSG and the DOE are planning a follow-up meeting to review and finalize next steps. In the meantime, we will continue to update the community as additional information becomes available. 

_~Von Welch and Ruth Pordes_

*ATLAS Update*

ATLAS physicists continue to analyze approximately one billion collision events that were taken with their detector during the 2011 Large Hadron Collider run. In the process, the need for specific simulated samples arose. 

All sites contributing to ATLAS computing were asked to help produce this important data. The collaboration, including all OSG sites that are part of the U.S. ATLAS Computing Facility, answered the call for processing stability and maximal resources to maximize the simulation throughput over the holidays.

With 45 million events produced per day, the holidays marked the highest sustained ATLAS grid production levels ever. In addition, the Tier-2 centers in the U.S. contributed prominently to urgent digitization and reconstruction tasks, a very demanding processing step that is carried out exclusively at Tier-1 centers in other regions. 
Thanks to the many collaboration members who put in extra time as part of this effort, and to the OSG collaboration for creating the infrastructure to make this possible. 

_~ Michael Ernst_ 

*CMS Update*

In December, we celebrated the holiday break by doing a complete reprocessing of the 2011 Monte Carlo data using an updated alignment for the CMS tracker. More than 1.1 billion events were processed (out of a 2.7 billion event request). Another 300 million events were processed to check the trigger and software performance under conditions similar to what has been proposed for the 2012 run. And another 3.5 billion events were generated in the standardized LHE format. In all, it was a very busy and successful holiday!

_~ Burt Holzman_ 

*OSG Document Database updated in the Last 30 Days* 
  
OSG-doc-#	Title	Author(s)	Topic(s)	Last Updated
1086-v1
OSG Strategic Accomplishments 2006-2011
OSG Council
Reports
13 Jan 2012
1084-v1
glexec Usability Improvements
Brian Bockelman
Blueprints
04 Jan 2012
1083-v1
Improved Site Accounting
Brian Bockelman
Blueprints
04 Jan 2012
1082-v1
Grid usage of Condor VM Universe
Brian Bockelman
Blueprints
04 Jan 2012
 
________________________________________ 


---+!! *OSG Newsletter, November 2012* 

---+++_*Save the date, March 11-14th, 2013, [[http://rac.uits.iu.edu/osgahm-2013][Open Science Grid All Hands]]*_

*IN THIS ISSUE*

%TOC%
 
----+++_Research Highlight:_
 
----+++*NEES uses Open Science Grid to Help Reduce Earthquake Damage*

 &lt;img src=&quot;%ATTACHURLPATH%/Clayton.jpg&quot; alt=&quot;Clayton.jpg&quot; width=&#39;133&#39; height=&#39;150&#39; style=&quot;float:left; margin: 10px 10px 10px 0;&quot; /&gt; 
 
Patricia Clayton and the George E. Brown, Jr. Network for Earthquake Engineering Simulation ([[http://nees.org][NEES]]) research team are striving to make buildings more resilient during earthquakes. The team is working to reduce the economic impact of earthquakes and get communities back up and running more quickly. The NEES team uses !OpenSees (the Open System for Earthquake Engineering Simulation) software on the OSG for predicting the seismic response of structural and geotechnical systems. The ability to run these extensive simulations might not be possible without the OSG.  [[https://www.opensciencegrid.org/bin/view/Management/ResearchHighlight33][Learn More…]]
 
_~ Greg Moore and  [[mailto:sjengel@indiana.edu][Sarah Engel ]]_
 
----+++*Computer Science Research to Advance the OSG*

 &lt;img src=&quot;%ATTACHURLPATH%/car2-dvdt.jpg&quot; alt=&quot;car2-dvdt.jpg&quot; width=&#39;248&#39; height=&#39;150&#39; style=&quot;float:right; margin: 10px 10px 10px 0;&quot; /&gt; 
 
In recognition of the importance of collaborative science, the US Department of Energy solicited proposals aimed at advancing the state-of-the-art in extreme scale scientific collaborations. Miron Livny (University of Wisconsin at Madison) and his colleagues: William Allcock (University of Chicago, Argonne National Laboratory), Ewa Deelman (University of Southern California), Douglas Thain (University of Notre Dame), and Frank Wuerthwein (University of California San Diego) have received an award under that program entitled “dV/dt - Accelerating the Rate of Progress towards Extreme Scale Collaborative Science”.
 
This project is focused on issues of resource management within a collaboration, supporting the “submit locally and compute globally” paradigm. Within this context, the goal is to advance the state-of-the-art in the areas of: trust, planning for resource provisioning and workload, computer, data, and network resource management. 
 
The approach will rely on experimentation with prototype implementations on at-scale computational environments that involve real-life scientific applications and advanced computing infrastructures such as the Argonne Leadership Computing Facility and the Open Science Grid. It is expected that research results generated as part of this project will be translatable to deployable and widely adopted software capabilities.
 
_~ [[mailto:deelman@isi.edu][Ewa Deelman ]]_

----+++*Precision Event Simulation for the LHC*
&lt;img src=&quot;%ATTACHURLPATH%/jetmulti_main_2.png&quot; alt=&quot;jetmulti_main_2.png&quot; width=&#39;183&#39; height=&#39;200&#39; style=&quot;float:left; margin: 10px 10px 10px 0;&quot; /&gt; 


Particle physics has entered a new era with the recent discovery of a new elementary boson at the Large Hadron Collider (LHC) at CERN. The task of the coming years is to measure the precise properties of this new particle in order to determine whether it is related to the Higgs boson predicted by the Standard Model. Meanwhile, searches for signals of new phenomena will continue to be carried out at higher energy and luminosity.
 
At hadron colliders, such as the LHC, measurements as well as searches are complicated by the omnipresent effects of the strong nuclear force.  Sprays of strongly interacting particles, or jets, are produced copiously, and lead to large amounts of irrelevant interaction events, or &quot;backgrounds&quot;. Precise predictions for these reactions are central to the analysis of LHC data.  We have developed a Monte Carlo event generator, [[https://sherpa.hepforge.org/trac/wiki][Sherpa]], capable of computing signals as well as backgrounds to unprecedented accuracy at the particle-level based on quantum chromodynamics calculations implemented by the [[http://blackhat.hepforge.org/][!BlackHat]] library.
 
!BlackHat and Sherpa are often used to determine the rates at which jets are produced in conjunction with electroweak bosons. The figure shows experimental data and a typical uncertainty band obtained with the newest version of the generator (orange) compared to the previous version (blue). Both calculations were computationally very demanding. The construction and validation of the event generator involved many tests, with a complete validation consuming as much as 100 CPU days.  Applications in particle physics phenomenology are often more challenging, with a single project requiring as much as 10-20 CPU years.
 
The Open Science Grid has enabled us to perform both validation and analysis tasks on very short time scales. We have carried out a background study for Supersymmetry searches within several days. This project, for example, required the use of 150,000 CPU hours and produced ~1 TB of data. We used custom scripts to transfer the data from each worker node to the Storage Element (SE) at the site where the job ran. Then later we transferred the data from the SEs back to SLAC. Recently, we&#39;ve started looking into using the OSG public storage service to automate the data transfer and ensure that data and software are uploaded to appropriate sites.
 
The Condor-based !glideinWMS system, used by OSG, proved to be a very convenient tool for large-scale Monte Carlo event production. We have recently explored MPI parallelization by running HTPC jobs on OSG in order to meet the increased demands of future experiments. Current and future public versions of !BlackHat and Sherpa are tested and validated with the help of the Open Science Grid.
 
_~ Stefan Höche, SLAC_,
_OSG Contact – [[mailto:mslyz@fnal.gov][Marko Slyz ]]_

----+++*Using OSG for Combinatorial Computing*
&lt;img src=&quot;%ATTACHURLPATH%/c4_and_c4k4-free.png&quot; alt=&quot;c4_and_c4k4-free.png&quot; width=&#39;242&#39; height=&#39;150&#39; style=&quot;float:right; margin: 10px 10px 10px 0;&quot; /&gt; 
 
 
Ramsey theory studies the properties that combinatorial structures need in order to guarantee that desired substructures are contained within them. It is often seen as the study of the order that comes from randomness, and has applications in mathematics, computer science, finance, economics, and other areas. Our research involves a computational approach to establishing the values of various Ramsey numbers, whose role is to quantify the general existential theorems in Ramsey theory. My advisor, Professor Stanislaw Radziszowski, is the author and maintainer of the dynamic survey [[http://www.combinatorics.org/][&quot;Small Ramsey Numbers&quot;]] which is recognized as a standard reference in the area. The problems arising in Ramsey theory are notoriously difficult to solve, both from theoretical and computational perspectives. Therefore, massive computations are often used to solve specific problems.
 
In this project, we are concerned with the Ramsey numbers R(C4,Km), that is, we wish to find the smallest n such that every graph on n vertices contains either a cycle of length four or an independent set of order m.  This computational approach requires massive computations on large sets of data, and the Open Science Grid is the perfect tool to make this happen. We can easily split up the data in groups small enough so that each job can process a group in a reasonable amount of time. I have found OSG incredibly easy to use, and owe Mats greatthanks for his constant help. The scripts used are intuitive and straightforward, and I have yet to come across any issues. I have been quite impressed with OSG and have had an excellent experience using it thus far.
 
The project has so far used 108,000 CPU hours on OSG.
 
R(C4,Km) is open for m=9,10, and we are already close to solving the case m=9. This result would not be possible without the Open Science Grid. This research will very likely lead to a conference presentation and a journal paper.
 
_~ Alexander Lange and Stanislaw Radziszowski, Rochester Institute of Technology,_
_OSG Contact – [[mailto:rynge@isi.edu][Mats Rynge]]_

----+++*OSG PKI*

OSG is now transitioning its new PKI services into production with the goal of being ready for the whole OSG community by the end of the year. 

While our goal is to make the transition as seamless as possible, it will bring some changes for each user, administrator and manager. You should be planning now for how you will handle the transition.Guidance can be found on the [[https://www.opensciencegrid.org/bin/view/Security/PKIDocumentationIndex][OSG PKI Documentation]]. 

Site and VO Registration Authorities (RAs) and Grid Admins who have not yet taken training should be taking it on-line and enrolling in the new PKI in support of their user communities. [[http://wdrs.fnal.gov/job_descript/administrative_support/admin_support_assist_3.html][Training]] is available. Please [[https://twiki.grid.iu.edu/bin/view/Security/OSGCATransition2012][view]] for the latest information.

Please contact  [[mailto: vwelch@indiana.edu][Von Welch]] or [[mailto:ruth@fnal.gov][Ruth Pordes]] with questions or concerns.

----+++*CILogon Identities gaining broader acceptance in OSG*

OSG is pleased to announce that OSG IT services (!DocDB, Twiki, !MyOSG,OIM and so on) have started allowing access to authorized OSG users with CILogon Basic CA certificates. [[https://cilogon.org/][CILogon Basic CA]] is an online Certificate Authority that automatically issues certificates to anyone based on their membership with a US higher education or research institute (e.g. DOE National Labs, NIH, NSF, etc). Compared to other classical CAs, this CA provides improvements over ease of use and processing speed; the users do not have to go through any manual identity vetting process. The certificates are issued within seconds and all interaction happens in the user&#39;s browser. Upon visiting the CA&#39;s web site, a user is redirected to his home organization&#39;s web site for authentication. If the authentication with the home organization succeeds, the user is issued a certificate within seconds. A list of organizations that partner with [[https://cilogon.org/][CILogon Basic CA]] is listed on the CA&#39;s website. 
 
The OSG team hopes that accepting CILogon Basic CA certificates will make the authentication process faster and easier for our users. Towards that goal, we are also working with OSG resource providers to accept the !CILogon Basic CA certificates for grid access. So far, MWT2, Purdue, Nebraska, and SPRACE sites have agreed to accept these certificates; we continue to increase the number of sites that will accept these certificates. If you think your site or VO could benefit from these certificates, or have any feedback or questions, please contact the [[mailto:osg-security-team@opensciencegrid.org][OSG Security Team]], or open a GOC ticket.
 
_~ [[mailto:maltunay@fnal.gov][Mine Altunay ]] , OSG Security Team_

----+++*OSG Campus Infrastructure Community*

Last week&#39;s  OSG Campus Infrastructures Community meeting at the University of California Santa Cruz was very useful and successful overall.  We encourage you to read the [[https://indico.fnal.gov/conferenceDisplay.py?confId=5927][presentations]].  The next steps for the Community are upcoming monthly webinars, planning for the Campus Infrastructures Day at the March OSG All Hands meeting in Indianapolis, and identifying a date and venue for the next in-person meeting (with an August timeframe most likely).  

Stay tuned for the following activities:  in December Dan Bradley will give a web-based tutorial on software access, and in January Shawn !McKee will talk on tools for tackling networking issues in distributed HTC environments. 

We would like to thank all who made the OSG CIC meeting a success, especially our local organizers from the Santa Cruz Institute for Particle Physics, Mykell Discipulo and Vicki Johnson.

~ _[[mailto:rwg@hep.uchicago.edu][Rob Gardner ]]_

----+++*An OASIS in sight*

Installing software on the OSG can be a daunting experience for a new organization.  Each site on the OSG has a unique shared file system for application software - referred to as $OSG_APP.  An organization can install their software in that directory at the site in order to make it available on the worker nodes.  However, the OSG does not provide tools for managing software installs across its 112 sites– both experienced and inexperienced virtual organizations (VOs) may find developing scripts from scratch difficult.
 
To address this difficulty, the OSG Technology and Productions team is working on a new service called &quot;OSG Application Software Installation Service&quot;.  This will provide a central server at the GOC where each VO can install their software (either through an interactive login or a Globus GRAM interface).  From the central service, OSG will move the software to each site.
 
Currently, the technology underpinning OASIS is the !CernVM File System, or CVMFS.  CVMFS is a FUSE-based file system which distributes the file system metadata and data via HTTP to each site, relying on HTTP caches to keep the load minimal on the central server.  CVMFS provides checksumming and integrity checking, preventing file corruptions from affecting jobs.  
 
We are currently planning the OASIS server deploy at the GOC, which will be open to beta testers in January, and hope to have a final production version in March.  For sites, OASIS will show up as another CVMFS mount - a familiar site to CMS and ATLAS system administrators.  For VOs, we&#39;re hoping it will be a huge timesaver and provide a better OSG experience.
 
_~ [[mailto:bbockelm@cse.unl.edu][Brian Bockelman ]]_

----+++*USCMS OSG*
 
For 2012, CMS has currently taken over 20 inverse femtobarns of data, and the run isn&#39;t over yet! We have also simulated more than 6 billion events this year. Over the last month, the Large Hadron Collider has performed well.  We finished the heavy data processing for the major winter conferences, including the Hadron Collider Physics Symposium in Kyoto and begun reconstructing one of the primary datasets in the background that was &quot;parked&quot; at CERN for later processing. We are also ramping up the integration of new services, such as !gLExec to facilitate glideinWMS usage, !XRootD to enable a global namespace and data storage system, and !CernVMFS to make the distributed software installation process simpler.

 _~ [[mailto:burt@fnal.gov][Burt Holzman]]_


----+++*From the Executive Director*
 
We are pleased to let you know that based on the input of a core group the Executive Team is currently planning to restructure the software area in a way that we believe will more effectively meet the needs of the community.  Part of this restructuring will be to expand the scope and responsibilities of the Technology Area. To this end Brian Bockelman has been added to the Executive Team group and I look forward to his expanded and continuing contributions as part of the leadership of the project.
 
_~ [[mailto:bauerdick@fnal.gov][Lothar Bauerdick]]_

 	 	 	 	 
 
 
 
 



-- Main.JemiseLockhart - 29 Nov 2012


 
          
 *OSG Newsletter, May 2012*

 *D0 Usage*

Since November 2005 the D0 experiment at Fermilab has generated over 2.3 billion simulated Monte Carlo events on the OSG via SAMGrid. The original pre-Grid production infrastructure has been successively enhanced to integrate non-Grid, OSG, and LHC Computing Grid (LCG) resources. A single submission system accepts end-user requests and submits jobs to the different computing resources. The required input datasets are automatically replicated to a local storage element.

The use of opportunistic OSG computing has made a significant contribution to over 230 papers published by D0, with more to come. Although data taking for the experiment has ended, analysis of the data is ongoing, aided by continued production of Monte Carlo on the OSG. 

_~ Robert Illingworth and Margaret Votava_


*New OSG PKI*

Planning for the new OSG public key infrastructure [[https://twiki.grid.iu.edu/bin/view/Operations/DigiCertPlanningDevAndImplementation][(PKI)]] is wrapping up and development is beginning. The team expects to have initial versions of the new PKI software and services ready for friendly testing in early June. Parties who are interested in being early testers are encouraged to join the email list in the URL below. 

 The new PKI will be ready for general usage in September. By February of 2013, it will take over from the current DOE Grids PKI.

 For details on planning, information about the weekly calls, or to join the email list for updates, please visit the following URL. You may also contact Ruth Pordes or Von Welch directly with any questions.

 _~ Von Welch_

*WLCG Technology and Software Collaboration* 

I will continue to coordinate the OSG contributions and interactions with the LHC Technology Evaluation Groups (TEGs), working very closely with US ATLAS, US CMS and ALICE-USA,  as the effort moves from reports to implementations. I am planning to present information from the OSG software and technology program at the monthly Grid Deployment Board (GDB) meetings. 

Don’t hesitate to give me your input and feedback here.

_~ Brian Bockelman_ 


*From the OSG Blogs*

   * Derek tells how to set up your own display of accounting information locally using the [[http://derekweitzel.blogspot.com/2012/05/installing-your-own-gratia-webpage.html][OSG Gratia software]]. He seems to have been travelling a lot with reports also of Condor Week and Chep on his site.. one of the side benefits of being deeply engaged with OSG??

   * Several bloggers reported on their experiences at [[http://blogs.grid.iu.edu/][Condor Week]].  

   * The Grid Operations Center crew have been experimenting with subscription messaging/alerting frameworks with a view to improve our [[http://insideosgops.blogspot.com/][operations services]] in those area.  If you are interested in trying these out do contact them – submit a ticket.

   * Our [[http://research.cs.wisc.edu/condor/][Condor]] external provider has been releasing new versions   – as you know OSG is a heavy user and is always seeking new features and bug fixes.. we encourage these to keep coming..

*Featured Site : Texas Tech University*

Site name:[[http://www.tntech.edu/][Texas Tech University]] (TTU)

Location: Lubbock, Texas

Owner: TTU High Performance Computing Center (HPCC), collaborating with domain scientists in various fields, primarily High Energy Physics (HEP), computational chemistry, biosciences, engineering, climate modeling and atmospheric research

Number of Cores: 10,596 (of which 930 are accessible at present to OSG either directly or through the SURAgrid VO)

Used by: USCMS, SURAgrid, DREAM, LIGO, DZero and SBGrid
Opportunistic Access: Available to all OSG VOs, with precedence managed by a hierarchy of local, grid and subordinate queues.  

 What is the best part of collaborating with and contributing to OSG?
We enjoy learning from our colleagues at other OSG sites, exchanging information about best practices with other members of the VOs in which we participate, and having access to a community that is dedicated to developing the best practices for collaborative VO-based computing.

 What is unusual about your site?
We maintain a TOP500-ranked cluster in addition to our OSG and other grid resources.

 What are the greatest challenges your site faces?
We are a long way from anywhere (300 miles to the nearest other major facility), and constantly have to work on obtaining the best possible network access to our remote location.

 What projects are you working on at your site?
Network and storage upgrades; creation of a dedicated high-speed network to link our various on-site and off-site science computing facilities.  Substantial storage speed upgrades.

_~ Alan Sill_

*From the Council Chair* 

I am delighted to be now working with the OSG Consortium, Council, and Executive Team as the new Council Chair. I look forward to our continuing as an active and growing organization with Lothar as the new Executive Director, Miron as the PI of the continuing core project, and the Satellites that are a key part of our distributed high-throughput computing planetary system. 
 
 I want to express my deep appreciation for all of the contributions and leadership of Paul Avery over many years in the iVDGL/GriPhyn, Trillium, and Open Science Grid projects. Paul will continue as a Council member, and I look forward to continuing to benefit from his wisdom and ideas.  Also, many thanks to Rick Snider for his work as co-chair of the Council over the past year. 

 As Lothar said last month, we have the support from both DOE and NSF to continue the project for the next five years – pending annual reviews and funds available of course!  I have been polling the Council members for their priorities for our work and look forward to hearing from more of you “out there” on your views.  

 OSG Consortium members and projects had significant presence in presenting at the recent Condor Week and Computing in High Energy Physics conferences, and Dan Fraser spoke at the Elsevier Conference on Big Data, E-Science and Science Policy in Canberra, Australia. (See presentations list at the document repository link below).  

_~ Ruth Pordes_

*Interesting Talks to Read*

 [[http://research.cs.wisc.edu/condor/CondorWeek2012/presentations.html][Condor Week 2012]] – Lothar Bauerdick and Miron Livny

 [[http://www.chep2012.org/][CHEP 2012]] – Brian Bockelman and Ruth Pordes


*From the OSG Document Database:*

OSG-doc-#	Title	Author(s)	Topic(s)	Last Updated
1110-v1
OSG Laboratory for Policy Makers
Dan Fraser
Outreach
01 Jun 2012
783-v5
CMS and ATLAS Monthly Metrics
Brian Bockelman
Metrics
31 May 2012
1108-v1
CHEP 2012
Executive Board 
Reports
30 May 2012
1063-v1
Year6A Work Plan Source Data
OSG Executive Board et al.	Year Six Workplans
Governance
Work Plans
22 Dec 2011
1086-v2
OSG Strategic Accomplishments 2006-2011
- 
Reports
20 May 2012
1109-v0
CA Transition Contingency Plan, Risk Analysis, Response Procedures
Keith Chadwick
General Operations
15 May 2012
1107-v1
Open Science Grid talk at the 2012 Condor Week
Lothar A.T. Bauerdick
Outreach
08 May 2012







-- Main.JemiseLockhart - 13 Jun 2012

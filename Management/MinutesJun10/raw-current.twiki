-- Main.RuthPordes - 10 Jun 2005

-- Main.RuthPordes - 27 May 2005

---++Introduction
Minutes of the Management meeting, Jun 10, 2005. 

   * MeetingMinutes

---++Coordinates

2:30pm Central, 1-510-665-5437 #6740

---++Attending
Booker Bense, Mike Shuey, Leigh, Brian (TTU), Razvan, Abhishek, Dane, Rob, Kent


---++Agenda

---+++Reports from Technical Groups and Activities
   * Provisioning. 
     Leigh has been working on the cache. Need VO information for the static information. Only a few VOs have set up their infrastructure. Need production 
&lt;a href=&quot;http://osg.grid.iu.edu/OSG/index.php?option=com_content&amp;task=view&amp;id=51&amp;Itemid=67&quot;&gt; VO Information &lt;/a&gt; at
Liaisons need to register and get information to Leigh asap. Send mail to VO-ADMINS and any responses by 9 am are included.  Need to address this before Monday at 9am for the default installation. How much effort is it to add it after the fact? Easy at the cache. For site installer means a reinstallation of a particular package. List of VOMS updated in the same fashion as the CRLS? yes, this can be done. GUMS plus infrastructure that needs to be clarified.
    What version of OSG should this be - OSG 0.2.1 which would show on GridCAT. 
    Are there instructions for how to &quot;desupport&quot; a VO? The site admin edits the file and removes the information. If there is a problem with a VO where is the information about the VO support center? ATLAS will discuss their support center 10 days from now.
    LIGO example. Will not have a VOMS server. Would like to let other VOs use the hardware. 
    Use Operations meeting on  Monday pm with a list of VOMS to include in the 0.2.1 group and the decision will be made at the operations meeting. Invite VO-Admins to attend with specific information. Who will write the email?  Will point to 
&lt;a href=&quot;http://osg.grid.iu.edu/OSG/index.php?option=com_wrapper&amp;Itemid=71&amp;elMenu=Grid%20Support&quot;&gt; Support Centers &lt;/a&gt;

    Scheduling migration has been delayed so we will allow people to self-schedule and come when they are ready. Can resources who have 0.1.6 deployed reregister with the OSG GridCAT. Preference is that you reinstall with 0.2.1 but they can bring 0.1.6 resources to production labelled as such. Is there a verification process? Using siteverify.pl and publish in GridCAT. Who decides if you make the requirements? Can one be grey in one GRidCAT and green in a separate GridCAT. If you had 2 gatekeeper installations then you can do this, this is equivalent to two headnodes on one cluster.
    Can a site - gatekeeper - advertise itself to OSG and the ITB. Is monitoring affected? What about the VOs that are supported - GridEX is not supported on OSG but is required on ITB. Need an arrangement between operations and Site to discuss. For UCSD it is actually a decision for the US CMS support center. The operations model will be exercised during deployment. 
   Provisioning of the Documentation for Operations needs to be completed. We need to revisit forking the documentation from Integration. Still expect improvement in the documentation during deployment. In the not too distant future ITB will move on and will need separate document. 
   Provisioning phase is not complete until the documentation process is complete.
   Requirements and policies for setting up the system are not part of installation - at the moment they are included as recommended recipes. Formal requirements are in the deployment document. Is this information extracted for the operations page? What is the appropriate meeting for the documentation discussions: OSG-DOCs. Is this part of the distributed operations model and Support Centers Technical. May be able to put into the location what Ligo would like to have? Something like the User Guide from Anne Heavey in Grid3. Need an outline set up with the basic infrastructure would be a good thing and getting the layout right and categories would be helpful and we can fill it in as time goes on. 
Provision Operations Documentation. Dane will send the provisioning link to the Management group. &lt;a href=&quot;http://osg.ivdgl.org/twiki/bin/view/Provisioning/HowToGetStarted&quot;&gt; move to operations &lt;/a&gt;

   * Operations 
   * Integration - will be available to help with provisioning and operations. Will be continuing things coming up which will warrant continued integration focus; GT4 GRAM; GIP configuration documentation; Does deployed infrastructure support measuring the metrics?  Is ACDC a source of information for metrics? Then we will need to put some energy behind this or is this an opportunistic VO applications? What are the accounting requirements for 0.2 deployment? Does ACDC depend on MIS-CI?  Who is presenting the accounting for OSG - Sudhir. Ask Iosif to have the OSG MonaLisa to point to the OSG sites. 

   * MIG. 
   * Storage. Razvan. Would like to have SRM-CP client to be part of the deployment and to use the OSG Cache to deploy this service. Would like some attention from Trash/Trash/Integration when the package is available.

   * Interoperability - Mike 
      * LCG Interoperability is proceeding. ATLAS and CMS jobs are being run on LCG and TeraGrid as well on OSG. LCG Interoperability is structured using the GlueSchema,  GIP, RB to gateway between LCG and OSG. LCG looks like a single site and everything sent towards the OSG is gatewayed through one node through the LCG and vice-versa. Iowa doing this work for CMS. Will distribute jobs at the backend. Collect all the monitoring information in a central place and doing the translation. Ruth should post the documents and get some more information posted. Documentation needed. Next phone con is Thur 16th. TeraGrid DAC - exploratory allocation. cycles at Purdue, TACC, ANL. Greg Cross, Chris, Shaowen - running ATLAS and CMS applications on TeraGrid resources, but need to address finer details of job monitoring. Late June hope to have rough ideas on this. Will submit a much larger allocation requests in July. CMS user can submit jobs to the TeraGrid? No automatic job migration from OSG to TeraGrid resource. Recommendation that before larger allocation request the a) Authentication b) Authorization b) Policy and c) Job Scheduling gateways are solved. This application is regarded as a Science Gateway by TeraGrid. We need to use this as a means to define a partner grid. currently installing applications on the TeraGrid and gateway jobs. Any larger allocation for US CMS would need to have successful tests from Craig for production etc. 
     Are there any policy type show stoppers: Difficulties are the VO group accounts and representation; Accounting for a class of user; 


Applications:
  OSG 0.2.1 US ATLAS says this is ok; It has been validated on a number of sites; Slac it is fine. Kent says it is ok.

---++Issues

   * 

---++ Action Items


&lt;!-- MAJOR UPDATES
For significant updates to the topic, consider adding your &#39;signature&#39; (beneath this editing box) !--&gt;

*Major updates*:%BR%
&lt;!--Future editors should add their signatures beneath yours!--&gt;
-- Main.Ruth - 10 June 2005

%STOPINCLUDE%

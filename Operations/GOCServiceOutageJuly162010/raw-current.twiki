---++Outage Report, Thursday 15 July, 14:33 through 18:45 EDT

---+++ Incident Details 

At 14:33 EDT (18:33 UTC) July 16th, several GOC services became unavailable. The situation was corrected at approximately 18:45 (22:45 UTC) and the community notified of service availability at 20:00 (0:00 UTC)

The services affected were:
   * !MyOSG
   * !TWiki
   * OIM
   * OSG Ticket Exchange
   * OSG Display
   * MIS and OSGEDU VOMS
   * GOC Ticket
   * GOC software cache

*NOTE: The BDII was not affected.*

The initial outage notification can be seen [[http://osggoc.blogspot.com/2010/07/goc-service-outage-july-15-2010.html][here]]
and the resolution notification can be seen [[http://osggoc.blogspot.com/2010/07/goc-services-restored.html][here]].

---+++ Cause

The cause of the failure has been determined to be associated with the automatic propagation of an improperly constructed set of configuration files. Every five minutes all servers (on both physical and virtual machines, production and ITB) check whether or not the local copy of a .tar file matches that on a central machine. If a mismatch is detected, the server synchronizes its local copy with the central copy and installs the newly fetched configuration. A change to the script constructing the central .tar file to implement additional capability was being made at the time of the failure. The remote servers were erroneously allowed to continue their periodic testing against the central .tar file. At update time the .tar file was in an unknown state, propagated to the remote machines and installed leading to the failure of the services.

---+++ Lingering Issues

It was discovered postfix did not restart properly on the GOC Ticket machine. This was restarted on Monday morning.

---+++ Steps Taken During the Outage
 
The problem was observed at 14:36 but the erroneous configuration had propagated to most servers by that time. *Automatic propagation was suspended* but the servers had already failed. Details of the failure mechanism are being investigated but symptoms included failure of ssh and the inability of root to login. This destroyed the ability of the servers to communicate with each other and respond to outside requests.

*The immediate solution was to suspend automatic updates, replace the erroneous configuration files and restart the servers and services. Automatic update remains suspended* and a test virtual machine is being configured to allow further investigation of the failure mechanisms.

---+++ Long Term Solutions

As a longer-term solution, *configuration updates will hence occur only during regularly scheduled maintenance windows* and will be initiated by the system administrator rather than automatically. Currently, it is not possible to propagate changes to a subset of servers. For example, production and ITB servers all have the same configuration. Until the capability to update selectively is implemented, all servers will continue to share a common configuration. (Adding this ability was being investigated at the time of the failure) Once this capability exists, ITB and production servers will be updated independently during their respective maintenance windows. It is not anticipated that this relatively infrequent interval will effect services adversely, for example, the previous update occurred on July 9, 14:20 UTC, one week before the event causing this outage.

---+++ Failure analysis update

It has been determined that the actual mechanism of failure was not directly associated with the *contents* of the propagated .tar file but rather the permissions given to &quot;/&quot; on the effected machines. Read/execute access for &quot;others&quot; was removed (recursively, starting from &quot;/&quot;)  by tar as the archive was unpacked leaving the machine in an unusable state.

Basic system services (for example, postfix) have been examined on all machines and restarted where required. A script has been written that verifies all services listed as requested to start at boot time have, in fact, started. If it is found that they did not, they are started in the proper order. A re-run of the script verifies all services are present. Execution of this script is now part of the restart SOP.

---+++ Follow Up Items

   * Cron Audit - OSG Ops audited all cron jobs running on OSG service housed at Indiana University. This review is attached.
   * OSG Ops War Room Procedures - OSG Ops has created dedicated coordinates for phone and IM communications during a Operations Incident. These coordinates are being tested and will be published to the entire community shortly. 

-- Main.RobQ - 16 Jul 2010


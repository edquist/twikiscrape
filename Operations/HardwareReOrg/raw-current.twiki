
%TOC%

---+ OSG Operation Center Internal Hardware Reorganization Project Plan

Version Control

| Document Version | Author | Notes |
| 1.0 | Rob Quick | First Draft |
| 1.1 | Tom Lee and Tim Silvers | Updates to Draft |
| 1.2 | Rob Quick | Move to TWiki and Updates |
| 1.2.1 | Arvind Gopu | BDII based updates | 

---++ Background and Motivation

The Grid Operations Centers (GOC) for the Open Science Grid (OSG) origins dates back to previous grid projects such as Grid3 and iVDGL. During this time grid infrastructure services and hardware resources available to the GOC has grown in an organic fashion. This has caused a mixing of both service structure and hardware architectures. This project plan is an attempt to consolidate them into a homogenous production and service development environment, provide a stable and consistent internal monitoring structure, to improve security procedures, and to set a path toward failover and disaster recovery mechanisms for all services provided by the GOC.

---++ Goals

---+++ Separate Development and Production Services
 Currently there are several machines that have mixed production and development services. This will be stopped. Machines will be monitored specifically based on the criticality of the services running on the machine. The Indiana University Grid Infrastructure Group will provide warranty resources for all critical machinery.

---+++ Move to a Common Operating System on all Hardware
 Red Hat Enterprise Linux 5 will be the preferred operating system used by the GOC. Updates and patches will be provided by the Indiana University Grid Infrastructure group along with administrator resources.

---+++ Deploy Virtual Machines
 For future growth and ease of redundancy, virtual machines will be used to house all development services. This will also allow us to more easily more toward service redundancy on production services in the future.

---+++ Provide a Standard Security Policy for Hardware and Service Architecture
 A consistent hardware security policy will be set up based on recommendation from the IU security office policy and procedure. Sensitive data security requirement will be discussed with the OSG Security Officer and be implemented as necessary.

---+++ Provide Response Monitoring for Hardware and Services
 An internal fabric monitoring system will be implemented. This will include status and metric monitoring based on Nagios and Munin open source software. Monitoring response policy will be set using the criticality scale explained later in this document.

---+++ Move to Failover between IU-Bloomington and IUPUI for Services Located at Indiana University
 Critical services will be positioned to set up a failover plan. Separate physical locations will be used.

---++ Services and Service Criticality

---+++ Criticality for Services

| Criticality | Rank | Discription | Response Time |
| Critical | 4 | Services considered critical to a properly function grid. These include services depended on by VOs to run jobs and complete work. Monitoring services that alert of critical service failure are critical. | 60 Minutes |
| High | 3 | Services that are essential to a smoothly operating grid. Outages would not cause loss of work, but would inconvenience the community&#39;s operation. | Next business day (Priority) |
| Low | 2 | Services that are non essential to grid operation, but necessary for grid use. These include documentation services and support services. | Next business day (Non-Priority) |
| Development or Testing Services | 1 | These are services unnecessary to the production OSG. | Best effort |

---+++ Current Service Listing

| Service Name | Criticality | Physical Location | Currently Redundant | Need for Redundancy | Notes |
| GOC Mail | 4 | IUB/IUPUI | Y | Y | |
| Footprints Ticketing | 4 | IUB/IUPUI | Y | Y | |
| OSG CEMon/BDII Aggregator | 4 | IUB | N | Y | |
| GOC VOMS | 4 | IUB | N | Y | |
| GOC GUMS | 4 | IUB | N | Y | |
| OSG TWiki | 4 | IUB | N | Y | |
| ReSS | 4 | FNAL | N | Y | |
| VORS | 4  | IUB | N | Y | |
| GOC Nagios | 4 | IUB | N | Y | |
| RSV Gratia | 3 | IUB | N | Y | |
| OIM | 3 | IUB | N | N | |
| OSG Software Cache | 3 | IUB | N | N | |
| GridScan | 3 | IUB | N | N | Supplies data to VORS interface |
| Gratia Accounting | 3 | FNAL | N | N | |
| VOMS Monitor | 2 | IUB | N | N | |
| GOC Web Pages | 2 | IUB | N | N | |
| GIP Validator | 2 | IUB | N | N | |
| ITB/VTB CEMon/BDII Aggregators | 2 | IUB | N | N | |
| Development and Testing Services | 1 | IUB/IUPUI | N | N | |

---+++ Current Structure Upgrading and Maintaining Services
 Currently complicated by not having development environments to thoroughly test changes before being implemented in the production environments. Additionally, there are further complications from having highly critical production services running on the same machines with non-critical services. Monitoring of these services could be improved if these were separated.

---+++ Proposed Structure
 In the proposed environment production services will be separated from development services. Therefore, development environments for each production service will need to be developed if they are not already in existence. Furthermore, services will be grouped onto machines with similar criticality. For example, a service of a criticality 3 will not be hosted on the same server with services of criticality of 4, 2 or 1. However, note that in some cases a mixed environment will be unavoidable. But efforts should be made to not put services at opposite ends of critical service level on the same machine. For instance, a service of criticality 1 should not be on the same machine as a 4. The highest criticality services should be hosted on their own hardware, or with as few other services as possible. No more than two high criticality services should exist on the same machine.

Separating services so that they reside on machines in this manner will allow us to greatly improve our internal monitoring, and subsequently, our response time to failures or issues. A failure on a machine which only hosts high criticality services can be monitored differently from a machine that hosts lower criticality services. In other words, if a high criticality machine fails on monitoring, the alert tools and processes can provide direct paging, special internal procedures, email alerts to select addresses, etc. Some of this will be accomplished through tools like Nagios which will automate more of these processes, leaving less to human error or potential delays.

---++ Hardware and Hardware Criticality

---+++ Criticality for Hardware

| Criticality | Rank | Discription | Response Time |
| Critical | 4 | Machine housing a critical (4) criticality level service. | 60 Minutes |
| High | 3 | Machine housing a high (3) criticality level service | Next business day (Priority) |
| Low | 2 | Machine housing a low (2) criticality level service | Next business day (Non-Priority) |
| Development or Testing Services | 1 | Machine housing a development or testing service. | Best effort |

---+++ Hardware Listing

| Machine Name | Services | Production or Development | Critically Monitored | Notes |
| Funafuti | VORS DB | Production | Y | |
| Nukufetau | VORS Web Pages, OSG SW Cache, VOMS-Monitor, GOC Web Pages | Production | Y | |
| Grid01 | GridScan | Production | N | |
| FUI | GIP Validator, VTB CEMon/BDII Aggregator | Mixed | N | |
| Dahmer | VORS Development | Development | N | |
| Bundy | CEMon/BDII Aggregator | Production | Y | |
| Iggy | SSH Gateway, Wombat, Nagios, Other monitors: VORS failure rate monitor &amp; Resource Status Report script | Mixed | N | This is sort of becoming a general monitor machine in addition to being our jump-off point. |
| Wombat | | Development | N | Should go away with Munin implementation |
| NUI | Trash.ReleaseDocumentationMonALISA | Unknown | Y | |
| Son of Sam | OSG RSV, OIM Registration DB | Production | Y | |
| Madman | OIM Development, RSV Development Development | Development | N | |
| Ruckus | Syslog-NG | Development | N | |
| Jazmine | VOMS/GUMS | Production | Y | |
| Grandad | Xen VM experimentation | Development | N | Once Xen is up and running, the VMs on this host could serve as multiple low-use development machines.  |
| Huey | | | | |
| Riley | | | | |
| Wuncler | Twiki Development | Development | N | Used to be grid03 |
| Dubois | | | N | If I recall, this is just a DNS. We were going to use the old TB03 or Bats, but those machines were old and therefore, salvaged. |

---+++ Current Structure

The following is another table of the servers as they currently stand, with emphasis on the fact that services of different levels of criticality are located together on the same server, making the server&#39;s criticality level ambiguous. Where there are problems with this ambiguity or other issues, this is noted, and our recommendation for a course of action is noted as well. Listed also is each server&#39;s warranty expiration date, which may have a bearing on any plans.

| Server | Server Criticality | Warranty Status | Problems | Recommendation |||||||
| Funafuti | 3-4 | Expires 6/25/09 | VORS on same server as different-criticality registration forms and DB Becomes a clear-cut 3 when VORS is superseded | |||||||
| Nukufetau | 2-4 | Expires 6/25/09 | Wide range of criticality | Move VOMS monitor and GOC web pages to Fui; becomes a clear 3 when VORS is superseded | ||||||
| Grid01 | 3 | Expires 10/6/09 | Runs only one service | Move Gridscan to Nukufetau; repurpose this machine |||||||
| Fui | 2 | Expired | Combines production service with semidevelopment service | VTB CBA can move to a VM on Grandad; VOMS monitor and GOC web pages move here |||||||
| Dahmer | 4 | Expires 10/6/09 | | Leave as-is (may want to update OS) |||||||
| Bundy  | 2-4 | Expires 10/6/09 | Combines critical production service with semidevelopment service | Move ITB CBA to a VM on Grandad, making this machine a clear-cut 4 |||||||
| Iggy | n/a | Expired | | May want to move monitoring to a more secure server such as Grid01 or Madman |||||||
| Nui | 1 | Expires 6/25/09 | | Leave as-is (may want to update OS, or wait until Trash.ReleaseDocumentationMonALISA dies a natural death) |||||||
| Sonofsam | 4 | Expires 10/6/09 | | Leave as-is (may want to update OS to RHEL 5.1 instead of 5.0 to match madman) |||||||
| Madman | 1 | Expired | Development machine for two separate services | Move development services to separate VMs on Grandad |||||||
| Wuncler | 1 | Expired | Entire machine devoted to low-use development service | Move Twiki development to a VM on Grandad |||||||
| Ruckus | n/a | Expires 5/10/10 | syslog-ng underutilized; GI has offered to let us use their syslog-ng server | Possibly use GI&#39;s syslog-ng server for centralized logging, freeing up this machine for something else |||||||
| Jazmine | 4 | Expires 5/10/10 | | Leave as-is (may want to update OS) |||||||
| Grandad | n/a | Expires 5/10/10 | | Once properly configured, create several VMs and move development to them |||||||
| Huey | n/a | Expires 5/9/10 | | |||||||
| Riley | n/a | Expires | 5/9/10 | | ||||||

4.4. Proposed Structure

The following is our proposal for the eventual distribution of GOC services. Mixed servers will be a thing of the past; servers will contain only services of the same criticality level, allowing each server itself to have a well-defined criticality level, allowing monitoring and support to be targeted much more precisely.

The fact that VORS will be superseded in the near future is a boon to the new structure; its fragility and dependence on multiple servers is one of the factors contributing to the current ambiguity of the servers&#39; criticality.

In the following table, each service is listed with its criticality level following it in parentheses. The term %u201CCEMon/BDII Aggregator%u201D has been abbreviated CBA for brevity.

| Server | Services and Criticality | Server Criticality | Notes ||
| Dahmer | Unused (1) | 1 | || 
| Bundy| OSG CBA1 (CEMon/BDII Aggregator 1) (4) | 4 | | One of this or CBA2 should always be alive |
| Riley| OSG CBA2 (CEMon/BDII Aggregator 2) (4) | 4 | | Second round-robin CBA server; One of this or CBA1 should always be alive |
| Jazmine | VOMS/GUMS (4) | 4 | ||
| Sonofsam | OSG RSV (4), OIM reg. DB (4) | 4 | ||
| Nukufetau | OSG software caches (3) (incl. CA certs?), Gridscan (3) | 3 | ||
| Fui | GIP Validator (2), VOMS Monitor (2), GOC Web Pages (2) | 2 | As this machine&#39;s warranty has expired, may want to use a newer one instead %u2013 or may want to leave it like this, as it&#39;s only a 2. ||
| Grandad (VMs) | VTB CBA (2), ITB CBA (2), OIM development (1), RSV development (1), TWiki development (1) | 2 | Xen VMs, each with own external and internal IP (if required), each running RHEL 5.1 (or same OS as production in any case). May want to use one physical server for the 2s and another for the 1s. ||
| Nui | Trash.ReleaseDocumentationMonALISA (1) | 1 | ||
| Iggy | SSH gateway n/a Madman, Nagios monitor, Munin monitor, Centralized config. | n/a | ||
| Funafuti | Freed up for repurposing | | ||
| Grid01 | Freed up for repurposing | | ||
| Ruckus | Possibly freed up for repurposing | | ||
| Wuncler | Freed up for repurposing | | ||
| Huey |Twiki server (4) - is this change permanent? | 4| Located at IUPUI ||

---++ Process and Timeline

---+++ Preliminary steps
   * CA, Firewall, and User synchronization. 

---++ Future Directions

-- Main.RobQ - 26 Aug 2008

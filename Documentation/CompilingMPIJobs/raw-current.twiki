%LINKCSS%

---+!! %SPACEOUT{ &quot;%TOPIC%&quot; }%
%TOC%

%STARTINCLUDE%

%WARNING% *The contents of this twiki page are outdated, and are kept for historical purposes.  Please exercise caution prior to starting a new project based on this information.  Instead, consider whether [[Documentation.HighThroughputParallelComputing][High-Throughput Parallel Computing]] might work for you instead.*

---++ Picking A Site

The first step is to pick an OSG site that supports your VO and has an MPI version installed. The easiest way to do this is decide on an MPI implementation (MPICH, MPICH2, OpenMPI, etc.) and perform an LDAP query. For example, to see all of the MPICH versions installed on Purdue&#39;s Steele cluster, the following command can be used:

&lt;pre class=screen&gt;
tg-steele$ ldapsearch -x -l 60 -b mds-vo-name=Purdue-Steele,mds-vo-name=local,o=grid -h is.grid.iu.edu -p 2170 &quot;GlueSoftwareName=MPICH&quot; GlueSoftwareEnvironmentSetup
&lt;/pre&gt;

The &quot;GlueSoftwareEnvironmentSetup&quot; field shows what command to use in order to load the MPI version into your environment. This is important for compiling and running the application.

---++ Compiling MPI Jobs

Compiling the application can be done in one of two ways. If the site allows local logins, you can login directly to the server, run the command to load MPI into your environment, and compile your application. This is essentially the same workflow you would use to run the application on your own machine.

Since most sites don&#39;t allow local logins, however, another way to compile your application is to submit a batch job that will compile the application for you. This can be accomplished by writing a short script that will source the module and compile the application. The following is an example script that will compile the cpi program.
---+++ A Sample Compile Script

&lt;pre class=screen&gt;
#!/bin/bash
#
# Right now on Purdue&#39;s Steele cluster, the modules program is not in the user&#39;s
# path when a job is run. In order to ensure the module command works, we need
# to source the module setup script. For sites using softenv, sourcing
# /etc/profile.d/softenv.sh should work instead.

source /etc/profile.d/modules.sh
# This is where the command from GlueSoftwareEnvironmentSetup goes
module load mpich-gcc

mpicc -o cpi cpi.c
&lt;/pre&gt;

---+++ A Sample Submit Script

Once you have a script that will compile your application, you can submit a Condor-G job to compile your application. For example, the following submit script will compile and return the working executable for the above compile script on Purdue&#39;s Steele cluster:

&lt;pre class=screen&gt;
Universe = grid
Grid_Resource = gt2 lepton.rcac.purdue.edu/jobmanager-pbs

Executable = compile.sh

Output = compile_job.out
Log    = compile_job.log
Error  = compile_job.error
WhenToTransferOutput = ON_EXIT
Transfer_Input_Files = cpi.c,compile.sh
Transfer_Output_Files = cpi

Queue 
&lt;/pre&gt;

Once the Condor-G job ends, you&#39;ll most likely have to make your program executable by running:
&lt;pre class=screen&gt;
chmod +x &lt;program_name&gt;
&lt;/pre&gt;

---++ Running your application

Now that you&#39;ve compiled the executable, it&#39;s time to run the job. The following RSL attributes need to be included in your Condor-G job:
   * jobType=mpi
   * handle=MODULE_NAME
   * directory=JOB_DIRECTORY

Here is an example submit script for a simple &quot;cpi&quot; application:

&lt;pre class=screen&gt;
# file: cpi.submit
Universe = grid
Grid_type= gt2

# Set Scheduler to the proper resource
GlobusScheduler = lepton.rcac.purdue.edu/jobmanager-pbs

# Make sure to set handle to the appropriate software module
GlobusRSL = (jobType=mpi)(handle=mpich2-gcc)(xcount=2)(host_xcount=1)(directory=/home/ba01/u100/ahoward/testjobs/cpi)

Executable = /home/ba01/u100/ahoward/testjobs/cpi/cpi
Arguments = -i 10000 

Stream_output = False
Stream_error = False

WhenToTransferOutput = ON_EXIT
TransferExecutable = False

Output = cpi.out
Error = cpi.err
Log = cpi.log

Notification = NEVER
Queue
&lt;/pre&gt;

Finally, the job can be started by running:

&lt;pre class=screen&gt;
tg-steele$ grid-proxy-init
tg-steele$ condor_submit cpi.submit
&lt;/pre&gt;

%STOPINCLUDE%

%BOTTOMMATTER%

-- Main.AndrewHoward - 04 Dec 2008


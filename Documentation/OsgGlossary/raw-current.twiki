---+ Glossary of OSG and Grid Computing Terms (as used in OSG)  

%RED%This is the direct copy from the osg website glossary (before moving it to twiki); obsolete and replaced by GlossaryOfTerms%ENDCOLOR%

%INCLUDE{&quot;GlossaryJumpIndex&quot;}%

&lt;p&gt;To include a term in the OSG Glossary, please &lt;a href=&quot;mailto:osg-docs@opensciencegrid.org&quot;&gt;email osg-docs&lt;/a&gt;         

#LtrA
 &lt;b&gt;Accounting (grid accounting)&lt;/b&gt; &lt;br /&gt; 				The OSG accounting system tracks VO members&amp;#39; resource usage and presents that information in a consistent Grid-wide view, focusing in particular on CPU and Disk Storage utilization.&lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;ACDC Job monitoring&lt;/b&gt; &lt;br /&gt; 				An application using grid submitted jobs to query the job managers and collect info about jobs. This info is stored in a DB and available for aggregated queries and browsing.&lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;Administrative Domain&lt;/b&gt; &lt;br /&gt; 				This refers to a set of personnel responsible for the maintenance of a given site who can answer questions, solve problems, and so on.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Agent &lt;br /&gt; 				&lt;/b&gt;A software component in OSG that operates on behalf of a User or Resource Owner or another Agent.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Alliance&lt;br /&gt; 				&lt;/b&gt;A collaboration of small application communities that develops systems and runs on the persistent grid infrastructure. &lt;!--(changed) late apr --&gt;&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt; 					     Application&lt;/b&gt; &lt;br /&gt; 				With respect to grid computing in general, &amp;quot;application&amp;quot; refers to a &amp;quot;layer&amp;quot; of grid components (above infrastructure and resources). &lt;!--(new from Ruth) late apr --&gt;An application  is a name used to identify  a set of software that will execute computational jobs, manage data (access, store, read...) and has many attributes. Any application when invoked (executed) includes information that allows tracing back to the individual who is responsible for the execution. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Application administrator&lt;/b&gt; &lt;br /&gt; 				A person designated by a VO who is charged with making sure that a particular application works on the participating grid resources.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Application community&lt;/b&gt; &lt;br /&gt; 				Providers of a particular end-to-end application and/or system that runs on the persistent grid infrastructure. Smaller application communities may collaborate as Alliances in developing systems and running them on the grid. These organizations will contribute by providing application requirements and interfaces to the grid services. An application community may consist of or span multiple VOs or VO groups &lt;!--(change per Alain/Ruth) mid apr --&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Application middleware&lt;/b&gt;            &lt;br /&gt; 				&lt;!--(new from Ruth) mid apr --&gt;     Application Middleware is application-specific middleware which has some embedded capabilities and interfaces that are not general, e.g., information providers and configuration of MDS.&amp;nbsp; This middleware depends on grid-wide middleware.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;ARDA&lt;/b&gt; &lt;br /&gt; 				Architectural Roadmap towards Distributed Analysis (ARDA) is a project within LCG that seeks to coordinate the prototyping of distributed analysis systems for the LHC experiments. (&lt;a href=&quot;http://lcg.web.cern.ch/LCG/peb/arda/Default.htm&quot;&gt;http://lcg.web.cern.ch/LCG/peb/arda/Default.htm&lt;/a&gt;)           				&lt;br /&gt; 				&lt;div class=&quot;head&quot; id=&quot;Layer1&quot;&gt; 					&lt;br /&gt; 					&lt;b&gt;Atlas&lt;/b&gt; &lt;br /&gt; 					A Toroidal LHC ApparatuS experiment at the LHC at CERN (&lt;a href=&quot;http://atlas.web.cern.ch/Atlas/Welcome.html&quot;&gt;http://atlas.web.cern.ch/Atlas/Welcome.html&lt;/a&gt;) &lt;br /&gt; 					&lt;br /&gt; 					&lt;b&gt;Auditing (grid auditing)&lt;/b&gt; &lt;br /&gt; 					Grid auditing in OSG relates to resolving claims of challenged authentication and exposed risk on grid services which accept delegated credentials. The auditing system will use information from the accounting system and link it to information from other sources to allow full tracking and analysis of the actions and events related to a user&amp;#39;s resource usage.&lt;br /&gt; 					&lt;br /&gt; 					&lt;b&gt;AUP&lt;/b&gt; &lt;br /&gt; 					Acceptable Use Policy&lt;/div&gt; 		
%INCLUDE{&quot;GlossaryJumpIndex&quot;}%		
#LtrB	
&lt;b&gt;BDII&lt;/b&gt; &lt;br /&gt; 				(&lt;a href=&quot;http://lfield.home.cern.ch/lfield/cgi-bin/wiki.cgi?area=bdii&amp;amp;page=documentation&quot;&gt;Berkeley Database Information Index&lt;/a&gt;) is an LCG implementation of Globus GIIS-like information index based on the Berkeley Database.    

*BeStMan*%BR%
Berkeley Storage Manager (BeStMan) is an LBNL implementation of SRM based on Unix disks and HPSS.
%INCLUDE{&quot;GlossaryJumpIndex&quot;}%
#LtrC
	&lt;b&gt;CASTOR&lt;/b&gt; &lt;br /&gt; 				The CASTOR Project stands for CERN Advanced STORage Manager, and its goal is to handle LHC data in a fully distributed environment. See &lt;a href=&quot;http://castor.web.cern.ch/castor/&quot;&gt;http://castor.web.cern.ch/castor/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;CE-SE binding&lt;/b&gt;            &lt;br /&gt; 				&lt;!--(change to CS-SS binding or leave out? Doesn&#39;t meld with GLUE terminology)  --&gt;     Job scheduling often requires both a compute element (CE) to run the job and a Storage Element (SE) to provide for an input or output storage extent. Currently     &lt;!--?? --&gt;     , there are static relationships between individual CEs and SEs set by Site Admins. The CE-SE bind schema (part of GLUE schema) aims at providing the means for publishing such a relationship with eventual per-pair data. At the moment, the published information is limited to the local mount point on the CE pointing to the SE&amp;#39;s storage space.     &lt;!--This has probably changed. --&gt;&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Certificate&lt;/b&gt; &lt;br /&gt; 				A public-key certificate is a digitally signed statement from one entity (e.g., a certificate authority), saying that the public key (and some other information) of another entity (e.g., the grid user) has some specific value.  The X.509 standard defines what information can go into a certificate, and describes how to write it down (the data format).  Read more at &lt;a href=&quot;http://java.sun.com/products/jdk/1.2/docs/guide/security/cert3.html#intro&quot;&gt; http://java.sun.com/products/jdk/1.2/docs/guide/security/cert3.html#intro&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Certificate Authority&lt;/b&gt; &lt;br /&gt; 				An entity that issues certificates. OSG recognizes certificates issued by a number of certificate authorities, e.g., DOEGrids Certificate Service at &lt;a href=&quot;http://www.doegrids.org/&quot;&gt;http://www.doegrids.org/&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Chimera&lt;/b&gt; &lt;br /&gt;  				The Chimera Virtual Data System (now known as The GriPhyN Virtual Data System) is a grid middleware component that combines a virtual data catalog, for representing data derivation procedures and derived data, with a virtual data language interpreter that translates user requests into data definition and query operations on the database. It is a GriPhyN deliverable, packaged and distributed as part of the Virtual Data Toolkit (VDT). Read more at &lt;a href=&quot;http://vds.uchicago.edu/twiki/bin/view/VDSWeb/WebMain&quot;&gt;http://vds.uchicago.edu/twiki/bin/view/VDSWeb/WebMain&lt;/a&gt;  . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Clarens&lt;/b&gt; &lt;br /&gt; 				The &lt;a href=&quot;http://clarens.sourceforge.net/&quot;&gt;Clarens Grid-Enabled Web Services Framework&lt;/a&gt; is an open source, secure, high-performance &amp;quot;portal&amp;quot; for ubiquitous access to data and computational resources provided by computing grids . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Cluster&lt;/b&gt; &lt;br /&gt; 				A networked group of worker nodes (plus head node, if applicable) at a site. In the GLUE schema, a cluster is a container that groups together subclusters, or computer nodes. A cluster may be referenced by more then one computing element (CE).&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;CMS&lt;/b&gt; &lt;br /&gt; 				Compact Muon Solenoid experiment at the LHC at CERN (&lt;a href=&quot;http://cmsinfo.cern.ch/Welcome.html/&quot;&gt;http://cmsinfo.cern.ch/Welcome.html/&lt;/a&gt;)&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Compute element (CE)&lt;/b&gt; &lt;br /&gt; 				Compute element (also called a compute service or CS) is a term used in Grids to denote any kind of computing interface, e.g., a job entry or batch system. A compute element consists of one or more similar machines, managed by a single scheduler/job queue, which is set up to accept and run grid jobs. The machines do not need to be identical, but must have the same OS and the same processor architecture. A CE must run (among other things) a process called the &amp;quot;gatekeeper&amp;quot;. A CE is defined with regards to the GLUE schema at &lt;a href=&quot;http://www.cnaf.infn.it/%7esergio/datatag/glue/v11/CE/GlueCE_DOC_V_1_1.htm&quot;&gt;http://www.cnaf.infn.it/~sergio/datatag/glue/v11/CE/GlueCE_DOC_V_1_1.htm&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Compute Node (CN)&lt;/b&gt; &lt;br /&gt; 				Individual host of a farm or cluster. Information about CN&amp;#39;s may or may not be visible to the Grid, depending on how the farm is managed. &lt;!--Individ host of a CE, rather?  april --&gt;&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Condor &lt;br /&gt; 				&lt;/b&gt;Condor is a specialized workload management system for compute-intensive jobs. Like other full-featured batch systems, Condor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. Condor is a grid middleware component developed at UW Madison, packaged and distributed as part of the Virtual Data Toolkit (VDT). Read more at &lt;a href=&quot;http://www.cs.wisc.edu/condor/&quot;&gt;http://www.cs.wisc.edu/condor/&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Consumer &lt;br /&gt; 				&lt;/b&gt;A User or Agent who makes use of an available Resource or Agent or Service. Contract Agreement between Consumer(s) and/or VO(s) and/or Provider(s) expressed through Policies. Simplest contract is a consumer-provider match based on their policies.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Trash.Trash/Trash/IntegrationCoreMIS&lt;/b&gt; &lt;br /&gt; 				The Monitoring and Information Services Core Infrastructure; this is released as MIS-CI.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;CRL&lt;/b&gt; &lt;br /&gt; 				(Certificate Revocation List) is one of two common methods when using a public key infrastructure for maintaining access to servers in a network. The CRL is exactly what its name implies: a list of subscribers paired with digital certificate status. The list enumerates revoked certificates along with the reason(s) for revocation. The dates of certificate issue, and the entities that issued them, are also included. In addition, each list contains a proposed date for the next release. When a potential user attempts to access a server, the server allows or denies access based on the CRL entry for that particular user. 				
%INCLUDE{&quot;GlossaryJumpIndex&quot;}%
#LtrD				&lt;b&gt;DAG&lt;/b&gt; &lt;br /&gt; 				A data structure used by DAGman (see DAGman) to represent dependencies; each job is a node in a DAG; each node can have a number of parent or children nodes but no loops. A DAG is defined by a .dag file, listing each of its nodes and their dependencies:&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;DAGman Directed Acyclic Graph Manager&lt;/b&gt; &lt;br /&gt; 				A Grid middleware component to manage interdependent Grid jobs, provided by the Condor team. It allows you to specify dependencies between your Condor jobs (e.g., don&amp;#39;t run B till A has completed successfully). Acts as a &amp;quot;meta-scheduler&amp;quot; managing the submission of your jobs to Condor, based on the DAG dependencies. DAGman is packaged as part of the Virtual Data Toolkit (VDT). Read more at &lt;a href=&quot;http://www.cs.wisc.edu/condor/dagman/&quot;&gt;http://www.cs.wisc.edu/condor/dagman/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Data federation&lt;/b&gt; &lt;br /&gt; 				The process of bringing data together in a single virtual location for on-demand application access. &lt;!--?? april --&gt;DC04 (CMS) CMS Data Challenge of February 2004, CMS Level-2 milestone, exercising the CMS data streaming at a 5% level, to verify the computing model and to prepare for the Computing TDR. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;DC2 (ATLAS)&lt;/b&gt; &lt;br /&gt; 				ATLAS Data Challenge of early 2004, see &lt;a href=&quot;http://www-theory.lbl.gov/%7Eianh/dc/dc2.html&quot;&gt;http://www-theory.lbl.gov/%7Eianh/dc/dc2.html&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;dCache&lt;/b&gt; &lt;br /&gt; 				DCache is a disk cache which provides a system for storing and retrieving huge amounts of data, distributed among a large number of heterogenous server nodes, under a single virtual filesystem tree with a variety of standard access methods. DCache is a joint venture between &lt;a href=&quot;http://www.desy.de&quot;&gt;DESY &lt;/a&gt; and &lt;a href=&quot;http://www.fnal.gov&quot;&gt;Fermilab&lt;/a&gt;. See &lt;a href=&quot;http://www.dcache.org/&quot;&gt;http://www.dcache.org/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Delegation&lt;/b&gt; &lt;br /&gt; 				An entrustment of decision-making authority during transfer of request for work or offer of resources from a User or Agent to another Agent or Provider, or vice versa. The latter is provided with a well-defined scope of responsibility and privilege at each such layer of transfer of request or offer.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;DGT (CMS)&lt;/b&gt; &lt;br /&gt; 				US CMS Development Grid Testbed&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Distinguished Name (DN)&lt;/b&gt;. &lt;br /&gt; 				The unique name of the entity whose public key the certificate identifies. The DN includes the subject&amp;#39;s Common Name (first and last name), Organizational Unit (e.g., institution), Organization (e.g., VO), and 2-character Country code (and optionally locality and state or province).&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Disk Cache&lt;/b&gt; &lt;br /&gt; 				A disk cache can be viewed as an intermediate &amp;quot;relay station&amp;quot; between client applications and an HSM. It decouples the potentially slow network transfer (to and from client machines) from the fast storage media I/O in order to keep the HSM from bogging down. See dCache.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;DPE (CMS)&lt;/b&gt; &lt;br /&gt; 				US CMS Distributed Production Environment that US CMS is putting together to automate CMS simulation applications on VDT/EDG-based Grids&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;DRM Disk Resource Manager &lt;br /&gt; 				&lt;/b&gt;(DRM) is what we typically call an SRM implementation on top of a disk system. A DRM is a component that controls the use of a shared disk cache in a data grid.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Durable file&lt;/b&gt; &lt;br /&gt; 				A durable file has the behavior of a volatile file in that it has a lifetime associated with it, but also the behavior of a permanent file in that when the lifetime expires the file is not automatically eligible for removal.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Durable storage&lt;/b&gt; &lt;br /&gt; 				Storage in which a &amp;quot;durable&amp;quot; physical replica of a file can only be created and removed by the administrator of the disk cache. A durable file is designated to stay in cache by the disk cache administrator (i.e,. not subject to dynamic removal) until he or she decides to change its status to &amp;quot;volatile&amp;quot;.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Dynamic Workspace&lt;/b&gt; &lt;br /&gt; 				A persistent, extensible, managed collection of objects and tools hosted on a grid.

%INCLUDE{&quot;GlossaryJumpIndex&quot;}% 				
#LtrE
			&lt;b&gt;Economy Set &lt;br /&gt; 				&lt;/b&gt;of benefits made and costs accrued as seen by Consumers and Providers.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;EDG&lt;/b&gt; &lt;br /&gt; 				The European Data Grid project, initiated at CERN and EU countries, has developed a set of Grid services built upon VDT. Read more at &lt;a href=&quot;http://eu-datagrid.web.cern.ch/eu-datagrid/&quot;&gt;http://eu- datagrid.web.cern.ch/eu-datagrid/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;EGEE &lt;br /&gt; 				&lt;/b&gt;The &lt;a href=&quot;http://public.eu-egee.org/&quot;&gt;Enabling Grids for E-sciencE (EGEE)&lt;/a&gt; project is funded by the  		European Commission and aims to build on recent advances in grid technology and develop a service grid  		infrastructure which is available to scientists 24 hours-a-day.          &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Enstore&lt;/b&gt; &lt;br /&gt; 				Enstore is the mass storage system implemented at Fermilab as the primary data store for large data sets. See &lt;a href=&quot;http://computing.fnal.gov/docs/products/enstore/index.html&quot;&gt;http://computing.fnal.gov/docs/products/enstore/&lt;/a&gt;  		and &lt;a href=&quot;http://hppc.fnal.gov/enstore/&quot;&gt;http://hppc.fnal.gov/enstore/&lt;/a&gt;.    

%INCLUDE{&quot;GlossaryJumpIndex&quot;}%
#LtrF
			&lt;b&gt;Fabric&lt;/b&gt;             &lt;br /&gt; 				&lt;!--from http://www.sys-con.com/webservices/articleprint.cfm?id=347 --&gt;     With respect to grid computing, &amp;quot;fabric&amp;quot; refers to a &amp;quot;layer&amp;quot; of grid components (underneath applications, tools, and middleware). Fabric encompasses local resource managers (e.g., operating systems, queuing systems, device drivers, libraries, etc.), and networked resources (e.g., compute and storage resources, data sources, etc.). Grid applications use grid tools and middleware components to interact with the fabric.                     &lt;!--(changed, Craig) april --&gt;&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt; 					     Facility&lt;br /&gt; 				&lt;/b&gt;A &amp;quot;facility&amp;quot; is a logical name denoting a collection of one or more sites under the same administrative domain. Facilities are anticipated to provide other grid services - especially, but not exclusively, services in support of the major applications running for whom they provide computational resources &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Federation Peer&lt;/b&gt; &lt;br /&gt; 				level coupling of resources within OSG or of external grids to OSG. A federated union of resources or grids enables jobs to migrate between them. 				
%INCLUDE{&quot;GlossaryJumpIndex&quot;}%
#LtrG
&lt;b&gt;Ganglia&lt;/b&gt; &lt;br /&gt; 				Ganglia is a scalable, distributed monitoring system for high-performance computing systems such as clusters and Grids. Read more at &lt;a href=&quot;http://ganglia.sourceforge.net/&quot;&gt;http://ganglia.sourceforge.net/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Gatekeeper&lt;/b&gt; &lt;br /&gt; 				A gatekeeper is a process used at a site to take incoming job requests and check the security to make sure each is allowed to use the associated computing resource(s). The gatekeeper process starts up the job-manager process after successful authentication.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GFAL&lt;/b&gt; &lt;br /&gt; 				&lt;a href=&quot;http://grid-deployment.web.cern.ch/grid-deployment/gis/GFAL/gfal.3.html&quot;&gt;Grid File Access Library&lt;/a&gt;, is an LCG-proposed POSIX interface for the normal file I/O operations (Open/Seek/Read/Write/Close) which includes a limited set of POSIX functions for file management. The GFAL interface is designed to hide the grid storage interactions (replica catalog, SRM and file access mechanism) from user applications. It consists of the Grid File Access Library, and a File System built on top of this library, which will support internally diverse file access protocols (e.g., file: rfio: dcap: root:).&lt;br /&gt; 				&lt;br /&gt; 				&lt;!--(no good web page for reference) april --&gt;&lt;b&gt;GGF&lt;/b&gt;            &lt;!--(change 4/21) --&gt;&lt;br /&gt; 				The Global Grid Forum ( GGF ) is a community-initiated forum of thousands of individuals from industry and research leading the global standardization effort for grid computing.  GGF&amp;#39;s primary objectives are to promote and support the development, deployment, and implementation of Grid technologies and applications via the creation and documentation of &amp;quot;best practices&amp;quot; - technical specifications, user experiences, and implementation guidelines. (&lt;a href=&quot;http://www.gridforum.org/&quot;&gt;http://www.gridforum.org/&lt;/a&gt; ) &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GIG&lt;/b&gt; &lt;br /&gt; 				Grid Infrastructure Group at the University of Chicago.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GIIS (MDS)&lt;br /&gt; 				&lt;/b&gt;(Grid Index Information Service, a component of MDS) maintains a current index of all the registered resources known in the grid. It provides a means of knitting together arbitrary GRIS services to provide a coherent system image that can be explored or searched by grid applications. GIISes thus provide a mechanism for identifying &amp;quot;interesting&amp;quot; resources, where &amp;quot;interesting&amp;quot; can be defined arbitrarily. For example, a GIIS could list all of the computational resources available within a confederation of laboratories, or all of the distributed data storage systems owned by a particular agency. In Grid3 we are implementing a hierarchical GIIS structure. Read more at &lt;a href=&quot;http://griddev.uchicago.edu/download/grid3/doc.pkg/GIIS-configuration/giis_config.doc&quot;&gt;http://griddev.uchicago.edu/download/grid3/doc.pkg/GIIS-configuration/giis_config.doc&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GIP&lt;/b&gt; &lt;br /&gt; 				Generic Information Providers (interoperable with LCG)&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GIS (MDS)&lt;/b&gt; &lt;br /&gt; 				MDS Grid Information System (for discovery and monitoring) is a service that allows the storage of information about the state of the Grid infrastructure. One of its services is to publish information via LDAP. The GIS functions as a yellow pages directory for retrieving a list of categorized entities. Examples of such categories are lists of computers or people.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Globus Toolkit &lt;br /&gt; 				&lt;/b&gt;The Globus Toolkit is an open source software toolkit used for building grids. It is being developed by the Globus Project and many others all over the world. Read more at &lt;a href=&quot;http://www-unix.globus.org/toolkit/&quot;&gt;http://www-unix.globus.org/toolkit/&lt;/a&gt;. The Virtual Data Toolkit (VDT) includes the Globus Toolkit. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GLUE&lt;/b&gt; &lt;br /&gt; 				The Grid Laboratory Uniform Environment (GLUE) effort is sponsored by the High Energy and Nuclear Physics Intergrid Joint Technical and Coordination Boards. It aims to sponsor and enable interoperability between the EU physics grid project efforts (EDG, DataTag, etc.) and the US physics grid project efforts (iVDGL, PPDG, GriPhyN). GLUE addresses each service required by an end-to-end application or experiment system running over the grid, starting from the lowest level functionality for which interoperability between international projects is needed. Read more at &lt;a href=&quot;http://www.hicb.org/&quot;&gt;http://www.hicb.org/&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GLUE Schema &lt;br /&gt; 				&lt;/b&gt;The GLUE Schema (see GLUE) activity aims to define a common conceptual data model to be used for grid resources monitoring and discovery. It aims to define, publish and enable the use of common schemas for interoperability between the EU and US physics grid project efforts. Read more at &lt;a href=&quot;http://www.hicb.org/&quot;&gt;http://www.hicb.org/ http://www.cnaf.infn.it/~sergio/datatag/glue/index.htm&lt;/a&gt; &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GOC&lt;/b&gt; &lt;br /&gt; 				Grid Operations Center. It functions as a central repository for configuration information regarding the network, storage and computing resources at all the grid sites. It also serves as a central monitoring point for operational activity at each site and among the sites. See &lt;a href=&quot;http://osggoc.blogspot.com/&quot;&gt;http://www.grid.iu.edu/&lt;/a&gt; .&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GRAM&lt;/b&gt; &lt;br /&gt; 				Globus Resource Allocation Manager, GRAM, is a basic library service that provides capabilities to do remote-submission job start up. GRAM unites Grid machines, providing a common user interface so that you can submit a job to multiple machines on the Grid fabric. GRAM is a general, ubiquitous service, with specific application toolkit commands built on top of it. Read more at &lt;a href=&quot;http://www-unix.globus.org/developer/resource-management.html&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.globus.org/toolkit/gram/&quot;&gt;http://www.globus.org/toolkit/gram/&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Grid&lt;/b&gt; &lt;br /&gt; 				A named set of Services, Providers, Resources, and Policies, overlapping and/or including other Grids operating as a coherent infrastructure in support to the contracting Virtual Organizations. Providers may delegate their contracts with the participating VOs to the Grid administration.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Grid3 or Grid2003&lt;/b&gt; &lt;br /&gt; 				Joint project between iVDGL, PPDG and the US LHC software and computing projects during the second half of 2003, to provide a VDT-based, cross-institution and cross-experiment Grid platform for LHC applications. Read more at &lt;a href=&quot;http://www.ivdgl.org/grid3/&quot;&gt;http://www.ivdgl.org/grid3/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GridCat &lt;br /&gt; 				&lt;/b&gt;A high level grid cataloging system using status dots on geographic maps as well as a catalog of participating sites. It is used as an operations tool as well as a monitoring service.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Grid File Name &lt;br /&gt; 				&lt;/b&gt;(GFN) A globally unique file name for a file in the grid. This name is assigned by data collection creator, and registered in the RC. The GFN is equivalent to the LFN.  &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GridFTP&lt;/b&gt; &lt;br /&gt; 				Grid version of the File Transport Protocol for moving large datasets between storage elements within a data grid. The implementation used in Grid3 is from the Globus Toolkit.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GridICE&lt;/b&gt; &lt;br /&gt; 				GridICE is a grid monitoring service. It is the outcome of the research and development experience within the INFN-GRID and the DataTAG projects in the area of grid monitoring. See &lt;a href=&quot;http://server11.infn.it/gridice/&quot;&gt;http://server11.infn.it/gridice/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Gridmap file&lt;/b&gt; &lt;br /&gt; 				&lt;!--(change 4/21) --&gt;     A file stored on a site&amp;#39;s gatekeeper node which defines a static mapping between users&amp;#39; X509 certificates (containing distinguished names) and local user names. It controls all user access to the local resources. Maps can be shared, or individual map files can be created for each gatekeeper. See a sample gridmap file at &lt;a href=&quot;http://proj-clasp.web.cern.ch/proj-CLASP/meetings/HTASC/htasc_globus/tsld007.htm&quot;&gt;http://proj-clasp.web.cern.ch/proj-CLASP/meetings/HTASC/htasc_globus/tsld007.htm&lt;/a&gt;. Also see VOMS, GUMS and Privilege.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GriPhyN&lt;/b&gt; &lt;br /&gt; 				Grid Physics Network, an NSF-funded US grid project providing Grid technologies centered around the idea of Virtual Data. Read more at &lt;a href=&quot;http://www.griphyn.org&quot;&gt;http://www.griphyn.org&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GRIS (MDS)&lt;/b&gt; &lt;br /&gt; 				Grid Resource Information Service is an information provider (IP), also called a sensor or probe, that gathers and generates data from a site and publishes it into the MDS GIIS. There is a one-to-one correspondence between a GRIS and a site. A GRIS provides information such as the number of nodes in each cluster and which applications have been installed on the nodes.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GSI &lt;br /&gt; 				&lt;/b&gt;Grid middleware component &amp;quot;Grid Security Infrastructure&amp;quot;, Public Key- based, provided by the Globus Toolkit. It consists of the gss-api library for adding authentication to a program. GSI provides programs, such as grid-proxy-init, to facilitate login to a variety of sites, while each site has its own flavor of security measures.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GSIFTP&lt;/b&gt; &lt;br /&gt; 				service GSI enabled FTP (gsiftp) is a modification of FTP that allows FTP to use PKI certificates and designated proxy certificates for authentication.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GT3 or GT4&lt;/b&gt; &lt;br /&gt; 				&lt;a href=&quot;http://www.globus.org/toolkit/&quot;&gt;Globlus Toolkit&lt;/a&gt; version 3 or 4 (and so on)                   GUID         Grid Unique Identifier                &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;GUMS&lt;/b&gt; &lt;br /&gt; 				(Grid User Management System) GUMS is a Grid Identity Mapping Service. It maps the credential for each incoming job at a site to an appropriate site credential, and communicates the mapping to the gatekeeper. GUMS is particularly well suited to a heterogeneous environment with multiple gatekeepers; it allows the implemenation of a single site-wide usage policy, thereby providing better control and security for access to the site&amp;#39;s grid resources. Read more at &lt;a href=&quot;http://grid.racf.bnl.gov/GUMS/&quot;&gt;http://grid.racf.bnl.gov/GUMS/&lt;/a&gt;.    				

#LtrH
	&lt;b&gt;Head Node&lt;/b&gt; &lt;br /&gt; 				&lt;!--(change 4/21) --&gt;This is an inexact term that generally refers to a node in an RP cluster through which jobs are submitted. In a simple cluster, the head node is the one on which the gatekeeper and monitoring software are installed. In a more complicated cluster, there may be multiple gatekeepers on different nodes, and other issues, thus &amp;quot;head node&amp;quot; is not well defined. A head node would be networked to a group of worker nodes.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;HE(N)P&lt;/b&gt; &lt;br /&gt; 				High Energy (and Nuclear) Physics Host A physical computing element. This element characterizes the physical configuration of a computing node, including processors, software, storage elements, etc.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;HPSS&lt;/b&gt; &lt;br /&gt; 				High performance Storage System is a hierarchical storage system designed to manage and access petabytes of data. See &lt;a href=&quot;http://www4.clearlake.ibm.com/hpss/index.jsp&quot;&gt;http://www4.clearlake.ibm.com/hpss/index.jsp&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;HRM&lt;/b&gt; &lt;br /&gt; 				Hierarchical Storage Manager (HRM) is what we typically call an SRM implementation on top of a Hierarchical mass storage system. An HRM stages files from tertiary storage (e.g., HPSS) into its disk cache. 				

#LtrI
			&lt;b&gt;IBP&lt;/b&gt; &lt;br /&gt; 				The &lt;a href=&quot;http://loci.cs.utk.edu/ibp/&quot;&gt;Internet Backplane Protocol&lt;/a&gt; (IBP) is middleware for managing and using remote storage. It was invented to support Logistical Networking in large scale, distributed systems and applications. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;iGOC&lt;/b&gt; &lt;br /&gt; 				The iVDGL Grid Operations Center at Indiana University serves as a single central point of contact for iVDGL (and Grid3) information and operations. Read more at &lt;a href=&quot;http://igoc.iu.edu/&quot;&gt;http://igoc.iu.edu/&lt;/a&gt; . Also see GOC. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;IGT (of CMS)&lt;/b&gt; &lt;br /&gt; 				US CMS Trash/Trash/Integration Grid Testbed; used to test and integrate Grid components and the CMS Distributed Processing Environment, before deployment in production.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Information Provider&lt;/b&gt; &lt;br /&gt; 				(IP) Information Provider software interfaces to any data collection service (e.g., Ganglia), collects virtually any type of data it&amp;#39;s asked to, and communicates the information to a GRIS for publishing to the grid.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;ITB&lt;/b&gt; &lt;br /&gt; 				Trash/Trash/Integration Test Bed. The product of integrating new services with existing services and core infrastructure for testing prior to deployment. An ITB release is defined as a set of functionalities that are provided by a site (gatekeeper services running on a computing element), a VO (VO specific services provided by that VO, such as a VOMS server), or another resource provider (such as a group providing a monitoring or discovery service), that may be used in the context of the ITB.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;iVDGL&lt;/b&gt; &lt;br /&gt; 				international Virtual Data Grid Laboratory, an NSF-funded project. The iVDGL is a global Data Grid that will serve forefront experiments in physics and astronomy. Its computing, storage and networking resources in the U.S., Europe, Asia and South America provide a unique laboratory that will test and validate Grid technologies at international and global scales. Read more at &lt;a href=&quot;http://www.ivdgl.org/&quot;&gt;http://www.ivdgl.org/&lt;/a&gt;.     				  &lt;p&gt;&lt;a title=&quot;LTR_J&quot; name=&quot;LTR_J&quot;&gt;&lt;/a&gt;&lt;b&gt;J&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_J&quot; name=&quot;LTR_J&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_J&quot; name=&quot;LTR_J&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_J&quot; name=&quot;LTR_J&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_J&quot; name=&quot;LTR_J&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_J&quot; name=&quot;LTR_J&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q |  &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z  &lt;/p&gt;   				&lt;b&gt;JClarens&lt;/b&gt; &lt;br /&gt; 				&lt;a href=&quot;http://clarens.sourceforge.net/jclarens/&quot;&gt;JClarens&lt;/a&gt; is a Java-based supplement to the Python-based Clarens Web Services Framework developed at Caltech. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Job manager&lt;/b&gt; &lt;br /&gt; 				A Globus term that refers to a program used to manage jobs at a grid site (e.g., LSF, PBS, Condor, LoadLeveler). 				&lt;p&gt;&lt;a title=&quot;LTR_L&quot; name=&quot;LTR_L&quot;&gt;&lt;/a&gt;&lt;b&gt;L&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_L&quot; name=&quot;LTR_L&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_L&quot; name=&quot;LTR_L&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_L&quot; name=&quot;LTR_L&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_L&quot; name=&quot;LTR_L&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_L&quot; name=&quot;LTR_L&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q |  &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z  &lt;/p&gt;   				&lt;b&gt;LCG LHC&lt;/b&gt; &lt;br /&gt; 				Computing Grid project at CERN. The goal of the LCG project is to meet the LHC experiments&amp;#39; unprecedented computing needs by deploying a worldwide computational grid service, integrating the capacity of scientific computing centers spread across Europe, America and Asia into a virtual computing organisation. See &lt;a href=&quot;http://lcg.web.cern.ch/LCG/&quot;&gt;http://lcg.web.cern.ch/LCG/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;LDAP&lt;/b&gt; &lt;br /&gt; 				&lt;a href=&quot;http://www.gracion.com/server/whatldap.html&quot;&gt;Lightweight Directory Access Protocol&lt;/a&gt;, a protocol used to locate resources in a network.   &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;LHC&lt;/b&gt; &lt;br /&gt; 				the Large Hadron Collider at CERN (&lt;a href=&quot;http://lhc-new-homepage.web.cern.ch/lhc-new-homepage/&quot;&gt;http://lhc-new- homepage.web.cern.ch/lhc-new-homepage/&lt;/a&gt;)&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Lifetime (of a pin)&lt;/b&gt; &lt;br /&gt; 				A period of time for which a file is guaranteed to remain available (remain &amp;quot;pinned&amp;quot;) in a given storage area for a client. A pin lifetime is requested by a client and granted/denied by SRM at the time the file is moved in for that client. A separate, independent pin lifetime is set (or denied) for any other client who requests it once the file is already there. Client may release pin before lifetime expires. Action taken on file after last pin lifetime expires (or pin is released) is dependent on the SRM configuration.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Lifetime (of a space)&lt;/b&gt; &lt;br /&gt; 				A period of time for which a (temporary, volatile or durable) storage area is guaranteed to remain available. Particular space may be requested by a client, in which case space lifetime may be negotiated. Or space and space lifetime may be assigned by default by the SRM. Any pin lifetime set for a file in the space may not exceed the remaining space lifetime.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;LIGO &lt;br /&gt; 				&lt;/b&gt;Laser Interferometer Gravitational Wave Observatory experiment (&lt;a href=&quot;http://www.ligo.caltech.edu&quot;&gt;http://www.ligo.caltech.edu&lt;/a&gt;) &lt;br /&gt;  					Locking (a file) A method used to coordinate the content of objects in the database (records, disk blocks, etc.) when they are subject to updates in order to ensure that the entire transaction that may involve multiple objects completes correctly. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Logical File Name&lt;/b&gt; &lt;br /&gt; 				(LFN) A globally unique name for a file on the grid, that is location and machine independent; it may point to any PFN for the given file. LFNs are governed by the replica catalog and use the RLS.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Logistical Networking&lt;/b&gt; &lt;br /&gt; 				Logistical Networking technology builds on a highly generic storage service that uses the Internet Backplane Protocol (IBP), which it deploys on storage servers called depots (see SPD or SRM Parallel Depot). 				&lt;p&gt;&lt;a title=&quot;LTR_M&quot; name=&quot;LTR_M&quot;&gt;&lt;/a&gt;&lt;b&gt;M&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_M&quot; name=&quot;LTR_M&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_M&quot; name=&quot;LTR_M&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_M&quot; name=&quot;LTR_M&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_M&quot; name=&quot;LTR_M&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_M&quot; name=&quot;LTR_M&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q |   &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z   &lt;/p&gt;  				&lt;b&gt;MDS&lt;/b&gt; &lt;br /&gt; 				The Monitoring and Directory Service is the Globus Toolkit implementation of a Grid Information Service (GIS). The MDS has the ability to function as a white pages directory for retrieving information associated with a particular name (distinguished name). Examples for such lookups are the number of CPUs and the operating system associated with a particular machine. Read more at &lt;a href=&quot;http://www.globus.org/toolkit/mds/&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.globus.org/mds/&quot;&gt;http://www.globus.org/mds/&lt;/a&gt; .&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Member (of OSG) &lt;br /&gt; 				&lt;/b&gt;A person affiliated with a member organization of OSG.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Member organization&lt;/b&gt; &lt;br /&gt; 				An institution, facility or VO that contributes to the development and resource pool of OSG.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Middleware&lt;/b&gt; &lt;br /&gt; 				Middleware is software that connects two or more otherwise separate applications across the Internet or local area networks. More specifically, the term refers to an evolving layer of services that resides between the network and more traditional applications for managing security, access and information exchange. OSG recognizes two levels of middleware: Grid middleware (e.g., VDT, Grid3-gridmap, etc.) and VO- specific application middleware.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;MIS-CI&lt;/b&gt; &lt;br /&gt; 				&lt;a href=&quot;http://osg.ivdgl.org/twiki/bin/view/Trash/Trash/Trash/IntegrationCoreMIS&quot;&gt;OSG MIS-CI&lt;/a&gt; is a generic monitoring system. It covers areas that other monitoring systems do not cover. The functionality and content of the MIS-CI is continually evolving and based on the information required for several grid services and monitors. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Trash.ReleaseDocumentationMonALISA&lt;/b&gt; &lt;br /&gt; 				MONitoring Agents using a Large Integrated Services Architecture. It is a grid middleware component that offers a scalable monitoring infrastructure based on intelligent agents implemented through a distributed services architecture. Each service can register itself and then be discovered and used by any other services or clients. All services and clients subscribing to a set of events (state changes) in the system are notified automatically. The framework integrates several existing monitoring tools and procedures to collect. Trash.ReleaseDocumentationMonALISA has been added to the VDT. Read more at &lt;a href=&quot;http://monalisa.cacr.caltech.edu/&quot;&gt;http://monalisa.cacr.caltech.edu/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Monitoring (grid monitoring)&lt;/b&gt; &lt;br /&gt; 				Grid monitoring involves collecting, analyzing and displaying information from the distributed production infrastructure in order to determine server status and application progress, and to log performance data of CPUs, networks and storage devices. OSG implements Trash.ReleaseDocumentationMonALISA as a monitoring infrastructure, GridCat as a high level grid cataloging system, and ACDC to provide near real-time snap shots of critical computational job metrics.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Monitoring and Discovery Service&lt;/b&gt; &lt;br /&gt; 				(MDS) The information services component of the Globus Toolkit, the Monitoring and Discovery Service (MDS), gathers information about Grid resources by means of the Grid Resource Information Service (GRIS) and the Grid Index Information Service (GIIS).&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;MSS&lt;/b&gt; &lt;br /&gt; 				Mass Storage System 				&lt;p&gt;&lt;a title=&quot;LTR_N&quot; name=&quot;LTR_N&quot;&gt;&lt;/a&gt;&lt;b&gt;N&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_N&quot; name=&quot;LTR_N&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_N&quot; name=&quot;LTR_N&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_N&quot; name=&quot;LTR_N&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_N&quot; name=&quot;LTR_N&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_N&quot; name=&quot;LTR_N&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q | &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z &lt;/p&gt;   				&lt;b&gt;NeST&lt;/b&gt; &lt;br /&gt; 				Network STorage (NeST) is a flexible software-only storage appliance designed to meet the storage needs of the Grid. See &lt;a href=&quot;http://www.cs.wisc.edu/condor/nest/&quot;&gt;http://www.cs.wisc.edu/condor/nest/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Network Element&lt;/b&gt; &lt;br /&gt; 				(NE) A network path or a set of network hops. This includes both end-to-end and hop-by-hop path information. 				&lt;p&gt;&lt;a title=&quot;LTR_O&quot; name=&quot;LTR_O&quot;&gt;&lt;/a&gt;&lt;b&gt;O&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_O&quot; name=&quot;LTR_O&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_O&quot; name=&quot;LTR_O&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_O&quot; name=&quot;LTR_O&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_O&quot; name=&quot;LTR_O&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_O&quot; name=&quot;LTR_O&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q | &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z &lt;/p&gt;   				&lt;b&gt;OGSA&lt;/b&gt; &lt;br /&gt; 				Open Grid Services Architecture, the next generation Grid architecture based on Web Services. Globus Toolkit 3 is being implemented using this standard. Read more at &lt;a href=&quot;http://www.globus.org/ogsa/&quot;&gt;http://www.globus.org/ogsa/&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;OGSI&lt;/b&gt; &lt;br /&gt; 				Open Grid Services Infrastructure; a grid infrastructure based on OGSA. needs URL.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Ownership&lt;/b&gt; &lt;br /&gt; 				A state of having absolute or well-defined partial rights and responsibilities for a Resource depending on the type of control. OSG considers two such types: actual Ownership and Ownership by virtue of a Contract/Lease. A Lessee is a limited Owner of the Resource for the duration of the Contract/Lease. &lt;!--Not sure what url to give - no obvious one that I find. april --&gt;				  &lt;p&gt;&lt;a title=&quot;LTR_P&quot; name=&quot;LTR_P&quot;&gt;&lt;/a&gt;&lt;b&gt;P&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_P&quot; name=&quot;LTR_P&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_P&quot; name=&quot;LTR_P&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_P&quot; name=&quot;LTR_P&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_P&quot; name=&quot;LTR_P&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_P&quot; name=&quot;LTR_P&quot;&gt;&lt;/a&gt;  &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q |   &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z   &lt;/p&gt;  				&lt;b&gt;Pacman Grid&lt;/b&gt; &lt;br /&gt; 				middleware component for packaging and distribution of software, developed at Boston U., supported by the iVDGL, used by the VDT, Atlas and CMS, and Grid3. Read more at &lt;a href=&quot;http://physics.bu.edu/%7eyoussef/pacman/&quot;&gt;http://physics.bu.edu/~youssef/pacman/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Pacman cache&lt;/b&gt; &lt;br /&gt; 				A pacman cache can be thought of as a collection of virtual applications. It is simply a URL or local file system location containing files with .pacman suffixes, e.g., xyz.pacman. Some caches are &amp;quot;registered&amp;quot; and have nice names like &amp;quot;VDT&amp;quot;, and some are just URLs or file system locations. Each xyz.pacman file corresponds to a package xyz.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Pacman file &lt;br /&gt; 				&lt;/b&gt;A .pacman file (e.g., xyz.pacman) contains instructions on how the software in the xyz package is fetched, installed, setup, uninstalled, what other packages it depends on, and so on.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Pacman package&lt;/b&gt; &lt;br /&gt; 				A software environment created by installation of a .pacman file.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Partner&lt;/b&gt; &lt;br /&gt; 				An individual or organization affiliated with a grid external to the Consortium with which the Open Science Grid interacts with through federation of resources.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Permanent storage&lt;/b&gt; &lt;br /&gt; 				A data storage system, or a data collection in a storage system, wherein a physical file can only be created and removed by the owner of the data collection.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Persistent storage&lt;/b&gt; &lt;br /&gt; 				See permanent storage.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Physical file name&lt;/b&gt; &lt;br /&gt; 				(PFN) The URL of a physical replica of a file, minus the protocol.&lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;Pinning (a file)&lt;/b&gt; &lt;br /&gt; 				Pinning refers to the capability of an SRM to keep a particular file in non-permanent storage space for a period of time set by the client, prior to making the file eligible for transfer or removal from the SRM. A pin is requested and released by a client. Pinning a file is a way of keeping the file in place, not locking its content (see locking).&lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;Pippy&lt;/b&gt; &lt;br /&gt; 				Pippy is an information provider compatible with the Monitoring and Discovery Service (MDS) of the Globus Toolkit and the Pacman software package installer. Pippy can read pacman databases and generate appropriate LDAP entries for installed software packages that are compatible with MDS. Once installed, pippy becomes part of the host&amp;#39;s Grid Resource Information Service (GRIS). Read more at &lt;a href=&quot;http://www-hep.uta.edu/~mcguigan/pippy/&quot;&gt;http://heppc12.uta.edu/~mcguigan/pippy/&lt;/a&gt;  . &lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;Policy&lt;/b&gt; &lt;br /&gt; 				A statement of well-defined requirements, conditions or preferences put forth by a Provider and/or Consumer that is utilized to formulate decisions leading to actions and/or operations within the infrastructure.&lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;POOL&lt;/b&gt; &lt;br /&gt; 				POOL is an LCG software system and project, developed with US participation. It has been created to implement a common persistency framework for the LCG application area and to replace the Objectivity database system in the US LHC software. POOL is tasked to store experiment data and metadata in the multi Petabyte area in a distributed and grid enabled way. Read more at &lt;a href=&quot;http://pool.cern.ch/&quot;&gt;http://pool.cern.ch/&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;PPDG&lt;/b&gt; &lt;br /&gt; 				Particle Physics Data Grid project, funded by DOE. Read more at &lt;a href=&quot;http://www.ppdg.net/&quot;&gt;http://www.ppdg.net/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Privilege&lt;/b&gt; &lt;br /&gt; 				The &lt;a href=&quot;http://computing.fnal.gov/docs/products/voprivilege/index.html&quot;&gt;VO Privilege Project&lt;/a&gt; implements finer-grained authorization for access to grid-enabled resources and services in order to improve user account assignment and management at grid sites, and reduce the associated administrative overhead. Depending on its implementation, privilege relies on, interfaces to and further develops at least some of the following independent pieces of VO-implemented and site-implemented authorization software: VOMS, VOMRS, Grid-map callout interface, GUMS, and SAZ.   				 

*PRIMA* %BR% 
PRIvilege Management and Authorization, a component of the privilege project for user authorization at a site, is used with GUMS and VOMS to implement dynamic, fine-grained, role-based identity mapping. PRIMA extracts the VOMS attributes containing the VO and role information from the user&#39;s proxy certificate, and queries GUMS for an appropriate local user account assignment.

&lt;p&gt;&lt;a title=&quot;LTR_R&quot; name=&quot;LTR_R&quot;&gt;&lt;/a&gt;&lt;b&gt;R&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_R&quot; name=&quot;LTR_R&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_R&quot; name=&quot;LTR_R&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_R&quot; name=&quot;LTR_R&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_R&quot; name=&quot;LTR_R&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_R&quot; name=&quot;LTR_R&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q | &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z &lt;/p&gt;   				
&lt;b&gt;RB&lt;/b&gt; &lt;br /&gt; 				Resource Broker. Grid middleware component that brokers the running of Grid jobs (making use of information services to obtain grid status information about available resources) and schedules jobs.&lt;br /&gt; 				&lt;br /&gt; 				
*RBAC* %BR%
[[VOAdminInOsg#Role_Based_Access_Control_RBAC_I][Role-Based Access Control]]; an infrastructure which provides a framework for role-based access to resources and services. 

&lt;b&gt;Release&lt;/b&gt; &lt;br /&gt; 				An OSG Release refers to a set of functionality and agreed interfaces rather than the collection of software which provides exemplars of this functionality and interfaces.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Replica catalog&lt;/b&gt; &lt;br /&gt; 				Provides mappings between logical names for files and one or more copies of the files on physical storage systems.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Resource Owner &lt;br /&gt; 				&lt;/b&gt;Has permanent specific control, rights and responsibilities for a Resource associated with ownership.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;RLI&lt;/b&gt; &lt;br /&gt; 				Grid middleware component, Replica Location Interface, used to distribute RLS information.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;RLS&lt;/b&gt; &lt;br /&gt; 				Grid middleware component, Replica Location Service, provides information about location of data sets within the data grid. EDG and Globus (with EDG) have both developed implementations of RLS. Grid3 uses the Globus-EDG implementation&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;RP&lt;/b&gt; &lt;br /&gt; 				Resource Provider: a facility offering resources (e.g., CPU, network, storage) to other parties (e.g., VOs) according to a specific Memorandum of Understanding (MOU). 				&lt;p&gt;&lt;a title=&quot;LTR_S&quot; name=&quot;LTR_S&quot;&gt;&lt;/a&gt;&lt;b&gt;S&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_S&quot; name=&quot;LTR_S&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_S&quot; name=&quot;LTR_S&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_S&quot; name=&quot;LTR_S&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_S&quot; name=&quot;LTR_S&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_S&quot; name=&quot;LTR_S&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q |  &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z  &lt;/p&gt;   				&lt;b&gt;SAZ&lt;/b&gt; &lt;br /&gt; 				&lt;a href=&quot;http://www.fnal.gov/docs/products/saz/SAZ.htm&quot;&gt;Site AuthoriZation&lt;/a&gt;; part of VOX. SAZ allows individual site authorities to impose their own flavor of authentication and authorization on grid users, and thus to ultimately control access to their site. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Schema&lt;/b&gt; &lt;br /&gt; 				a schema refers to a description of objects and attributes needs to describe Grid resources, and the relationships between the objects.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;SDSS&lt;/b&gt; &lt;br /&gt; 				Sloane Digital Sky Survey (&lt;a href=&quot;http://www.sdss.org/&quot;&gt;http://www.sdss.org/&lt;/a&gt;) &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Security Control&lt;/b&gt; &lt;br /&gt; 				of and reaction to intentional unacceptable use of any part of the infrastructure.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Service&lt;/b&gt; &lt;br /&gt; 				A method for accessing a Resource or Agent.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;SIHag&lt;/b&gt; &lt;br /&gt; 				Security Incident Handling activity group&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Site&lt;/b&gt; &lt;br /&gt; 				&lt;!--(old definition) Set of grid computing resources (compute and/or storage nodes) owned and managed by the same institution and by a single VO, all reporting to a single, dedicated GRIS. A site has at least one gatekeeper, a single installation of MONALisa and of Ganglia, and has a consistent path for $APP and $DATA. A given location may house multiple grid sites. --&gt;&lt;!--(new, Craig) april --&gt;     A site is a logical name denoting a concrete, persistent, uniquely identifiable, and testable collection of Services, Providers and Resources for administrative purposes. A Facility is a collection of Sites under a single administrative domain. A site offers computing services, persistent storage services, or both.A site offering computing services is uniquely identified by a single gatekeeper service (hostname and port) and a single gsiftp service (hostname and port). Multiple sites at a facility may share certain services at that facility.&lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;Site Administrator &lt;br /&gt; 				&lt;/b&gt;The person local to a site who is responsible for implementing and maintaining the site.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Site File Name&lt;/b&gt; &lt;br /&gt; 				SFN A site specific file name for a replica. It is the choice of the site administrator what these file names reflect; they can be a disk address, a tape address, or a name independent of both. The site name for a replica is registered in the replica catalog as part of a site URL that includes the protocol and site location. In DataGrid terminology, the SFN is equivalent to the file path of the TFN.&lt;br /&gt; 				&lt;br /&gt; 				
&lt;b&gt;SPD SRM Parallel Depot&lt;/b&gt;; &lt;br /&gt; 				an integration of the Storage Resource Manager (SRM) with a new technology for wide area data management, Logistical Networking (LN), which is now being used by several important OSG application communities (see Logistical Networking). The strategy of combining SRM with IBP-based depots is designed to provide an interoperable foundation for wide area storage infrastructure that can address the problems of in-transit data management through a familiar interface, but with maximum flexibility and performance.&lt;br /&gt; 				&lt;br /&gt; 				

*Squid* %BR%
Squid is a web caching service that speeds up downloads from http servers by locally caching files and serving the cached files rather than retrieving the files over the internet.  Also referred to as a proxy, the typical use case would be for squid to be installed on a single server on a cluster with CE nodes using that server to proxy http and https requests.

&lt;b&gt;SRB&lt;/b&gt; &lt;br /&gt; 				Storage Resource Broker. Grid middleware component used for data management, and developed by the San Diego Supercomputing Center with support from PPDG. Read more at &lt;a href=&quot;http://www.sdsc.edu/DICE/SRB/&quot;&gt;http://www.sdsc.edu/DICE/SRB/&lt;/a&gt; . &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;SRM&lt;/b&gt; &lt;br /&gt; 				Grid middleware component Storage Resource Manager, used for data management and virtualization of storage interfaces, and developed by LBNL with support from Jefferson Lab, BNL and Fermilab. PPDG supported. SRM provides shared storage resource allocation and scheduling. It manages space, manages files on behalf of users, manages file sharing, manages multi-file requests, and provides grid access to and from an MSS. An SRM doesn&amp;#39;t perform file transfers, rather it invokes file transfer services as needed, monitors transfers and recovers from failures. There are several types of SRMS: DRM, TRM, and HRM. Read more at &lt;a href=&quot;http://sdm.lbl.gov/srm-wg/&quot;&gt;http://sdm.lbl.gov/srm-wg/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;SRM-dCache&lt;/b&gt; &lt;br /&gt; 				An implementation of SRM as a &amp;quot;door&amp;quot; into dCache.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;SS&lt;/b&gt; &lt;br /&gt; 				Storage Service, term used in Grids to denote any storage interface. (In the GLUE schema, this corresponds to SE.)&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;STAR&lt;/b&gt; &lt;br /&gt; 				A Solenoid Tracker At the RHIC (STAR) experiment at the Brookhaven National Laboratory &lt;a href=&quot;http://www.star.bnl.gov/&quot;&gt;http://www.star.bnl.gov/ &lt;br /&gt; 					&lt;br /&gt; 				&lt;/a&gt;&lt;b&gt;Storage Element&lt;/b&gt; &lt;br /&gt; 				(SE) Any data storage resource that is registered in a Grid Information Service (GIS), contains files registered in a Replica Location Service (RLS), and provides access to remote sites via a Grid interface (e.g., GSI authenticated). (LCG-1 definition:) A Storage Element (SE) provides uniform access and services to large storage spaces. The storage element may control large disk arrays, mass storage systems and the like. &lt;!--TBD: Does storage element include not only the storage head node but also the tape and disk storage systems?  --&gt;&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Storage Resource&lt;/b&gt; &lt;br /&gt; 				Any storage system able to provide storage to different clients.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Storage Space&lt;/b&gt; &lt;br /&gt; 				types The concept of permanent, durable, or volatile space supports space reservations in SRM. In most cases there is a one-to-one mapping between file types and the space types they are assigned to, however not always. E.g., it can be useful to support volatile files in permanent space. The lifetime of the space into which a file is put must be at least as long as the file&amp;#39;s pin lifetime.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Subcluster&lt;/b&gt; &lt;br /&gt; 				In the GLUE schema, a subcluster represents a &amp;quot;homogeneous&amp;quot; collection of nodes, where the homogeneity is defined by a collection whose required node attributes all have the same value. For example, a subcluster represents a set of nodes with the same CPU, memory, OS, network interfaces, etc. Subclusters provide a convenient way of representing useful collections of nodes.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;SURL&lt;/b&gt; &lt;br /&gt; 				A Site URL (SURL) is a URL (universal resource locator) that implements the internet protocol for SRM and contains a reference to a data grid node (site) plus site file name, e.g., srm://host.domain/path/file. 				&lt;p&gt;&lt;a title=&quot;LTR_T&quot; name=&quot;LTR_T&quot;&gt;&lt;/a&gt;&lt;b&gt;T&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_T&quot; name=&quot;LTR_T&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_T&quot; name=&quot;LTR_T&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_T&quot; name=&quot;LTR_T&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_T&quot; name=&quot;LTR_T&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_T&quot; name=&quot;LTR_T&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q | &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z &lt;/p&gt;   				&lt;b&gt;          Temporary file         &lt;br /&gt; 				&lt;/b&gt;A file in temporary storage.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Temporary storage (space)&lt;/b&gt; &lt;br /&gt; 				A shared space that is allocated to a user, but can be reclaimed by the file system (after some guaranteed amount of time has passed). If space is reclaimed, all the files in that space are removed by the file system. The implication is that files in these spaces are also temporary.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;TeraGrid&lt;/b&gt; &lt;br /&gt; 				TeraGrid is an open scientific discovery infrastructure, launched by the &lt;a href=&quot;http://www.nsf.gov/&quot;&gt;National Science Foundation &lt;/a&gt;in August 2001, combining leadership class resources at eight partner sites to create an integrated, persistent computational resource. TeraGrid is coordinated through the Grid Infrastructure Group (GIG) at the University of Chicago. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;TeraGrid DAC &lt;br /&gt; 				&lt;/b&gt;&amp;quot;Development Allocations&amp;quot; (DAC accounts) on the TeraGrid, appropriate for PIs to develop their applications, to experiment with TeraGrid platforms or to use TeraGrid systems as part of classroom instruction.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Tier 0&lt;/b&gt; &lt;br /&gt; 				Initial tier in the grid hierarchy; it is the site at which raw data is taken. The experimental online system interfaces to the tier 0 resources. For the LHC experiments, CERN is the tier 0 facility. Fermilab is the tier 0 facility for the Run II experiments at the Tevatron.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Tier 1 &lt;br /&gt; 				&lt;/b&gt;Next tier, after tier 0, in grid hierarchy. Tier 1 sites are connected to a Tier 0 site based on an MOU with the Tier 0 site. Typically a tier 1 site offers storage, analysis, and services, and represents a broad constituency (e.g., there may be a single tier 1 site per country or region which connects with multiple tier 2 sites in that country or region). In the US, tier 1 centers for ATLAS and CMS are BNL and Fermilab, respectively&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Tier 2 &lt;br /&gt; 				&lt;/b&gt;Tier 2 is the next level down in the grid hierarchy of sites, after tier 1. Tier 2 sites are typically regional computing facilities at University institutions providing a distributed Grid of facilities.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Tools&lt;/b&gt; &lt;br /&gt; 				With respect to grid computing, &amp;quot;tools&amp;quot; refers to a &amp;quot;layer&amp;quot; of grid components (underneath applications, and above middleware and fabric). The tools layer encompasses resource brokers, monitoring tools debuggers, etc.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;transfer file name&lt;/b&gt; &lt;br /&gt; 				(TFN) The ip address of a file, containing the transfer protocol name, the host and the LFN.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Trillium&lt;/b&gt; &lt;br /&gt; 				The three US HEP-related Grid projects, iVDGL, GriPhyN and PPDG are coordinating their activities and are collectively called Trillium.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;TRM&lt;/b&gt; &lt;br /&gt; 				A Tape Resource Manager (TRM) is a middleware layer that interfaces to systems that manage robotic tapes in a data grid. TRM is one type of a Storage Resource manager (SRM).&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;TURL&lt;/b&gt; &lt;br /&gt; 				A Transfer URL (TURL) is a URL used in the file transfer negotiation. A TURL includes the transfer protocol name and the name of the file server machine along with the path to the file. 				&lt;p&gt;&lt;a title=&quot;LTR_U&quot; name=&quot;LTR_U&quot;&gt;&lt;/a&gt;&lt;b&gt;U&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_U&quot; name=&quot;LTR_U&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_U&quot; name=&quot;LTR_U&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_U&quot; name=&quot;LTR_U&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_U&quot; name=&quot;LTR_U&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_U&quot; name=&quot;LTR_U&quot;&gt;&lt;/a&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; | &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q | &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |   &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z &lt;/p&gt;   				&lt;b&gt;US ATLAS &lt;br /&gt; 				&lt;/b&gt;The group of US institutions in the ATLAS collaboration.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;US CMS &lt;br /&gt; 				&lt;/b&gt;The group of US institutions in the CMS collaboration.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;User&lt;/b&gt; &lt;br /&gt; 				A person who makes a request of the Open Science Grid infrastructure. 				&lt;p&gt;&lt;a title=&quot;LTR_V&quot; name=&quot;LTR_V&quot;&gt;&lt;/a&gt;&lt;b&gt;V&lt;/b&gt;&lt;/p&gt;&lt;a title=&quot;LTR_V&quot; name=&quot;LTR_V&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_V&quot; name=&quot;LTR_V&quot;&gt;&lt;/a&gt; &lt;a title=&quot;LTR_V&quot; name=&quot;LTR_V&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;LTR_V&quot; name=&quot;LTR_V&quot;&gt;&lt;/a&gt; 				  &lt;p&gt;&lt;a title=&quot;LTR_V&quot; name=&quot;LTR_V&quot;&gt;&lt;/a&gt;  &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q |  &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z  &lt;/p&gt;  				&lt;b&gt;VDT&lt;/b&gt; &lt;br /&gt; 				Virtual Data Toolkit, developed by the GriPhyN project, provides deployment and integration of US Grid middleware to the Trillium and US LHC projects; most notable components are the Globus Toolkit and Condor. (http://www.lsc- group.phys.uwm.edu/vdt/)&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Virtual Data &lt;br /&gt; 				&lt;/b&gt;The concept that data can be stored as the information about how it can be reproduced. &lt;!--(changed 4/20) --&gt;&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Virtual Organization&lt;/b&gt; &lt;br /&gt; 				(VO) A dynamic collection of Users, Resources and Services for sharing of Resources (Globus definition). A VO is party to contracts between Resource Providers &amp;amp; VOs which govern resource usage &amp;amp; policies. A subVO is a sub-set of the Users and Services within a VO which operates under the contracts of the parent. (previous definition) A participating organization in a grid to which grid end users must be registered and authenticated in order to gain access to the grid&amp;#39;s resources. A VO must establish resource-usage agreements with grid resource providers. Members of a VO may come from many different home institutions, may have in common only a general interest or goal (e.g., CMS physics analysis), and may communicate and coordinate their work solely through information technology (hence the term &lt;i&gt;virtual&lt;/i&gt;). An organization like an HEP experiment can be regarded as one VO.  &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Virtual Site&lt;/b&gt; &lt;br /&gt; 				A set of sites that agree to use the same policies in order to act as an administrative unit. Sites and Facilities negotiate a common administrative context to form a &amp;quot;virtual&amp;quot; site or facility.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Volatile file&lt;/b&gt; &lt;br /&gt; 				A file that is temporary in nature, but has a (pin) lifetime guarantee.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;Volatile storage&lt;/b&gt; &lt;br /&gt; 				Storage in which the &amp;quot;volatile&amp;quot; physical replica of a file is subject to removal from the SRM or DRM according to preset policies.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;VO manager &lt;br /&gt; 				&lt;/b&gt;The person designated by a VO to validate/approve new VO members. The VO manager verifies a user&amp;#39;s VO membership for participating grid sites on demand.&lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;VOMRS&lt;/b&gt; &lt;br /&gt; 				The VOM Registration Service (VOMRS) is a server that provides the means for registering members of a Virtual Organization, and coordination of this process among the various VO and grid resource administrators. VOMRS consists of a database to maintain user registration and institutional information, and a web user interface (web UI) for input of data into the database and manipulation of that data. See &lt;a href=&quot;http://computing.fnal.gov/docs/products/vomrs/&quot;&gt;http://computing.fnal.gov/docs/products/vomrs/&lt;/a&gt;. &lt;br /&gt; 				&lt;br /&gt; 				&lt;b&gt;VOMS&lt;/b&gt; &lt;br /&gt; 				&lt;a href=&quot;http://hep-project-grid-scg.web.cern.ch/hep-project-grid-scg/voms.html&quot;&gt;Virtual Organization Membership Service&lt;/a&gt;: a Grid middleware component provided by the EDG. VOMS     is a system that manages real-time user authorization information for a VO. VOMS is designed to maintain only general information regarding the relationship of the user with his VO, e.g., groups he belongs to, certificate-related information, and capabilities he should present to resource providers for special processing needs. It maintains no personal identifying information besides the certificate.   	     			  				 &lt;p&gt; &amp;nbsp;&lt;a href=&quot;#LTR_A&quot;&gt;A&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_B&quot;&gt;B&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_C&quot;&gt;C&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_D&quot;&gt;D&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_E&quot;&gt;E&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_F&quot;&gt;F&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_G&quot;&gt;G&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_H&quot;&gt;H&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_I&quot;&gt;I&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_J&quot;&gt; J&lt;/a&gt; |&amp;nbsp; K |&amp;nbsp;&lt;a href=&quot;#LTR_L&quot;&gt;L&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_M&quot;&gt;M&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_N&quot;&gt;N&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_O&quot;&gt;O&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_P&quot;&gt;P&lt;/a&gt; |&amp;nbsp; Q |  &amp;nbsp;&lt;a href=&quot;#LTR_R&quot;&gt;R&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_S&quot;&gt;S&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_T&quot;&gt;T&lt;/a&gt; |&amp;nbsp;&lt;a href=&quot;#LTR_U&quot;&gt;U&lt;/a&gt; |  &amp;nbsp;&lt;a href=&quot;#LTR_V&quot;&gt;V&lt;/a&gt; |&amp;nbsp; W |&amp;nbsp; X |&amp;nbsp; Y |&amp;nbsp; Z  &lt;/p&gt;  				 &lt;p&gt;&lt;!--(new, april) --&gt;&lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt;&lt;span class=&quot;bodyorange&quot;&gt;&lt;/span&gt;&lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt;Further resources for grid-related terminology: &lt;/p&gt;&lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt; &lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt; &lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt; 				&lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt; 				  &lt;ul&gt;&lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt; 					&lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt; 					 &lt;li&gt;&lt;a title=&quot;further_resources&quot; name=&quot;further_resources&quot;&gt;&lt;/a&gt;Grid Acronym Soup (GAS) at &lt;a href=&quot;http://www.gridpp.ac.uk/docs/GAS.html&quot;&gt;http://www.gridpp.ac.uk/docs/GAS.html&lt;/a&gt;.&lt;/li&gt;  					    					     					  &lt;li&gt;VOMRS Glossary at &lt;a href=&quot;http://computing.fnal.gov/docs/products/vomrs/glossary.html&quot;&gt;http://computing.fnal.gov/docs/products/vomrs/glossary.html&lt;/a&gt;.&lt;/li&gt;   					  &lt;li&gt;Grid2003 Glossary at &lt;a href=&quot;http://www.ivdgl.org/grid2003/documents/document_server/uploaded_documents/doc--786--glossary.doc&quot;&gt;http://www.ivdgl.org/grid2003/documents/document_server/uploaded_documents&lt;br /&gt;  								/doc--786--glossary.doc&lt;/a&gt;&lt;/li&gt;   					  &lt;li&gt;LCG Engineering Series, LCG-1 Glossary (in progress) &lt;a href=&quot;http://lcg.web.cern.ch/LCG/peb/GTA/GTA-ES/Glossary-v0.2.doc&quot;&gt;http://lcg.web.cern.ch/LCG/peb/GTA/GTA-ES/Glossary-v0.2.doc&lt;/a&gt;.&lt;/li&gt;   				&lt;/ul&gt;   				  &lt;p class=&quot;body&quot;&gt;&amp;nbsp;&lt;/p&gt;   			
-- Main.AnneHeavey - 25 Oct 2007


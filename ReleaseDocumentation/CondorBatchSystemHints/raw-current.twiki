---+!! *&lt;noop&gt;%SPACEOUT{ &quot;%TOPIC%&quot; }%*
%DOC_STATUS_TABLE%
%TOC{depth=&quot;2&quot;}%

---+ About This Document
%ICON{hand}% This Twiki page is for site admins to share the various tricks they have learned in setting up Condor batch systems to work with the Open Science Grid. 

---+ Hardware Configuration for a Condor Pool.
   *  It is strongly recommended that the machine that runs daemons condor_collector and condor_negotiator (your condor master) be different  than the machine which is your Open Science Grid gatekeeper and runs your condor_schedd.
   *  It is also strongly recommended that the machine which runs condor_collector and condor_negotiator not be a busy NFS file server.
   *  The Fermi General Purpose Grid cluster (which has ~1900 batch slots) has three physical machines (dell poweredge 1950, 2x quad core Xeon, 2.66GHz, each hosting four virtual machines.  5 are used as condor servers:
      1. main condor_collector, condor_negotiator, and postgres database  (6GB RAM, 4 cores)
      1. backup condor_collector and condor_negotiator (2 GB RAM, 1 core)
      1. Globus gatekeeper, condor_schedd plus condor_quill &amp;mdash; handles all inbound grid jobs from OSG (6 GB RAM, 4 cores)
      1.  Globus gatekeeper, condor_schedd plus condor_quill &amp;mdash; handles all inbound grid jobs from !FermiGrid gateway (6GB RAM, 4 cores)
      1.  Globus gatekeeper, condor_schedd plus condor_quill &amp;mdash; handles opportunistic jobs from !FermiGrid gateway (6GB RAM, 4 cores).

   * 3 auxiliary machines, 1 master and 2 slave NIS servers:
   * 1 !BeSTMan Gateway node.
   *  Any production globus-gatekeeper/condor_schedd should have at least 4GB of RAM.  More is better.
   *  Condor collector/negotiator is not so CPU-intensive, but needs a good, fast network connection.   If you don&#39;t have a good network connection, or if you have a very sick machine, condor_status will show that there are nodes dropping in and out of the condor pool from time to time.  If so, then definitely split the collector-negotiator to a separate machine and consider setting UPDATE_COLLECTOR_USING_TCP = true.


---+ Software Configuration

   *  Although the OSG Compute Element install will give you a condor batch system as part of the install, it is better to install it separately. This way you will be able to upgrade the condor independently of the rest of the VDT.  Condor typically has new versions a lot faster than the VDT does.
   *  We have found it useful to use the RPM version of Condor for this reason.  A bit more post-configuration is needed with the RPM though, you have to put in your startup scripts manually.
   *  Earlier versions of condor shipped with open HOSTALLOW_WRITE = *, which meant anyone in the world can join your condor pool.  This is a potential security hole and you need to close it as soon as possible.  There is no reason for OSG functionality to do this.  From condor 6.8.1 this is fixed and you are forced to enter some IP address in HOSTALLOW_WRITE before condor will start.
    *  The OSG Operations group sent out details on how to use stronger authentication than merely IP addresses to configure your condor pool.  This means using either a shared password secret or GSI authentication. Details are at [[http://www.cs.wisc.edu/condor/osg_security_recommendations.html]].  Condor as installed by the VDT incorporates the shared pool password mode of authentication as suggested in the document.
   *  It is possible to nfs-mount the condor software so it is seen by all worker nodes.   The default configuration that the VDT will do is to make =$VDT_LOCATION/condor/local.nodename= directories for each node, under the main condor installation.  However, if you are going to do this, it is recommended to change your CONDOR_LOCAL_DIR to be somewhere local on the worker node so you don&#39;t have to write your log  files across NFS, you would only be reading the config files across NFS.
   *  The alternative is to install condor, probably via RPM, on every worker node.  It is a lot more work to do it this way because the configuration files are not shared, but you gain the easy management of RPM, and a condor configuration that will still start up OK even if your nodes come up before the network does.  

---+ Personal Condor
Personal Condor is installed by the VDT in three possible situations:
   1 If you install the OSG Client package
   1 If you install the OSG Managed Fork package on top of the OSG CE and do not have an external Condor Installation defined by VDTSETUP_CONDOR_LOCATION
   1 If you install the OSG Globus-Condor-Setup package on top of the OSG CE and do not have an external Condor Installation defined by VDTSETUP_CONDOR_LOCATION.  (This is BAD--you&#39;ll wish you hadn&#39;t done this.)

In all three of these situations, by default the VDT condor configuration will configure your system 
to be a Personal Condor pool of one node, with daemons MASTER, STARTD, SCHEDD, COLLECTOR, and NEGOTIATOR.  Much more information is available in the [[http://www.cs.wisc.edu/condor/manuals/][Condor Manual]] but in general, the MASTER is the daemon that starts all the others, the STARTD lets you run jobs on your local node, the COLLECTOR and NEGOTIATOR are head-node like daemons that manage the condor pool, and the SCHEDD is the daemon that queues up jobs for submission, to your local node or elsewhere.

In situation 1 and 2, if you are not running a local condor batch system there is no need to be running 
a COLLECTOR, NEGOTIATOR, or STARTD.  You can change this by editing the file,
which will be (by default)  in $VDT_LOCATION/condor/local.&lt;nodename&gt;/condor_config.local.
Make the line DAEMON_LIST read
&lt;pre class=&quot;file&quot;&gt;
DAEMON_LIST = MASTER, SCHEDD
&lt;/pre&gt;
Then stop and restart condor.

You only need a MASTER and SCHEDD if you are submitting jobs to remote grid sites.  Also 
only need a MASTER and SCHEDD if you are just using condor to manage the Trash.ReleaseDocumentationManagedFork 
job manager and nothing else.  You also only need a MASTER and SCHEDD to use the 
OSG Resource Selection service.  If you try to do condor_status command without a collector running
you will get an error but everything will still work.

There is one other important setting for beginners in the condor configuration file, and this is
HOSTALLOW_READ and HOSTALLOW_WRITE.   Current versions of condor leave HOSTALLOW_WRITE 
unset and force you to edit the configuration file to set it to something.  Older versions of
condor set it to &quot;*&quot; which would allow any machine in the world to read and write your condor daemons.
The correct setting for this variable depends on your usage.


   1 Submit node only, HOSTALLOW_READ and HOSTALLOW_WRITE can be set to be localhost only.
   1 Trash.ReleaseDocumentationManagedFork only, HOSTALLOW_READ and HOSTALLOW_WRITE can be set to be localhost only
   1 Condor batch system--at minimum  HOSTALLOW_READ and HOSTALLOW_WRITE must include your head node and all the worker nodes in your condor pool.

For the OSG Resource Selection Service it is necessary to have a wide-open HOSTALLOW_READ
and HOSTALLOW_WRITE.  It is expected that eventually this will change to use GSI authentication.

---+ Condor-G

A commonly asked question is--how do I just install the Condor-G client--the answer is that you
can&#39;t. Condor-G is built into Condor and the only way to run it is to install all the condor binaries
either from the VDT or from the RPM or from the tarball.  But with the suggestions above 
you will only be running the daemons that are needed to run the client.


---+ *Comments*
%COMMENT{type=&quot;tableappend&quot;}%

&lt;!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = StevenTimm

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Knowledge

  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = KarthikArun
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%

   * Local TEST_PASSED = %IN_PROGRESS%


############################################################################################################
--&gt;


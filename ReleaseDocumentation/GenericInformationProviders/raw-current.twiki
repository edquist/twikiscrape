---+!! *&lt;noop&gt;%SPACEOUT{ &quot;%TOPIC%&quot; }%*
%DOC_STATUS_TABLE%
%TOC{depth=&quot;2&quot;}%

---++ Adding site specific attributes to GIP output that are not included in Glue schema

The GIP uses the Glue Schema to publish information about the site; however, the Glue Schema constraints GIP to only publish a fixed set of attributes.

The problem of adding custom information to our sites keeps arising under different circumstances. For example
   1 FNAL uses a grid to fabric requirement forwarding: how to set batch system constraints to submit jobs to a subcluster specified via grid requirements; 
   1 MPI parameters: how to give applications information about MPI library location, version, etc. 

Extending the Glue schema is an option, but the process is time consuming and requires coordination among various Grids. Without this coordination, OSG cannot extend this schema for its own purposes because it breaks interoperability with LCG.

Our solution to this problem is giving a mechanism for administrators to extend the schema published by GIP and filter out these extra attributes before they are published to BDII. This way, the schema available in BDII is still compliant with the Glue Schema and interoperability is maintained. At the same time, other information systems, such as ReSS, will be able to publish the extra custom attributes for the OSG users.

---+++ How do I get my GIP to publish non Glue Schema attributes?
 The procedure in this section is for adding new attributes to existing DNs. 
%NOTE% If you want to add a completely new DN, follow instructions in case 2 (&quot;adding new DNs&quot;) in section &quot;[[GenericInformationProviders#Overwriting_attributes_published][Overwriting attributes published by GIP]]&quot; below. 
   1 Add the extra attributes to appropriate GIP templates file. There are two sets of templates for GIP 1.0, and both must be edited. Look in =$VDT_LOCATION/gip/templates= and =$VDT_LOCATION/templates/compat=. In the upcoming GIP 1.0.2 bugfix release (slated for September 2008), you do not need to edit *any* templates. Just add it to alter-attributes.conf, as documented in the next step. The only restrictions on the newly added attributes are: 
      * The attribute should not start with the keyword &#39;Glue&#39; 
      * It should not be called &#39;dn&#39; 
      * It should not be called &#39;objectClass&#39; 
   1 Set the value that you want to publish . This can be done in any of the following ways: 
      * Add an entry in the =alter-attributes.conf= file to change this value. (see how to do this in case 1 in section &quot;[[GenericInformationProviders#Overwriting_attributes_published][Overwriting attributes published by GIP]]&quot; below). 
      * You can set static attribute value in the template file (step 1) 
      * Write a more full-fledged plugin, if the values you need to publish change dynamically. (GIP plugins and providers included in the GIP can be found in =$VDT_LOCATION/gip/libexec/=) 
   1 Run =$VDT_LOCATION/vdt/setup/configure_gip= (if you don&#39;t, the values will get few hours to get updated, since configure_gip gets run every few hours) 
   1 Run =$VDT_LOCATION/gip/libexec/osg-info-wrapper= to see the GIP output. 

---++++!! An Example
 Let us say you want to advertise MPI attributes (!MPIInterconnect, !MPICompilerLocation, MPIVendor) that are associated with a particular subcluster. To accomplish this you would do the following 
   * First you will modify the =$VDT_LOCATION/gip/etc/GlueCluster.template= and add the following entry within the subcluster region &lt;pre class=&quot;file&quot;&gt;MPIInterconnect:
MPIVendor:
MPICompilerLocation: &lt;/pre&gt; 
   * Next update =$VDT_LOCATION/gip/etc/alter-attributes.conf= to add the values associated with the cluster. &lt;pre class=&quot;file&quot;&gt;dn: GlueSubClusterUniqueID=mysubcluster1.univ.edu, GlueClusterUniqueID=mycluster.univ.edu,mds-vo-name=local,o=grid&lt;br /&gt;MPIVendor: MPICH&lt;br /&gt;MPIInterconnect: Infiniband&lt;br /&gt;MPICompilerLocation: /usr/local/mpi/mpi-compiler&lt;br /&gt;&lt;br /&gt;dn: GlueSubClusterUniqueID=mysubcluster2.univ.edu, GlueClusterUniqueID=mycluster.univ.edu,mds-vo-name=local,o=grid&lt;br /&gt;MPIVendor: OpenMPI&lt;br /&gt;MPIInterconnect: GigE&lt;br /&gt;MPICompilerLocation: /sw/mpi/mpi-compiler &lt;/pre&gt; 
   * Next run =$VDT_LOCATION/vdt/setup/configure_gip= and verify the newly added values are getting propagated by checking the output of =$VDT_LOCATION/gip/libexec/osg-info-wrapper=.

---++ Overwriting attributes published by GIP

Since =configure_gip= runs as a cron jobs site admins who used to manually update some values can no longer directly update the ldif files. Instead we have added two files: =alter-attributes.conf= and =add-attributes.conf=. These files are to be located at =$VDT_LOCATION/gip/etc/=. They exploit the plugin and provider mechanism of OSG-GIP to give admins the highest priority to set any values. This approach is particularly useful in case you want to add new dn&#39;s. This also makes it easier to preserve changes across software updates.

%IMPORTANT% If you want to alter or add multiple DNs, remember to have two new-line characters between the DNs.

%IMPORTANT% Each override entry has to begin with the line &lt;br /&gt; =dn: Glue...UniqueId=...,...= %BR% This line is crucial to identify where the information you are putting should go. This can be thought of as an hash key, telling GIP what exact attribute should be altered. ([[http://tille.garrels.be/training/ldap/ch01s03.html][More about the ldif format...]])

---++++!! Typical use-case for overwriting attributes

---+++++!! Case 1: Altering values published for attributes for existing DNs

This allows the site admin to alter the values that are being published.

Example 1: Changing the values associated with !GlueHostNetworkAdapterInboundIP and !GlueHostNetworkAdapterOutboundIP By default these attributes are initialized as follows:

&lt;verbatim&gt;GlueHostNetworkAdapterInboundIP: FALSE
GlueHostNetworkAdapterOutboundIP: TRUE &lt;/verbatim&gt;

Let us say for example you want to specify that you Worker nodes don&#39;t have network access. This can be done as follows:

Edit/Create =$VDT_LOCATION/gip/etc/alter-attributes.conf=, and add lines similar to the example below (You need to change &#39;grow-test1.its.uiowa.edu&#39; to your sites &#39;FQDN&#39;)

&lt;pre class=&quot;file&quot;&gt;dn: GlueSubClusterUniqueID=grow-test1.its.uiowa.edu, GlueClusterUniqueID=grow-test1.its.uiowa.edu,mds-vo-name=local,o=grid
GlueHostNetworkAdapterOutboundIP: FALSE &lt;/pre&gt;

Example 2: If you want to state that you have inbound access to your worker node, as well as change the Operating System value being published for this !SubCluster. In this case you will again edit =$VDT_LOCATION/gip/etc/alter-attributes.conf= and add the lines as you see in the illustration below.

%TWISTY{
mode=&quot;div&quot;
showlink=&quot;Show...&quot;
hidelink=&quot;Hide&quot;
showimgleft=&quot;%ICONURLPATH{toggleopen-small}%&quot;
hideimgleft=&quot;%ICONURLPATH{toggleclose-small}%&quot;
}%
&lt;pre class=&quot;screen&quot;&gt;dn: GlueSubClusterUniqueID=grow-test1.its.uiowa.edu, GlueClusterUniqueID=grow-test1.its.uiowa.edu,mds-vo-name=local,o=grid
GlueHostNetworkAdapterInboundIP: TRUE
GlueHostOperatingSystemName: My OS
GlueHostOperatingSystemRelease: MY OS RELEASE
GlueHostOperatingSystemVersion: MY-OS_V1 &lt;/pre&gt;
%ENDTWISTY%
&lt;br/&gt;
---+++++!! Case 2: Adding new DNs
 This gives site admin a mechanism to publish new information that are not being published by GIP. This will be most useful if site admins want to publish multiple subclusters.

%NOTE% This is an addition to existing !DNs which are already being published.

Example: Let us say you already have one !SubCluster being published by !GIP and you would like to associate two more sub-clusters (with different architecture, !OS) within the same cluster. Let these be identified by unique IDs grow-subcluster1.its.uiowa.edu and grow-subcluster2.its.uiowa.edu, and let them be associated with the cluster grow-cluster.its.uiowa.edu. Then in order to publish these two new !SubClusters, we will add the following entries to the =$VDT_LOCATION/gip/etc/add-attributes.conf= file.

%IMPORTANT% It is important to get the !UniqueIDs correct since LDAP used them to form its directory structure

%TWISTY{
mode=&quot;div&quot;
showlink=&quot;Show...&quot;
hidelink=&quot;Hide&quot;
showimgleft=&quot;%ICONURLPATH{toggleopen-small}%&quot;
hideimgleft=&quot;%ICONURLPATH{toggleclose-small}%&quot;
}%
&lt;pre class=&quot;file&quot;&gt;
dn: GlueSubClusterUniqueID=grow-subcluster1.its.uiowa.edu, GlueClusterUniqueID=grow-cluster.its.uiowa.edu,mds-vo-name=local,o=grid
objectClass: GlueClusterTop
objectClass: GlueSubCluster
objectClass: GlueSchemaVersion
objectClass: GlueHostApplicationSoftware
objectClass: GlueHostArchitecture
objectClass: GlueHostBenchmark
objectClass: GlueHostMainMemory
objectClass: GlueHostNetworkAdapter
objectClass: GlueHostOperatingSystem
objectClass: GlueHostProcessor
objectClass: GlueInformationService
objectClass: GlueKey
GlueChunkKey: GlueClusterUniqueID=grow-cluster.its.uiowa.edu
GlueHostApplicationSoftwareRunTimeEnvironment: OSG-ITB-0.5.2
GlueHostArchitectureSMPSize: 2
GlueHostBenchmarkSF00: 380
GlueHostBenchmarkSI00: 400
GlueHostMainMemoryRAMSize: 512
GlueHostMainMemoryVirtualSize: 1024
GlueHostNetworkAdapterInboundIP: FALSE
GlueHostNetworkAdapterOutboundIP: TRUE
GlueHostOperatingSystemName: MAC OS X
GlueHostOperatingSystemRelease: 10.4.6
GlueHostOperatingSystemVersion: Unknown
GlueHostProcessorClockSpeed: 1000
GlueHostProcessorModel: Mac
GlueHostProcessorVendor: Mac
GlueSubClusterName: grow-subcluster1.its.uiowa.edu
GlueSubClusterUniqueID: grow-subcluster1.its.uiowa.edu
GlueSubClusterPhysicalCPUs: 1
GlueSubClusterLogicalCPUs: 2
GlueSubClusterTmpDir: /grow/data-local/data
GlueSubClusterWNTmpDir: /tmp
GlueInformationServiceURL: ldap://grow-test1.its.uiowa.edu:2135/mds-vo-name=local,o=grid
GlueSchemaVersionMajor: 1
GlueSchemaVersionMinor: 2

dn: GlueSubClusterUniqueID=grow-subcluster2.its.uiowa.edu, GlueClusterUniqueID=grow-cluster.its.uiowa.edu,mds-vo-name=local,o=grid
objectClass: GlueClusterTop
objectClass: GlueSubCluster
objectClass: GlueSchemaVersion
objectClass: GlueHostApplicationSoftware
objectClass: GlueHostArchitecture
objectClass: GlueHostBenchmark
objectClass: GlueHostMainMemory
objectClass: GlueHostNetworkAdapter
objectClass: GlueHostOperatingSystem
objectClass: GlueHostProcessor
objectClass: GlueInformationService
objectClass: GlueKey
GlueChunkKey: GlueClusterUniqueID=grow-cluster.its.uiowa.edu
GlueHostApplicationSoftwareRunTimeEnvironment: OSG-ITB-0.5.2
GlueHostArchitectureSMPSize: 2
GlueHostBenchmarkSF00: 380
GlueHostBenchmarkSI00: 400
GlueHostMainMemoryRAMSize: 512
GlueHostMainMemoryVirtualSize: 1024
GlueHostNetworkAdapterInboundIP: FALSE
GlueHostNetworkAdapterOutboundIP: TRUE
GlueHostOperatingSystemName: RedHat Generic
GlueHostOperatingSystemRelease: RedHat Unknown
GlueHostOperatingSystemVersion: Unknown
GlueHostProcessorClockSpeed: 1000
GlueHostProcessorModel: Intel(R) Pentium(R) D CPU 3.00GHz
GlueHostProcessorVendor: GenuineIntel
GlueSubClusterName: grow-subcluster2.its.uiowa.edu
GlueSubClusterUniqueID: grow-subcluster2.its.uiowa.edu
GlueSubClusterPhysicalCPUs: 1
GlueSubClusterLogicalCPUs: 2
GlueSubClusterTmpDir: /grow/data-local/data
GlueSubClusterWNTmpDir: /tmp
GlueInformationServiceURL: ldap://grow-test1.its.uiowa.edu:2135/mds-vo-name=local,o=grid
GlueSchemaVersionMajor: 1
GlueSchemaVersionMinor: 2 
&lt;/pre&gt; 
%ENDTWISTY%
&lt;br/&gt;
---+++!! Adding site defined constraints to custom configure condor status commands
 In GIP, it is now possible to add &quot;--constraint&quot; options to condor commands from GIP, to allow for custom configuration of GIP values. This constraint gets passed on to condor_status requests made by the GIP dynamic plugins. The use case for this came from BNL, where they had the nodes being shared between both the ITB and production cluster and they specify constraints in their condor class ads, as to how many nodes are available in each case. By default GIP dynamic plugins get the information about free and total cpus by executing a &quot;condor_status&quot; command without any parameter. So GIP was publishing incorrect information that could potentially affect !ReSS. In order to address this use case we added a new attribute (OSG_GIP_CONDOR_STATUS_CONSTRAINT) can be added to =gip-attributes.conf=.

In order to enable this functionality (add administrator defined &quot;--constraint&quot; option to condor_status queries made by osg-info-dynamic-condor) add the following lines
&lt;pre class=&quot;file&quot;&gt;
OSG_GIP_CONDOR_STATUS_CONSTRAINT=&quot;Your_constraint&quot;
export OSG_GIP_CONDOR_STATUS_CONSTRAINT
&lt;/pre&gt; to the =$VDT_LOCATION/monitoring/gip-attributes.conf=

An example constraint
&lt;pre class=&quot;file&quot;&gt;
OSG_GIP_CONDOR_STATUS_CONSTRAINT=&quot;&#39;CPU_Type == \&quot;osgitb\&quot;&#39;&quot;
&lt;/pre&gt;


---++ Explanation of GIP / Glue Attributes

What follows is a large amount of [[http://glueschema.forge.cnaf.infn.it/Spec/V13][Glue attributes]] which are added by the GIP with some notes on what the attributes mean.

%NOTE% Not all the values here have been properly configured; yet, they are just provided as an example.

---+++!! GlueSite

*OSG unique site name* &lt;br /&gt; =GlueSiteName: CIT_CMS_OSG=

*Human readable description of the site* &lt;br /&gt; =GlueSiteDescription: Caltech CMS OSG Site=

*Site email contacts*
&lt;verbatim&gt;
GlueSiteUserSupportContact: mailto: my@email.com
GlueSiteSysAdminContact: mailto: my@email.com
GlueSiteSecurityContact: mailto: my@email.com
&lt;/verbatim&gt;

*Geographic location of the site*
&lt;verbatim&gt;
GlueSiteLocation: Pasadena, US 
GlueSiteLatitude: 34
GlueSiteLongitude: 118
&lt;/verbatim&gt;

*Web site describing the site, if any*
&lt;verbatim&gt;
GlueSiteWeb: http://www.opensciencegrid.org
&lt;/verbatim&gt;

---+++!! GlueCEUniqueID

GlueCEUniqueID defines a gatekeeper combined with a jobmanager and queue. The format for the GlueCEUniqueID is hostname:port/jobmanager-batch-queue .

*Please note that &#39;jobmanager-condor-atlas&#39; does not exist as a submittable jobmanager. &#39;jobmanager-condor&#39; is the actual jobmanager that exists while &#39;atlas&#39; is the queue that the job should be submitted to*

*The idea is that this should express how many CPU&#39;s are available at this time to a VO*

&lt;verbatim&gt;
dn: GlueCEUniqueID=rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas, mds-vo-name=local,o=grid
GlueCEHostingCluster: rsgrid3.its.uiowa.edu
GlueCEName: atlas
GlueCEUniqueID: rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas
GlueCEInfoGatekeeperPort: 2119
GlueCEInfoHostName: rsgrid3.its.uiowa.edu
GlueCEInfoLRMSType: condor
GlueCEInfoLRMSVersion: 6.7.7
&lt;/verbatim&gt;

*How many total CPU&#39;s (busy or available) are accessible on this queue via Grid interfaces to all VO. This parameter can be customized by site administrators.*
&lt;verbatim&gt;
GlueCEInfoTotalCPUs: 10
GlueCEInfoJobManager: condor
GlueCEInfoContactString: my-ce.my.domain:2119/jobmanager-condor-atlas
&lt;/verbatim&gt;

*How many total job slots (busy or available) are accessible by policy via Grid interfaces for a certain VO. For PBS, VO information is not available: the parameter is the total number of job slots for any VO i.e. !GlueCEInfoTotalCPUs*
&lt;verbatim&gt;
GlueCEPolicyAssignedJobSlots: 0
&lt;/verbatim&gt;

*How many CPU&#39;s are available right now via Grid interfaces to all VO*
&lt;verbatim&gt;
GlueCEStateFreeCPUs: 10
&lt;/verbatim&gt;

*How many job slots are available right now via Grid interfaces for a certain VO. For PBS, VO information is not available: the parameter is min( maximum_number_of_queuable_jobs - total_jobs_in_queue , free_cpus_for_the_queue ). In general, maximum_number_of_queuable_jobs may be different from free_cpus_for_the_queue by administrative policy.*
&lt;verbatim&gt;
GlueCEStateFreeJobSlots: 0
&lt;/verbatim&gt;

*RunningJobs + WaitingJobs*
&lt;verbatim&gt;
GlueCEStateTotalJobs: 0
&lt;/verbatim&gt;

*How many jobs are waiting in the queue*
&lt;verbatim&gt;
GlueCEStateWaitingJobs: 0
GlueCEStateWorstResponseTime: 0
&lt;/verbatim&gt;

*How long a VO can expect to wait before being submitted to a machine*
&lt;verbatim&gt;
GlueCEStateEstimatedResponseTime: 0
&lt;/verbatim&gt;

*How long a job may remain in the queue before being removed (CPU time)*
&lt;verbatim&gt;
GlueCEPolicyMaxCPUTime: 0
GlueCEPolicyMaxRunningJobs: 0
GlueCEPolicyMaxTotalJobs: 0
&lt;/verbatim&gt;

*How long a job may remain in the queue before being removed (Wall time)*
&lt;verbatim&gt;
GlueCEPolicyMaxWallClockTime: 0
GlueCEPolicyPriority: 1
&lt;/verbatim&gt;

*Which VO may run jobs on this queue*
&lt;verbatim&gt;
GlueCEAccessControlBaseRule: VO:atlas
GlueForeignKey: GlueClusterUniqueID=rsgrid3.its.uiowa.edu
GlueInformationServiceURL: ldap://rsgrid3.its.uiowa.edu:2135/mds-vo-name=local,o=grid
&lt;/verbatim&gt;

---+++!! GlueSubClusterUniqueID

GlueSubClusterUniqueID provides information about what grid software version is installed on the cluster as well as general information about a cluster. It also can hold VO published information about what software has been installed. For example, !GlueHostApplicationSoftwareRunTimeEnvironment: CMKIN could also be added by a VO.

&lt;verbatim&gt;
dn: GlueSubClusterUniqueID=rsgrid3.its.uiowa.edu, GlueClusterUniqueID=rsgrid3.its.uiowa.edu, mds-vo-name=local,o=grid
GlueChunkKey: GlueClusterUniqueID=rsgrid3.its.uiowa.edu
&lt;/verbatim&gt;

*GlueHostApplicationSoftwareRunTimeEnvironment lists all the software installed on the cluster*
&lt;verbatim&gt;
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_1_0
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_1_1
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_2_0
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_3_0
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_3_1
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_4_0
GlueHostApplicationSoftwareRunTimeEnvironment: R-GMA
GlueHostArchitectureSMPSize: 2
&lt;/verbatim&gt;

*The SI and SF benchmarks have been used to make sure that a job will finish in a cluster before being removed*
&lt;verbatim&gt;
GlueHostBenchmarkSF00: 0
GlueHostBenchmarkSI00: 381
GlueHostMainMemoryRAMSize: 513
GlueHostMainMemoryVirtualSize: 1025
&lt;/verbatim&gt;

*Method of publishing information about whether or not the cluster nodes have inbound / outbound access*
&lt;verbatim&gt;
GlueHostNetworkAdapterInboundIP: FALSE
GlueHostNetworkAdapterOutboundIP: TRUE
GlueHostOperatingSystemName: Redhat
GlueHostOperatingSystemRelease: 7.3
GlueHostOperatingSystemVersion: 3
GlueHostProcessorClockSpeed: 1001
GlueHostProcessorModel: PIII
GlueHostProcessorVendor: intel
GlueSubClusterName: rsgrid3.its.uiowa.edu
GlueSubClusterUniqueID: rsgrid3.its.uiowa.edu
GlueSubClusterPhysicalCPUs: 0
GlueSubClusterLogicalCPUs: 0
GlueSubClusterTmpDir: /tmp
GlueSubClusterWNTmpDir: /tmp
GlueInformationServiceURL: ldap://rsgrid3.its.uiowa.edu:2135/mds-vo-name=local,o=grid
&lt;/verbatim&gt;

---+++!! GlueCESEBindGroupCEUniqueID

Defines which storage element is considered close to this Computing Element. This is done on a per VO basis. For example, these attributes can define the default location for Atlas data to go would be to rsgrid3.its.uiowa.edu and into the directory =/storage/atlas=. Another convention on the LCG is that the mount point has a folder in it for each VO that is accepted.

&lt;verbatim&gt;
dn: GlueCESEBindGroupCEUniqueID=rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas, mds-vo-name=local,o=grid, GlueCESEBindGroupCEUniqueID: rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas
GlueCESEBindGroupSEUniqueID: rsgrid3.its.uiowa.edu

dn: GlueCESEBindSEUniqueID=rsgrid3.its.uiowa.edu, GlueCESEBindGroupCEUniqueID=rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas, mds-vo-name=local,o=grid
GlueCESEBindSEUniqueID: rsgrid3.its.uiowa.edu
GlueCESEBindCEAccesspoint: /storage
GlueCESEBindCEUniqueID: rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas
GlueCESEBindMountInfo: none
GlueCESEBindWeight: 0
&lt;/verbatim&gt;

---+++!! GlueSEUniqueID

Defines an access point of a Storage Element.

&lt;verbatim&gt;
dn: GlueSEUniqueID=rsgrid3.its.uiowa.edu,mds-vo-name=local,o=grid
GlueSEUniqueID: rsgrid3.its.uiowa.edu
GlueSEName: my-site-name:disk
GlueSEPort: 8443
GlueSESizeTotal: 0
GlueSESizeFree: 0
GlueSEArchitecture: disk
GlueInformationServiceURL: ldap://rsgrid3.its.uiowa.edu:2135/mds-vo-name=local,o=grid
&lt;/verbatim&gt;

---+++!! GlueSEAccessProtocolLocalID

Defines the protocols that may be used to access this Storage Element. The example below defines gsiftp as the protocol.

&lt;verbatim&gt;
dn: GlueSEAccessProtocolLocalID=gsiftp, GlueSEUniqueID=rsgrid3.its.uiowa.edu,Mds-Vo-name=local,o=grid
GlueSEAccessProtocolLocalID: gsiftp
GlueSEAccessProtocolType: gsiftp
GlueSEAccessProtocolVersion: 1.0.0
GlueSEAccessProtocolPort: 2811
GlueSEAccessProtocolSupportedSecurity: GSI
GlueChunkKey: GlueSEUniqueID=rsgrid3.its.uiowa.edu
&lt;/verbatim&gt;

%STOPINCLUDE% 

&lt;!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AnthonyTiradani

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = GabrieleGarzoglio
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


############################################################################################################
--&gt;




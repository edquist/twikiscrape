---+!! *&lt;noop&gt;%SPACEOUT{ &quot;%TOPIC%&quot; }%*
%DOC_STATUS_TABLE%
%TOC{depth=&quot;2&quot;}%

---++ Introduction
Describes real-world hardware configurations for several sizes of OSG cluster from ITB to largest production.

---++ Basic design principles
The load on CE&#39;s and authentication servers such as GUMS depends on the rate of job start and stop, not necessarily on
the total size of the cluster.  Lots of short jobs will be the biggest stress on a cluster, particularly if the !GlideinWMS is
running jobs on your cluster and gLexec is being used.  The size of executable and the amount of file transfer also 
are critical in the load.  A lot of small files get written on the CE by globus and condor.  NFS configuration is easiest to 
set up, but shouldn&#39;t be tried unless you have an NFS server of the class of Netapp or Bluearc.  A single Linux box
serving NFS is not adequate even on a small ITB cluster.

---++ OSG persistent ITB testbed @ Fermilab
    * 2 head nodes, Dell !PowerEdge 1950, 16GB RAM, 2x Intel Xeon 5355 processor, 2.66 !GHz, single 500GB SATA disk. (vintage 2007)

    * 8 worker nodes, Dell !PowerEdge 1950, 16GB RAM, 2x Intel Xeon 5355 processor, 2.66 !GHz, single 500GB SATA disk. (vintage 2007)

    * Head nodes and worker nodes are both divided into virtual machines using Sci. Linux 5.5 / Xen 3.1.2.

    * 8 worker nodes divided into 32 virtual worker nodes (16 condor, 8 PBS, 8 SGE)

    * 2 head nodes divided into 7 virtual machines apiece.  each CE is 2GB of RAM. 

    * SE:  Two virtual machines, one for !dCache head node, and one for !dCache pool node, hosted in !FermiCloud.

    * dCache pool node is 2xIntel E5640 (Westmere), 6x2TB SATA drives in Raid 5 configuration.  (in progress)

    * GUMS server not part of OSG-ITB testbed but is hosted on an old 2005-vintage machine, load is low.

    * Home directories for grid users, as well as OSG_APP, OSG_DATA are served by Bluearc NAS appliance.

---++ General Purpose Grid Cluster @ Fermilab
    * Four head nodes, Dell !PowerEdge 1950, 16GB RAM, 2x Intel Xeon 5355 processor, 2.66 GHz, single 500GB SATA disk. (vintage 2007)

    * Head nodes divided into 4 virtual machines apiece.  On 1st 3:  Primary CE, 5GB RAM, Condor master, 4GB RAM, backup CE, 4GB RAM, YP sever 1GB RAM

    *  On 4th--Bestman Gateway, postgres database for condor quill.

    * 3260 active batch slots    

    * Note that you really don&#39;t need 3 CE&#39;s to keep 3260 batch slots full, one CE could do the job up to about 5K slots.  We do it for different functionalities and for redundancy in case one fails.

    * Home directories for grid users, as well as OSG_APP, OSG_DATA are served by Bluearc NAS appliance.

    * !AuthN/!AuthZ shared with rest of !FermiGrid which uses 2xDell Poweredge 2950 (vintage 2008) servers for GUMS and SAZ.

---++ !FermiGrid CDF Grid cluster as of 2007 (1200 slots, single CE)
   * CE with 6GB RAM.

   * At least 2 CPUs, as fast as you can get.

   * Enough disk to hold the VDT software and logs (~250GB).

   * For small­/medium size cluster, this machine can be an NFS server for home areas, too.

   * For large cluster we strongly recommend getting a separate enterprise-class NFS server for home areas and APP and DATA areas too.

   * The major loads of memory on a CE are the following:

   * Globus-ws container, 700MB

   * CEMon, 450 MB

   * For a condor-based system, one condor_shadow process per job that is running (7MB apiece).

   * See CondorBatchSystemHints for hints on how to move the condor master off of your head node  (this helps out reliability a lot).

   * See Trash.ReleaseDocumentationPbsBatchSystemHints for how to split the PBS master off of your head node.

---+++Load Limits
   * One example:  system fcdfosg2, 2x3.2GHz Xeon (hyperthreaded) vintage 2005, now 8GB of RAM.   Averages 6000 grid jobs a day, 2050 job slots running simultaneously.  Works ok but sometimes goes up to load of 50 during rapid job submission sequences.


---++ For VOMS Support
If you are supporting a VO, use a 2nd machine of with these specifications to be your VOMS server.

---++ GUMS Support
If you are running GUMS, use a 3rd server with these specifications. 


&lt;!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = StevenTimm

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Planning
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = HorstSeverini
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


############################################################################################################
--&gt;

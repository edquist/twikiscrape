<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> OSGMMExercises &lt; Education &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Education/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Education/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Education/OSGMMExercises?_T=16 Feb 2017" type="application/x-wiki" title="edit OSGMMExercises" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/bin/view/Education/WebRss" />    
<base href="https://twiki.opensciencegrid.org/bin/view/Education/OSGMMExercises"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#D0D0D0;
	}
	.patternBookView {
		border-color:#D0D0D0;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">

@import url('/pub/Documentation/Tools/exercises.css ');

</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <h1><a name="OSGMMExercises"></a>  <strong>OSGMMExercises</strong> </h1>
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Querying_ReSS_directly_using_con"> Querying ReSS directly using condor_status</a>
</li> <li> <a href="?cover=print#Querying_local_OSGMM_instance_us"> Querying local OSGMM instance using condor_status</a>
</li> <li> <a href="?cover=print#Copy_the_exercise_directory"> Copy the exercise directory</a>
</li> <li> <a href="?cover=print#BLAST_Condor_G_example_with_matc"> BLAST / Condor-G example with match making</a>
</li> <li> <a href="?cover=print#Povray_Exercise"> Povray Exercise</a> <ul>
<li> <a href="?cover=print#Starting_a_Run"> Starting a Run</a>
</li> <li> <a href="?cover=print#Checking_on_the_Jobs"> Checking on the Jobs</a>
</li> <li> <a href="?cover=print#Run_Directory"> Run Directory</a>
</li> <li> <a href="?cover=print#More_information"> More information</a>
</li></ul> 
</li></ul> 
</div>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Querying_ReSS_directly_using_con"></a> Querying <span class="twikiNewLink">ReSS<a href="/bin/edit/Education/ReSS?topicparent=Education.OSGMMExercises" rel="nofollow" title="ReSS (this topic does not yet exist; you can create it)">?</a></span> directly using condor_status </span></h2>
<p />
OSG provides a central <span class="twikiNewLink">ReSS<a href="/bin/edit/Education/ReSS?topicparent=Education.OSGMMExercises" rel="nofollow" title="ReSS (this topic does not yet exist; you can create it)">?</a></span> master server at osg-ress-1.fnal.gov. You can use the condor_status command to query it directly. But note that the collector contains something like 15000 ads. Let's start with just counting the ads using the wc (wordcount) command. Please run:
<p />
<pre class="screen">
$ condor_status -pool osg-ress-1.fnal.gov | wc -l
</pre>
<p />
How many entries did you get?
<p />
Let's ask for just the ads for sites supporting the osgedu VO. Run:
<p />
<pre class="screen">
$ condor_status -pool osg-ress-1.fnal.gov -constraint 'StringlistIMember("VO:osgedu";GlueCEAccessControlBaseRule)' | wc -l
</pre>
<p />
How many entries did you get?
<p />
You can also use the -format to list the site names and contact strings:
<p />
<pre class="screen">
$ condor_status -pool osg-ress-1.fnal.gov -constraint 'StringlistIMember("VO:osgedu";GlueCEAccessControlBaseRule)' -format '%-20s' GlueSiteUniqueID -format '%s\n' GlueCEInfoContactString
</pre>
<p />
The reason for duplicate entries is that they ads list different storage options. At this point we just want sitenames and contact strings, so let's narrow down the result set. Run:
<p />
<pre class="screen">
$ condor_status -pool osg-ress-1.fnal.gov -constraint 'StringlistIMember("VO:osgedu";GlueCEAccessControlBaseRule)' -format '%-25s' GlueSiteUniqueID -format '%s\n' GlueCEInfoContactString | sort | uniq
</pre>
<p />
This is the set of sites which advertises support for the osgedu VO. But not all these sites will actually work. They might be under maintenance, having problems or just misconfigured and not being able to run jobs from the VO.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Querying_local_OSGMM_instance_us"></a> Querying local OSGMM instance using condor_status </span></h2>
<p />
OSGMM uses similar queries as the ones above to download the ads for the configured VO. It then verifies the sites using test jobs (usually every 12-24 hours), and includes the test results in the ads. Let's query the local Condor collector which is used by the local OSGMM for sites which have passed the tests. Run:
<p />
<pre class="screen">
$ condor_status -constraint 'SiteVerified == true' -format '%-25s' GlueSiteUniqueID -format '%s\n' GlueCEInfoContactString | sort
</pre>
<p />
Note how this is a subset of what we found in the last query againt <span class="twikiNewLink">ReSS<a href="/bin/edit/Education/ReSS?topicparent=Education.OSGMMExercises" rel="nofollow" title="ReSS (this topic does not yet exist; you can create it)">?</a></span>. Some sites did not pass the verification jobs. The sites listed here did pass and should work if you send jobs to them.
<p />
The sites and OSGMM puts a lot of other interesting attributes into the ads. Take a look at one full ad by running:
<p />
<pre class="screen">
$ condor_status -l -constraint 'GlueSiteUniqueID == "RENCI-Engagement"'
</pre>
<p />
Some of the attributes you might want to use when matching jobs are:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table1" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<th bgcolor="#d8dde4" valign="top" class="twikiTableCol0 twikiFirstCol"> <a rel="nofollow" href="/bin/view/Education/OSGMMExercises?cover=print;sortcol=0;table=1;up=0#sorted_table" title="Sort by this column"><font color="#252b37">Attribute Name</font></a> </th>
			<th bgcolor="#d8dde4" valign="top" class="twikiTableCol1"> <a rel="nofollow" href="/bin/view/Education/OSGMMExercises?cover=print;sortcol=1;table=1;up=0#sorted_table" title="Sort by this column"><font color="#252b37">Value</font></a> </th>
			<th bgcolor="#d8dde4" valign="top" class="twikiTableCol2 twikiLastCol"> <a rel="nofollow" href="/bin/view/Education/OSGMMExercises?cover=print;sortcol=2;table=1;up=0#sorted_table" title="Sort by this column"><font color="#252b37">Description</font></a> </th>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> OSGMM_CENetworkOutbound </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> True/False </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> True if you can make outbound network connections from the compute nodes </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> OSGMM_CPUBits </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> 32/64 </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Is the system a 32 or 64 bit system? </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> OSGMM_MemPerCPU </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Megabytes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Memory available to a job </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> OSGMM_DISK_GB_AVAILABLE_OSG_DATA </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLast"> Gigabytes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> Available space under $OSG_DATA </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
For example, if you have a job which requires 50 GB of disk space under $OSG_DATA and 2GB memory, you can find sites by running:
<p />
<pre class="screen">
$ condor_status -constraint 'SiteVerified == true && OSGMM_MemPerCPU >= 2048000 && OSGMM_DISK_GB_AVAILABLE_OSG_DATA >= 50' -format '%-20s' GlueSiteUniqueID -format '%s\n' GlueCEInfoContactString | sort
</pre>
<p />
OSGMM can also install and advertise software/datasets on sites. For the upcoming exercises we will send jobs to sites which have blast and povray installed. These sites cab be found with:
<p />
<pre class="screen">
$ condor_status -constraint 'SiteVerified == true && OSGMM_Software_Blast == True' -format '%-25s' GlueSiteUniqueID -format '%s\n' GlueCEInfoContactString | sort
$ condor_status -constraint 'SiteVerified == true && OSGMM_Software_Povray == True' -format '%-25s' GlueSiteUniqueID -format '%s\n' GlueCEInfoContactString | sort
</pre>
<p />
<p />
Lastly, if all you want is a quick overview of site and job status, OSGMM provides a convenience command called condor_grid_overview. We will use that in the exercises below. For now, just try to run it:
<p />
<pre class="screen">
$ condor_grid_overview
</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Copy_the_exercise_directory"></a> Copy the exercise directory </span></h2>
<p />
For the next couple of exercises, we will use already existing job examples. Please copy the exercise directory by running the following command:
<p />
<pre class="screen">
$ cp -r /opt/workshop/osgmm-exercises ~/
</pre>
<p />
You should now have a directory under your home directory called "osgmm-exercises"
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="BLAST_Condor_G_example_with_matc"></a> BLAST / Condor-G example with match making </span></h2>
<p />
This example builds on the BLAST example you did yesterday and earlier today. It is using the information provided by <span class="twikiNewLink">ReSS<a href="/bin/edit/Education/ReSS?topicparent=Education.OSGMMExercises" rel="nofollow" title="ReSS (this topic does not yet exist; you can create it)">?</a></span> and OSGMM and will automatically distribute your jobs out on the grid. This example includes features such as failure detection and automatically resubmits so it looks a little bit complex compared to what we have done up until now. Let's start by taking a look at the provided files. Please open them up in an editor at least scan them. Cd into the work directory and list the files.
<p />
<pre class="screen">
$ cd ~/osgmm-exercises/blast/
$ ls
</pre>
<p />
Description of the files / directories:
<p /> <ul>
<li> <strong>blast-summarize</strong> - Same summary script that Alain provided yesterday.
</li></ul> 
<p /> <ul>
<li> <strong>inputs</strong> - Directory containing inputs for the jobs - the 10 ones we used yesterday afternoon
</li></ul> 
<p /> <ul>
<li> <strong>local-pre-job</strong> - It is a script which DAGMan runs before each job. It is mostly a placeholder in the Blast example.
</li></ul> 
<p /> <ul>
<li> <strong>local-post-job</strong> - It is a script which DAGMan runs <strong>locally</strong> after <strong>each job</strong> completes. This is a very important part of detecting job failures. Failures should be expected in any distributed system, so it is important to detect and handle failures accordingly. The local-post-job script checks the job output to determine if the job was successful or not, and if a failure is detected, the script exists with exit code of 1, which signals to the DAGMan that the job should be resubmitted. The local-post-job script is also a good hook to place post-processing. In this case we want to untar the output tar.gz file from the job and run blast-summarize on the output.
</li></ul> 
<p /> <ul>
<li> <strong>remote-blast-wrapper</strong> - This is the actual job that runs on the <strong>remote side</strong>. In most cases you will have to wrap your executable in a job wrapper to do the staging in/out (tar.gz in our case), work dir handling (moving to $OSG_WN_TMP for example for local disk I/O) and to do some extra error detection. 
</li></ul> 
<p /> <ul>
<li> <strong>runs</strong> - This is a directory in which is used to old each "run". When you start a new run, a timestamped directory will be created in here to hold logs and outputs.
</li></ul> 
<p /> <ul>
<li> <strong>submit</strong> - This is the submit script. It creates the Condor jobs based on inputs/parameters, sets up input files in tar.gzs and a DAGMan description. Then the run is submitted to Condor for execution.
</li></ul> 
<p />
Let's start a run and see how it works. First make sure you have a valid proxy:
<p />
<pre class="screen">
$ voms-proxy-info
</pre>
<p />
If you do not have a proxy or it is expired / close to expired, generate a new one with voms-proxy-init like described earlier.
<p />
Then, kick off a new run:
<p />
<pre class="screen">
$ ./submit
</pre>
<p />
Cd into runs/ and check the newly created run directory:
<p />
<pre class="screen">
$ cd  runs/
$ ls
</pre>
<p />
Cd into the timestamped directory and ls in there. You should see something like:
<p />
<pre class="screen">
$ ls
10.condor  alljobs.log       inputs-9.tar.gz        outputs-1.tar.gz
1.condor   inputs-10.tar.gz  logs                   outputs-2.tar.gz
2.condor   inputs-1.tar.gz   master.dag             outputs-3.tar.gz
3.condor   inputs-2.tar.gz   master.dag.condor.sub  outputs-4.tar.gz
4.condor   inputs-3.tar.gz   master.dag.dagman.log  outputs-5.tar.gz
5.condor   inputs-4.tar.gz   master.dag.dagman.out  outputs-6.tar.gz
6.condor   inputs-5.tar.gz   master.dag.lib.err     outputs-7.tar.gz
7.condor   inputs-6.tar.gz   master.dag.lib.out     outputs-8.tar.gz
8.condor   inputs-7.tar.gz   master.dag.lock        outputs-9.tar.gz
9.condor   inputs-8.tar.gz   outputs-10.tar.gz
</pre>
<p />
The input tar files contains the input queries. The output tar files are empty until they have been staged back from the remote side. The master.* files are Condor DAGMan files. Let's take a look at the condor submit script. 
<p />
<pre class="screen">
universe        = grid
grid_type       = gt2
globusscheduler = $$(GlueCEInfoContactString)
globusrsl       = (maxWallTime=60)(min_memory=800)(max_memory=800)
requirements    = ( (TARGET.GlueCEInfoContactString =!= UNDEFINED) \
                    && (TARGET.Rank > 300) \
                    && (TARGET.OSGMM_MemPerCPU >= (800 * 1000)) \
                    && (TARGET.OSGMM_Software_Blast == TRUE) \
                    && ( isUndefined(TARGET.OSGMM_Success_Rate_rynge) \
                          || (TARGET.OSGMM_Success_Rate_rynge > 75) ) \
                  )

# when retrying, remember the last 4 resources tried
match_list_length = 4
Rank              = (TARGET.Rank) - \
                    ((TARGET.Name =?= LastMatchName0) * 1000) - \
                    ((TARGET.Name =?= LastMatchName1) * 1000) - \
                    ((TARGET.Name =?= LastMatchName2) * 1000) - \
                    ((TARGET.Name =?= LastMatchName3) * 1000)

# make sure the job is being retried and rematched
periodic_release = (NumGlobusSubmits < 5)
globusresubmit = (NumSystemHolds >= NumJobMatches)
rematch = True
globus_rematch = True

# only allow for the job to be queued for a while, then try to move it
#  GlobusStatus==16 is suspended
#  JobStatus==1 is pending
#  JobStatus==2 is running
periodic_hold = ( (GlobusStatus==16) || \
                  ((JobStatus==1) && ((CurrentTime - EnteredCurrentStatus) > (30*60))) || \
                  ((JobStatus==2) && ((CurrentTime - EnteredCurrentStatus) > (60*60))) )

# stay in queue on failures
on_exit_remove = (ExitBySignal == False) && (ExitCode == 0)

executable = ../../remote-blast-wrapper
arguments = 2010-07-18_172638 1  query1

stream_output = False
stream_error  = False

WhenToTransferOutput = ON_EXIT
TransferExecutable = true

Transfer_Input_Files = inputs-1.tar.gz
Transfer_Output_Files = outputs-1.tar.gz

output = logs/1/job.out
error = logs/1/job.err
log = alljobs.log

notification = NEVER

queue

</pre>
<p />
Note how the requirements string list memory and other requirements of the job. This is what is used to match up with sites which should be able to handle our jobs. Also note globusscheduler = $$(<span class="twikiNewLink">GlueCEInfoContactString<a href="/bin/edit/Education/GlueCEInfoContactString?topicparent=Education.OSGMMExercises" rel="nofollow" title="GlueCEInfoContactString (this topic does not yet exist; you can create it)">?</a></span>). In the Condor-G example we had a site contact here. In match making we make Condor fill it out with information from the target site ad.
<p />
Now, check the status of your run with:
<p />
<pre class="screen">
$ condor_grid_overview
</pre>
<p />
Once jobs comes back, check the outputs/ directory for the blast output and summary outputs.
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Povray_Exercise"></a> Povray Exercise </span></h2>
<p />
For this demo we will be using Povray, which is a raytracer used to create three-dimensional graphics. A scene (including objects, textures, lights, cameras, ...) is described in a scene description language, and Povray then follow the light rays and creates a image with shadows, transparency and so on. This final output we are going to do in this demo is an image of a Bonsai tree:
<p />
<img src="/twiki/pub/Education/OSGMMExercises/rendered.png" alt="rendered.png" width='400' height='300' />
<p />
<p />
Running Povray can be pretty compute intensive. How intensive depends on scene complexity and output size of the image. Running on a grid like OSG, one of the things you will have to do is break your problem up into sub problems. When animation studios render full movies, each frame of the move is a job. For our rendering problem, we will split the task of rendering the full image up into the problems of rendering tiles of the image, and then when we have all the tiles, we will put the tiles together into the final image. How many tiles you want to do is a user setting, but one possible break down would be 8x6 (48 tiles).
<p />
<img src="/twiki/pub/Education/OSGMMExercises/rendered_split_lines.png" alt="rendered_split_lines.png" width='500' height='375' />
<p />
The steps necessary to create a workflow is:
<p /> <ul>
<li> for each tile, create a Condor submit file to describe the job and pass the needed parameters to the job
</li> <li> add all the Condor jobs to the DAGMan
</li> <li> submit the DAGMan
</li></ul> 
<p />
Just like in the BLAST example, we have a submit script which takes cares of these steps.
<p />
<h3><a name="Starting_a_Run"></a> Starting a Run </h3>
<p />
Description of the files / directories:
<p /> <ul>
<li> <strong>helpers</strong> - This is a directory holding helper scripts. In the case of the povray example, there is only one helper, tiles-combine, which is used to combine all the individual tiles into one final image
</li></ul> 
<p /> <ul>
<li> <strong>inputs</strong> - Directory containing inputs for the jobs, which include the Povray scene description for the Bonsai scene
</li></ul> 
<p /> <ul>
<li> <strong>local-pre-job</strong> - It is a script which DAGMan runs before each job. It is mostly a placeholder in the Povray example.
</li></ul> 
<p /> <ul>
<li> <strong>local-post-job</strong> - It is a script which DAGMan runs after each job completes. This is a very important part of detecting job failures. Failures should be expected in any distributed system, so it is important to detect and handle failures accordingly. The local-post-job script checks the job output to determine if the job was successful or not, and if a failure is detected, the script exists with exit code of 1, which signals to the DAGMan that the job should be resubmitted.
</li></ul> 
<p /> <ul>
<li> <strong>remote-povray-wrapper</strong> - This is the actual job that runs on the remote side. In most cases you will have to wrap your executable in a job wrapper to do the staging in/out, work dir handling (moving to $OSG_WN_TMP for example for local disk I/O) and to do some extra error detection.
</li></ul> 
<p /> <ul>
<li> <strong>runs</strong> - This is a directory in which is used to old each "run". When you start a new run, a timestamped directory will be created in here to hold logs and outputs.
</li></ul> 
<p /> <ul>
<li> <strong>submit</strong> - This is the submit script. It creates the Condor jobs based on inputs/parameters, and a DAGMan description. Then the run is submitted to Condor for execution.
</li></ul> 
<p />
<p />
Let's start a run with the default parameters:
<p />
<pre class="screen">
$ <b>./submit</b>
Run id is 2009-10-21_202324
Generating job 1 X: 1 {1:200}, Y: 1 {1:150}
....
Generating job 16 X: 4 {601:800}, Y: 4 {451:600}

Checking all your submit files for log file names.
This might take a while...
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor           : master.dag.condor.sub
Log of DAGMan debugging messages                 : master.dag.dagman.out
Log of Condor library output                     : master.dag.lib.out
Log of Condor library error messages             : master.dag.lib.err
Log of the life of condor_dagman itself          : master.dag.dagman.log

Condor Log file for all jobs of this DAG         : runs/2009-10-20_135040/alljobs.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 849162.
-----------------------------------------------------------------------

</pre>
<p />
What was submitted was one DAGMan which will manager 16 tile jobs.
<p />
<p />
<h3><a name="Checking_on_the_Jobs"></a> Checking on the Jobs </h3>
<p />
We can check on jobs in the queue using the <strong>condor_grid_overview</strong> command:
<p />
<pre class="screen">
$ <b> condor_grid_overview</b>
ID         DAG              Owner        Resource              Status      Command       TimeInState
========== ================ ============ ===================== =========== ============= ===========
703        (DAGMan)         train01                            Running     condor_dagman     0:03:44
704          |-job_1        train01      UCHC_CBG              Stage out   remote-povray-    0:00:30
705          |-job_2        train01      LIGO_UWM_NEMO         Pending     remote-povray-    0:03:30
706          |-job_3        train01      Clemson-ciTeam        Stage out   remote-povray-    0:02:46
708          |-job_5        train01      NYSGRID_CORNELL_NYS1  Stage out   remote-povray-    0:00:31
709          |-job_6        train01      Firefly               Stage out   remote-povray-    0:00:31
710          |-job_8        train01      Firefly               Stage out   remote-povray-    0:01:28
711          |-job_10       train01      NYSGRID-CCR-U2        Running     remote-povray-    0:00:31
712          |-job_12       train01      Nebraska              Pending     remote-povray-    0:03:26
713          |-job_7        train01      UCHC_CBG              Stage out   remote-povray-    0:00:31
714          |-job_9        train01      LIGO_UWM_NEMO         Pending     remote-povray-    0:03:21
715          |-job_11       train01      Clemson-ciTeam        Stage out   remote-povray-    0:00:31
717          |-job_14       train01      NYSGRID_CORNELL_NYS1  Stage out   remote-povray-    0:00:31
718          |-job_15       train01      Firefly               Stage out   remote-povray-    0:00:31
720          |-job_4        train01      Firefly               Submitting  remote-povray-    0:00:25
721          |-job_13       train01      Clemson-ciTeam        Submitting  remote-povray-    0:00:25

Site                      Total  Subm Stage  Pend  Run  Other  Rank Succes
========================= ===== ===== ===== ===== ===== ===== ===== ======
AGLT2                         0     0     0     0     0     0   500   100%
BNL-ATLAS                     0     0     0     0     0     0   500   100%
BNL-ATLAS                     0     0     0     0     0     0   500   100%
CIT_CMS_T2                    0     0     0     0     0     0   500   100%
CIT_CMS_T2                    0     0     0     0     0     0   500   100%
CIT_CMS_T2                    0     0     0     0     0     0   500   100%
Clemson-ciTeam                3     1     2     0     0     0   947   100%
Duke                          0     0     0     0     0     0   500   100%
Firefly                       2     0     2     0     0     0   200   100%
Firefly                       2     1     1     0     0     0   950   100%
LIGO_UWM_NEMO                 2     0     0     2     0     0   200   100%
NYSGRID-CCR-U2                1     0     0     0     1     0   168    84%
NYSGRID_CORNELL_NYS1          2     0     2     0     0     0   200   100%
Nebraska                      1     0     0     1     0     0   200   100%
RENCI-Engagement              0     0     0     0     0     0     0     0%
SBGrid-Harvard-East           0     0     0     0     0     0   500   100%
SPRACE                        0     0     0     0     0     0   500   100%
UCHC_CBG                      2     0     2     0     0     0   200   100%
UFlorida-HPC                  0     0     0     0     0     0   200   100%
UJ-OSG                        0     0     0     0     0     0   500   100%
WQCG-Harvard-OSG              0     0     0     0     0     0   500   100%

16 jobs; 0 matching, 3 pending remotely, 2 running, 0 held, 11 other
</pre>
<p />
The first section of the output lists jobs. The second section lists compute sites. Interesting notes:
<p /> <ul>
<li> <strong>Where does the site information come from?</strong> - The site information comes from <a href="https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/ResourceSelection/WebHome" target="_top">ReSS</a> which is one of the site information feeds in OSG. To see what the full Condor <span class="twikiNewLink">ClassAd<a href="/bin/edit/Education/ClassAd?topicparent=Education.OSGMMExercises" rel="nofollow" title="ClassAd (this topic does not yet exist; you can create it)">?</a></span> for a site looks like, try this command: <strong>condor_status -l RENCI-Trash/Engagement_belhaven-1.renci.org_jobmanager-condor</strong>  Information is advertised directly from the sites, and is less than 5 minutes old. Note that these are the set of sites which accepts OSGEDU VO jobs. The total number of sites on OSG is close to 100.
</li></ul> 
<p /> <ul>
<li> <strong>Multiple Sites with the same name</strong> - Some sites have multiple job gateways to spread the load out. The site entries in <strong>condor_grid_overview</strong> are one entry per gatekeeper.
</li></ul> 
<p /> <ul>
<li> <strong>Rank</strong> - Rank is used by Condor during the match making step. If a job matches multiple sites, the one with the highest rank will win. OSGMM continuously assigns and updates rank between 0 and 1000 depending on how well the sites is doing (running jobs, job success rates, etc). Ranks below 300 means that no more jobs will be sent to the site. If you keep on running <strong>condor_grid_overview</strong> you will see the rank column change.
</li></ul> 
<p /> <ul>
<li> <strong>Why do the jobs not go to certain sites?</strong> - The jobs depend on Povray being installed at the sites, and have that expressed as a requirement in the Condor job descriptions. Povray has not been built on all sites, and the jobs will not match those sites. 
</li></ul> 
<p /> <ul>
<li> <strong>Jobs timing out</strong> - the jobs are set up to time out after 10 minutes in being stuck pending. The result is that if a site is busy and not running the job, it will move to another site. Held state means that the job is being removed from the site, and will be resubmitted somewhere else.
</li></ul> 
<p />
<p />
<h3><a name="Run_Directory"></a> Run Directory </h3>
<p />
Outputs will appear in runs/{timestamp}, where the timestamp will be different for each submit. For example:
<p />
<pre class="screen">
$ <b>cd runs/2010-*/</b>
$ <b>ls</b>
10.condor          1.outputs.tar.gz  9.condor
10.outputs.tar.gz  2.condor          9.outputs.tar.gz
11.condor          2.outputs.tar.gz  alljobs.log
11.outputs.tar.gz  3.condor          jobkey.txt
12.condor          3.outputs.tar.gz  logs
12.outputs.tar.gz  4.condor          master.dag
13.condor          4.outputs.tar.gz  master.dag.condor.sub
13.outputs.tar.gz  5.condor          master.dag.dagman.log
14.condor          5.outputs.tar.gz  master.dag.dagman.out
14.outputs.tar.gz  6.condor          master.dag.lib.err
15.condor          6.outputs.tar.gz  master.dag.lib.out
15.outputs.tar.gz  7.condor          outputs
16.condor          7.outputs.tar.gz  parameters.txt
16.outputs.tar.gz  8.condor          tiles-combine.log
1.condor           8.outputs.tar.gz
</pre>
<p />
In here you will find:
<p /> <ul>
<li> <strong>DAGMan description and logs</strong> - master.dag describes what jobs are part of the DAG.
</li></ul> 
<p /> <ul>
<li> <strong>Condor</strong> - The Condor submit files are the NN.condor files. They look complicated, but once you have this set up, you can submit and let the system handle job failures. Also note the "requirements" string which states that we need Povray.
</li></ul> 
<p /> <ul>
<li> <strong>logs</strong> - Contains outputs from the jobs. Also note alljobs.log where all the jobs log to the same file - this is so that DAGMan only have to check one file to know what is happening with the jobs belonging to the DAG.
</li></ul> 
<p /> <ul>
<li> <strong>outputs</strong> - This is where the povray outputs go, which we then tile together to the final image.
</li></ul> 
<p />
<p />
 Each tile is added into the final image. During the run, tiles will come back probably out of order. Example:
<p />
<img src="/twiki/pub/Education/OSGMMExercises/output-tiles.png" alt="output-tiles.png" width='400' height='300' />
<p />
Once the run is done, you can cd into the outputs/ directory. In there you should see the tiles in ppm format.
<p />
<pre class="screen">
$ ls *.ppm 
result_1_1.ppm  result_2_1.ppm  result_3_1.ppm  result_4_1.ppm
result_1_2.ppm  result_2_2.ppm  result_3_2.ppm  result_4_2.ppm
result_1_3.ppm  result_2_3.ppm  result_3_3.ppm  result_4_3.ppm
</pre>
<p />
You can create the final image by running the following command from the <strong>run directory</strong> (the one named with a timestamp):
<p />
<pre class="screen">
$ cd ..
$ pwd
/home/rynge/osgmm-exercises/povray/runs/2010-12-07_092806
$ ../../helpers/tiles-combine
$ ls -l outputs/output.png 
-rw-r--r-- 1 rynge guest 5052706 Dec  7 11:23 outputs/output.png
</pre>
<p />
<p />
You can start another run by running "./submit" again. You can also change number of tiles by changing <strong>CHUNKSIZE_X</strong> and <strong>CHUNKSIZE_Y</strong> (try 200 and 150) on the top of the <strong>submit</strong> script. But don't make them to small. There is an overhead (staging, scheduling at multiple levels, ...) running jobs on the grid, so making the jobs to short will just make the overall runtime longer. For production jobs, we usually try to create jobs that are 4-24 hours long, which sometimes mean grouping many task into jobs.
<p />
<h3><a name="More_information"></a> More information </h3>
<p />
OSGMM:
<a href="http://osgmm.sourceforge.net/" target="_top">http://osgmm.sourceforge.net/</a>
<p />
Povray:
<a href="http://en.wikipedia.org/wiki/POV-Ray" target="_top">http://en.wikipedia.org/wiki/POV-Ray</a>
<p />
Bonsai Povray scene:
<a href="http://www.zazzle.com/bonsais_poster-228003235318786172" target="_top">http://www.zazzle.com/bonsais_poster-228003235318786172</a>
<p />
<p />
<p />
<p />
-- <a href="/bin/view/Main/MatsRynge" class="twikiLink">MatsRynge</a> - 03 Dec 2010</div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Education<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span><a href="/bin/view/Education/GridWorkshops" class="twikiLink">GridWorkshops</a> &gt; <a href="/bin/view/Education/Brazil2011" class="twikiLink">Brazil2011</a><span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>OSGMMExercises</span> <br />    
Topic revision: r6 - 12 Oct 2016 - 15:22:32 - <a href="/bin/view/Main/KyleGross" class="twikiLink">KyleGross</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
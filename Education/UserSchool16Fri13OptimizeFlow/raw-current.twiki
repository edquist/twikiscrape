
*2) Test the HTC optimization of the _permutation_ step.*  You will need to determine the number of permutations that could be batched in a single job process, if each process needs to run in ~30 minutes for good HTC scaling. To do this, you will need to run some test jobs, to see how long it takes a single job process to run, say, 10, 100, or 1000 permutations.  

- Running test jobs: 
   1. modify one of the permutation submit files to “queue” 10 processes (so that you can average time between the 10 test processes)
   1. add &quot;request_cpus = 1&quot; according to Joe&#39;s indication
   1. add reasonable first guesses for &quot;request_memory&quot; and &quot;request_disk&quot; (say, 1 GB?). 
   1. Make a few copies of this submit file so that you can change the last argument (the number of permutations) from &quot;10000&quot; to &quot;10&quot;, &quot;100&quot;, or &quot;1000&quot;. For time&#39;s sake, a member of your group should test all three of these variations at the same time! 

- Getting ready for the next step: 
   1. After each set of _permutation_ tests finishes, you’ll need to use =tarit.sh= (with the correct argument!) before running the test jobs for the QTL step.

*3) Test each of the _QTL mapping_ jobs* by submitting the three submit files after simply adding lines for &quot;request_memory&quot;, &quot;request_disk&quot;, and &quot;request_cpus&quot;. The resource needs (RAM and disk space) and execution time of each _QTL mapping_ job will likely increase with the total number of permutations from the previous _permutation_ step, though the execution time will likely still be short (according to Joe). You&#39;ll test optimized _permutation_ and _QTL mapping_ steps later on.

*4) Optimization planning:* In order to optimize Joe&#39;s workflow, we can 
 
- Permutation throughput:
Calculate the number of permutations that should be run _per job process_, such that the runtimes per process will be about 30 minutes (not exact, but closer to 30 than to 5 or 60). You can then calculate the number of processes that should be queued such that 100,000 permutations are calculated for each trait, so you want _job processes_ X _permutations_ to equal 100,000 total permutations for each of the three phenotype traits.
*Hint: You can use the &quot;condor_history&quot; feature (similar to condor_q, but for completed jobs) to easily view and compare the &quot;RUNTIME&quot; for jobs in a &quot;cluster&quot; (using the cluster value as an argument to =condor_history=.

   *Memory and disk or both steps:
Make sure to examine the log files of your _permutation_ and _QTL_ test jobs, so that you can extrapolate how much memory and disk should be requested in the submit files for the full-scale DAG.


---+++*When you&#39;re done with #4, move on to [[UserSchool16Fri13ExecFlow][Exercise 1.3]]*

%LINKCSS%

---+!! Portable Batch System (PBS) Setup for OSG
%TOC%



The &lt;firstterm&gt;Portable Batch System (PBS)&lt;/firstterm&gt; is a popular networked subsystem for submitting, monitoring, and controlling a work load of batch jobs on one or more systems. 

In this document, I will specifically describe deployment and configuration of Torque and its FIFO scheduler as the batch system backing an an OSG computing element.  The configuration steps outlined below, however, should apply nearly equally well to &lt;nop&gt;OpenPBS or PBS Pro.


---++ Flavors of PBS
PBS has a long history, and is currently available in three flavors: 
   * _&lt;nop&gt;OpenPBS_: the original PBS developed for NASA in the early to mid-1990s, available as open source
   * _PBS Pro_: a commercial version of PBS from Altair Engineering
   * _Torque_: the open source successor to &lt;nop&gt;OpenPBS

&lt;nop&gt;OpenPBS still works, and you can use it, but you should be aware that the focus of the open source development effort has moved on to Torque for some time now. If you want to run &lt;nop&gt;OpenPBS and support is important to you, you may purchase it from [[http://www.altair.com/][Altair Engineering]].  Personally, I am not aware of a technical argument to prefer &lt;nop&gt;OpenPBS to Torque.

&lt;firstterm&gt;PBS Pro&lt;/firstterm&gt; is a fine product. It is reasonably priced compared to competing commercial batch systems and has dedicated support from Altair Engineering should you desire it.  We use it where I work, and I have no serious complaints.  The most recent releases of PBS Pro (versions 7.x and later) have features that &lt;nop&gt;OpenPBS and Torque do not, as well as a more flexible resource specification than its open source counterparts.  If I recall correctly, you should be able to get a trial version of PBS Pro before you buy - contact Altair for info.  Prior to versions 7.x, you could plug in the MAUI scheduler in place of the FIFO scheduler that is included with PBS Pro.  With versions 7.x and later, this no longer seems to work.

As mentioned previously, Torque is the open source PBS project which in active development.  In its current 2.x versions, it has also matured into a quality product and should be more than capable of scaling to typical Trash/Tier3 cluster sizes. In addition, you have the option of using the more flexible and open source MAUI scheduler in place of the FIFO scheduler included with Torque.


---++Obtaining Torque

[[http://www.clusterresources.com/pages/products/torque-resource-manager.php][Torque]] is distributed as source files packaged as a tarball that you unpack and build yourself. You can download the Torque tarball from the [[http://clusterresources.com/downloads/torque][Cluster Resources site]]. Select the most recent version and save it to a file, (At the time of this writing, version 2.1.7 is the latest release). Copy the tarball to the node you intend to use as your OSG computing element headnode.

%STARTInsetBox% 
---++ Using RPMs to install Torque
If your head node runs an RPM-based Linux distribution and you don&#39;t want to build Torque yourself, you can skip the above step and the &quot;Building Torque&quot; section and use some pre-built RPM packages I have prepared.  They should work on any RHEL4-compatible machine and _may_ also work on other distributions: 
   * [[http://lorien.phys.ufl.edu/~prescott/centos/4.3/contrib/i386/][i386 RPMs]]
   * [[http://lorien.phys.ufl.edu/~prescott/centos/4.4/contrib/x86_64/][x86_64 RPMs]]
As implied by the URLs, I built the i386 packages on a &lt;nop&gt;CentOS 4.3 machine, and the x86_64 packages on &lt;nop&gt;CentOS 4.4.  Download the packages appropriate for your architecture, copy them to your head node, and skip to &quot;Installing Torque on the Head Node&quot;.

%ENDInsetBox%

---++Building Torque

We have a couple of options in ways to build Torque: you can either build binaries as one normally does from a tarball, or you can build RPMs.  

%NOTE% In my opinion, if you are running a Red Hat/RPM-based Linux distribution, you should build the RPMs. Having RPMs makes deploying Torque components on multiple machines easy, and the RPMs can likely be integrated into a cluster management tool like Rocks very trivially.

Unpack the tarball and change directory to the top level of the source tree.  You may do this as a normal user.    

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;tar xvzf torque-2.1.7.tar.gz &lt;/userinput&gt;
$ &lt;userinput&gt;cd torque-2.1.7 &lt;/userinput&gt;
&lt;/pre&gt;

%NOTE% If you are *not* going to make RPMs, you will find it convenient later to do this in an NFS-shared area.  If you are going to make RPMs, it doesn&#39;t matter where you do this.

You may wish to glance at the =README.torque= and =README.configure= files at this point. If you have built open source software before, you may also wish to examine the myriad of configuration options of the build: 

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;./configure --help &lt;/userinput&gt;
&lt;/pre&gt;

But you don&#39;t have to if you aren&#39;t too curious!

---+++ Some words about build dependencies
You will need a basic development environment installed in order to build Torque.  The GNU compiler collection is just what the doctor ordered; you need the C, C++, and Fortran 77 compilers installed to build everything.  Specifically, make sure you have the following installed: 
   * =gcc=
   * =g++
   * =g77=

You should have SSH clients installed (does anyone use =rsh= anymore?). The configuration step prior to compilation will check to see if =scp= is available. If so, it will be used as the &quot;remote copy program&quot; used to relay job stdout and stderr back to the user from the compute nodes.  

Torque comes with Tcl/Tk-based GUI programs monitor jobs and batch system status. To build these, you will need a Tcl/Tk development environment, as well.

On an Red Hat-based machine, everything will be built if you have these packages (and their associated dependencies) installed:
   * make
   * gcc
   * gcc-g++
   * gcc-g77
   * gawk
   * glibc-devel
   * bison
   * flex
   * groff
   * openssh-clients
   * tcl
   * tcl-devel
   * tclx
   * tclx-devel
   * tk
   * tk-devel
   * xorg-x11-xauth

More than likely, you have most of these installed already.


---+++ Configuring the source tree
Now we configure the source tree to prepare for building the package.  I want the installation path to be =/usr=. You can control it with the &lt;code&gt;--prefix=...&lt;/code&gt; option:

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;./configure --prefix=/usr &lt;/userinput&gt;
&lt;/pre&gt;

%NOTE% If you are building on a 64-bit OS with the above command, you should to add the option &lt;code&gt;--libdir=/usr/lib64&lt;/code&gt;.  If you don&#39;t do this, the applications linked against Torque shared libraries (such as MPI apps) may need help to find them at run time.

A bunch of test output will spew forth.  Ultimately, this step generates all the &lt;em&gt;Makefiles&lt;/em&gt; needed to build the package.  

Next, make:

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;make&lt;/userinput&gt;
&lt;/pre&gt;

This will simply build binaries.  

If we are on a Red Hat-based machine, you should run =make= *as root* with the =rpm= option:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;make rpm &lt;/userinput&gt;
&lt;/pre&gt;


This will build binary RPM packages.  

I am on a Red Hat-based machine, so I use =make rpm=.  

On any modern machine, this will take a few minutes at most. In the end, I am left with the following files:

&lt;pre class=&quot;programlisting&quot;&gt;
/usr/src/redhat/RPMS/i386/torque-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-debuginfo-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-docs-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-scheduler-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-server-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-mom-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-client-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-gui-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-localhost-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-devel-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-pam-2.1.7-1cri.i386.rpm
&lt;/pre&gt;

---++Installing Torque on the Head Node

Now we are ready to deploy some packages on our head node.  If you didn&#39;t create RPMs in the steps above, you need to *become root* on the head node and type =make install= from the top level of the Torque source tree.  Everything will be installed into =/usr/local= unless you specified an alternate installation path at =configure= time.

On the other hand, if you have RPMs read on.  On our head node, we at least want to install the Torque server and scheduler, and have commands available to submit, monitor, and remove jobs.  If you want your head node to be able to run batch jobs, you will also need to install the &quot;MOM&quot; package here.  You may also like to install the documentation and GUI tools.  To do all this, *become root*, go to the directory where the RPMs reside, and type:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;rpm -Uvh torque-server-2.1.7-1cri.i386.rpm &amp;#92;
    torque-scheduler-2.1.7-1cri.i386.rpm  &amp;#92;
    torque-mom-2.1.7-1cri.i386.rpm   &amp;#92;
    torque-client-2.1.7-1cri.i386.rpm  &amp;#92;
    torque-docs-2.1.7-1cri.i386.rpm  &amp;#92;
    torque-gui-2.1.7-1cri.i386.rpm  &amp;#92;
    torque-devel-2.1.7-1cri.i386.rpm  &amp;#92;
    torque-2.1.7-1cri.i386.rpm &lt;/userinput&gt;
&lt;/pre&gt;

---++ Installing Torque on the Compute Nodes

On the compute nodes, we need to be able to run jobs.  As an administrator, I also find it occasionally convenient to query the batch system from compute nodes, so installing client tools would be handy, too.

If you *don&#39;t* have RPMs, *become root* on one of your compute nodes, go to the top level directory of your NFS-shared Torque source tree, and type =make install=.  Repeat this step for every compute node you want to add to the batch system.

If you *do* have RPMs, go to the directory where they reside (copy them over if you need to), and say:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;rpm -Uvh torque-mom-2.1.7-1cri.i386.rpm  &amp;#92;
   torque-client-2.1.7-1cri.i386.rpm  &amp;#92;
   torque-2.1.7-1cri.i386.rpm &lt;/userinput&gt;
&lt;/pre&gt;

---++ Configuring SSH for PBS

The above Torque packages will use =ssh= for process transport and =scp= for copying of output files.  Torque must be able to use these tools with password-less authentication between the head node and the compute nodes.  There are two means of accomplishing this with &lt;nop&gt;OpenSSH: hostbased authentication, and user public key authentication.  While using individual user public keys certainly works, it is a pain to manage in my opinion, and error prone for any local users.  So in this section. I will describe how to set up hostbased authentication for &lt;nop&gt;OpenSSH in order to satisfy PBS&#39;s requirements.

Let&#39;s start on the head node.  First, create the =/etc/hosts.equiv= file and populate it with fully qualified domain names corresponding to your head node&#39;s LAN interface and all your compute nodes:

&lt;pre class=&quot;programlisting&quot;&gt;
headnode.internal.domain
computenode1.internal.domain
computenode2.internal.domain
...
&lt;/pre&gt;

In the =/etc/ssh/ssh_config= file, make sure the following are on so ssh clients can try hostbased authentications:

&lt;pre class=&quot;programlisting&quot;&gt;
HostbasedAuthentication yes
EnableSSHKeysign yes
&lt;/pre&gt;

In the =/etc/ssh/sshd_config=, we want the server to permit hostbased authentication attempts (only from hosts in our =/etc/hosts.equiv=):

&lt;pre class=&quot;programlisting&quot;&gt;
HostbasedAuthentication yes
IgnoreRhosts yes
IgnoreUserKnownHosts yes
&lt;/pre&gt;

Hostbased authentications will succeed for hosts whose public keys are in the =/etc/ssh/ssh_known_hosts= file.  So you will have to collect the keys for every host you intend to put in the batch system.  One way to to that is to use the =hosts.equiv= file you just populated:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;ssh-keyscan -t rsa,rsa1,dsa -f /etc/hosts.equiv &gt; /etc/ssh/ssh_known_hosts &lt;/userinput&gt;
&lt;/pre&gt;

Now restart sshd with =/etc/init.d/sshd restart=.  Copy the following files to each of your compute nodes and restart sshd on each:
   * =/etc/ssh/ssh_config=
   * =/etc/ssh/sshd_config=
   * =/etc/ssh/ssh_known_hosts=
   * =/etc/hosts.equiv=

You should now be able to authenticate as a normal user between hosts across your cluster without having to enter a password.

---++ Configuring the Head Node 

Here, we will initialize the server and create some queues.  You will need to do these steps *as root*.  My recommendation is to create a routing queue which can submit jobs to one or more execution queues, where the jobs will actually run.  The presence of the routing queue will allow some flexibility in the case you wish to have multiple execution queues.
In the &quot;usual&quot; case, where a dual-homed head node has a public interface exposed to the WAN and a private interface dedicated for LAN traffic, it makes sense to configure Torque to use the LAN interface.  I will assume this is the case.  

There are three PBS services currently deployed and enabled on your head node:
   * =pbs_server= runs the show, so to speak &amp;mdash; it instanciates the batch system on your cluster.
   * =pbs_sched= is the scheduler &amp;mdash; it makes the decisions of which jobs to run.
   * =pbs_mom= actually runs the jobs. (&lt;firstterm&gt;MOM: Machine Oriented Mini-server&lt;/firstterm&gt;) 

You will need to run the server and scheduler services.  In my experience, MOM is not usually run on the head node, as head nodes usually have plenty to do already.  They typically offer lots of key cluster infrastructure services like NFS, DNS, LDAP/NIS, etcetera; allowing compute and memory-intensive jobs to run on the head node can have adverse effects upon the entire cluster.  So feel free to simply turn MOM off on the head node:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;chkconfig pbs_mom off&lt;/userinput&gt;
&lt;/pre&gt;

If you wish to allow your head node to run jobs, you should make sure the =/var/spool/torque/server_name= file contains the hostname corresponding the LAN address of your head node.

The Torque configuration files live underneath =/var/spool/torque= by default.  On the head node, you will see the following files and subdirectories underneath =/var/spool/torque=:

&lt;pre class=&quot;screen&quot;&gt;
aux/         mom_logs/  pbs_environment  sched_priv/   server_name   spool/
checkpoint/  mom_priv/  sched_logs/      server_logs/  server_priv/  undelivered/
&lt;/pre&gt;

In the configuration steps which follow, we will be poking around this area on the head and compute nodes.

---+++ PBS Server

We can initialize the batch system thusly:

&lt;verbatim&gt;
pbs_server -t create [-h &lt;hostname&gt;] [-S &lt;hostname&gt;] [-M &lt;hostname&gt;]
&lt;/verbatim&gt;

The =-h &amp;lt;hostname&amp;gt;=, =-M &amp;lt;hostname&amp;gt;= and =-S &amp;lt;hostname&amp;gt;= options are only useful if the hostname corresponding to your head node&#39;s LAN address is *not* the same as the output of the =hostname= command; these options tell the server to use the supplied =&amp;lt;hostname&amp;gt;= for server, scheduler, and MOM communications, respectively.  In this case, you will also want to create =/etc/sysconfig/pbs_server= with the following contents:

&lt;pre class=&quot;programlisting&quot;&gt;
PBS_DAEMON=&quot;/usr/sbin/pbs_server -h &lt;replaceable&gt;&amp;lt;hostname&amp;gt;&lt;/replaceable&gt; -S &lt;replaceable&gt;&amp;lt;hostname&amp;gt;&lt;/replaceable&gt; -M &lt;replaceable&gt;&amp;lt;hostname&amp;gt;&lt;/replaceable&gt;&quot;
&lt;/pre&gt;

where =&amp;lt;hostname&amp;gt;= is the same hostname you used when you created the server instance.

We can now configure the server.  This is done with the =qmgr= command.  If =qmgr= is executed without any options, it will put you in an interactive shell from which you can just type in PBS commands.  But you can also feed commands to =qmgr= with the =-c= option.  Let&#39;s turn on scheduling, create a routing queue and an execution queue, and take care of some defaults:

&lt;verbatim&gt;
   * =qmgr -c &#39;set server scheduling=true&#39;=
   * =qmgr -c &#39;create queue defaultq&#39;=
   * =qmgr -c &#39;set queue defaultq queue_type = route&#39;=
   * =qmgr -c &#39;create queue batchq&#39;=
   * =qmgr -c &#39;set queue batchq queue_type = execution&#39;=
   * =qmgr -c &#39;set queue defaultq started = true&#39;=
   * =qmgr -c &#39;set queue defaultq route_destinations = batchq&#39;=
   * =qmgr -c &#39;set queue defaultq enabled = true&#39;=
   * =qmgr -c &#39;set server default_queue = defaultq&#39;=
   * =qmgr -c &#39;set queue batchq started = true&#39;=
   * =qmgr -c &#39;set queue batchq enabled = true&#39;=
   * =qmgr -c &#39;set server resources_default.ncpus = 1&#39;=
   * =qmgr -c &#39;set server resources_default.nodes = 1&#39;=
&lt;/verbatim&gt;

Once this is done, you can restart the =pbs_server= from the =init.d= script:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;/etc/init.d/pbs_server restart &lt;/userinput&gt;
&lt;/pre&gt;

---+++ PBS Scheduler

The scheduler configuration file can be found at =/var/spool/torque/sched_priv/sched_config=.  It is documented.  There is no need to change any of the values at this time, though.  If you ever do change them, be sure to restart the scheduler.

If you wish to bind the scheduler to an interface that resolves to a hostname other than the output of the =hostname=, as you may have  create the file =/etc/sysconfig/pbs_sched= with the following contents:

&lt;pre class=&quot;programlisting&quot;&gt;
PBS_DAEMON=&quot;/bin/sh -c &#39;h=`hostname`; hostname &amp;lt;hostname&gt; ; /usr/sbin/pbs_sched; hostname \$h&#39;&quot;
&lt;/pre&gt;

where =&amp;lt;hostname&amp;gt;= is the same hostname you used for the =pbs_server= setup.  This is a dirty trick.  But the fact is that the built-in Torque scheduler will only listen on the interface that corresponds to the output of =gethostname=, so the gloves may have to come off.

Start the scheduler *as root*:
&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;/etc/init.d/pbs_sched start &lt;/userinput&gt;.
&lt;/pre&gt;

The scheduler logs will go to =/var/spool/torque/sched_logs/&amp;lt;yyyymmdd&amp;gt;=; each day will have its own scheduler log.

---++ Configuring the Compute Nodes

As mentioned above, =/var/spool/torque/server_name= on each compute node needs to contain the correct value for the PBS server hostname.  Additional MOM configuration directives can be entered into =/var/spool/torque/mom_priv/config=.  For now, we will not put anything into the MOM config file, but we do need to create it or the MOM startup will complain.  

*As root*, do the following:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;touch /var/spool/torque/mom_priv/config &lt;/userinput&gt;
[root@hostname ~]$ &lt;userinput&gt;/etc/init.d/pbs_mom start &lt;/userinput&gt;
&lt;/pre&gt;

The MOM logs will go to =/var/spool/torque/mom_logs/&amp;lt;yyyymmdd&amp;gt;= (one log file per day).

---++ Adding Compute Nodes to PBS

Now we are ready to add compute nodes to the batch system.  Go back to the head node, and *as root* add compute nodes via =qmgr=:

&lt;pre class=&quot;screen&quot;&gt;
[root@hostname ~]$ &lt;userinput&gt;qmgr -c &#39;create node &amp;lt;fqdn&gt; np=&amp;lt;ncpus&gt;&#39; &lt;/userinput&gt;
&lt;/pre&gt;

where =&amp;lt;fqdn&amp;gt;= is the fully qualified domain name of your compute node and =&amp;lt;ncpus&gt;= is the number of processors it has.  Do this for each compute node you want to add to PBS.

---++ Testing the Installation
---+++ Query the Server and Queue Configuration

The following command will dump the server and queue configuration to stdout:

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;qmgr -c &#39;print server&#39; &lt;/userinput&gt;
&lt;/pre&gt;

You should see something similar to:

&lt;pre class=&quot;programlisting&quot;&gt;
#
# Create queues and set their attributes.
#
#
# Create and define queue defaultq
#
create queue defaultq
set queue defaultq queue_type = Route
set queue defaultq route_destinations = batchq
set queue defaultq enabled = True
set queue defaultq started = True
#
# Create and define queue batchq
#
create queue batchq
set queue batchq queue_type = Execution
set queue batchq enabled = True
set queue batchq started = True
#
# Set server attributes.
#
set server scheduling = True
set server default_queue = defaultq
set server log_events = 511
set server mail_from = adm
set server resources_default.ncpus = 1
set server resources_default.nodes = 1
set server scheduler_iteration = 600
set server node_check_rate = 150
set server tcp_timeout = 6
set server pbs_version = 2.1.7
&lt;/pre&gt;

---+++ Querying Node Status

You can query node status with the =pbsnodes= command.  =pbsnodes -a= will show node status for all compute nodes in the batch system.  You should see something like the following for each node in your batch system:

&lt;pre class=&quot;screen&quot;&gt;&gt;
invigo.local
     state = free
     np = 4
     ntype = cluster
     status = opsys=linux,uname=Linux invigo.local 2.6.9-34.ELsmp #1 SMP 
                  Wed Mar 8 00:27:03 CST 2006 i686,sessions=? 0,nsessions=? 0, 
                  nusers=0,idletime=535854,totmem=5223572kb,availmem=5036844kb, 
                  physmem=2074840kb,ncpus=4,loadave=0.00,netload=3206850556, 
                  state=free,jobs=? 0,rectime=1172691859
&lt;/pre&gt;

The state *free* means that the machine has capacity to run jobs, and *np* signifies the number of processors offerered.  

---+++ Creating and Submitting a Job

A PBS job is just a script - typically, a shell script.  In each job script, you can tell things to PBS with special shell comments.  As a *normal user*, fire up your favorite editor and let&#39;s create a job script with the following contents:

&lt;pre class=&quot;programlisting&quot;&gt;
#! /bin/sh
#PBS -N testjob
#PBS -o testjob.out
#PBS -e testjob.err
#PBS -M &lt;replaceable&gt;&amp;lt;your_email_here&gt;&lt;/replaceable&gt;
#PBS -l walltime=00:01:00

date
hostname
sleep 20
date
&lt;/pre&gt;

I saved the above into a file called =testjob.job=.  As you can see, the job just runs the =date= command, the =hostname= command, sleeps for 20 seconds, and runs the =date= command again.  The funny =#PBS= comments at the top of the script are directives for PBS.  =#PBS -N= sets the job name.  =#PBS -o= sets the job =stdout= file, and =#PBS -e= option sets filename for the =stderr= output.  The =#PBS -M= directive sets the email address to use for job summary reports.  Finally, the =#PBS -l walltime=00:01:00= is a resource request that asks PBS for one minute of walltime for the job.  There are many types of resource requests you can make - the job will be killed by PBS if it does not complete before that resource is exhausted.

You use the =qsub= command to submit the job to the batch system; just give qsub the name of your job script, like so:

&lt;verbatim&gt;
qsub testjob.job
&lt;/verbatim&gt;

The PBS id of the job will return to you on standard output.

---+++ Querying Job and Queue Status

The =qstat= program is used to query jobs and queue status.  If we just type =qstat=, we will see the job we just submitted (submit it again if it is already finished):

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;qstat&lt;/userinput&gt;
Job id              Name             User            Time Use S Queue
------------------- ---------------- --------------- -------- - -----
0.osg               testjob          prescott               0 R batchq
&lt;/pre&gt;

We see its full job ID, job name, user who submitted the job, how much time it has used, the state of the job (=R= means &quot;running&quot;) and what queue it is running in.  In our setup, the job was routed by the routing queue into the &quot;batchq&quot; execution queue, where it currently runs.

We can get oodles of information about a job with =qstat -f &amp;lt;job_id&amp;gt;=:

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;qstat -f 0&lt;/userinput&gt;
  Job Id: 0.osg.local
    Job_Name = testjob
    Job_Owner = prescott@osg.local
    job_state = R
    queue = batchq
    server = osg.local
    Checkpoint = u
    ctime = Wed Feb 28 20:37:21 2007
    Error_Path = osg.hpc.ufl.edu:/home/prescott/testjob.err
    exec_host = invigo.local/0
    Hold_Types = n
    Join_Path = n
    Keep_Files = n
    Mail_Points = a
    Mail_Users = prescott@hpc.ufl.edu
    mtime = Wed Feb 28 20:37:21 2007
    Output_Path = osg.hpc.ufl.edu:/home/prescott/testjob.out
    Priority = 0
    qtime = Wed Feb 28 20:37:21 2007
    Rerunable = True
    Resource_List.ncpus = 1
    Resource_List.nodect = 1
    Resource_List.nodes = 1
    Resource_List.walltime = 00:01:00
    session_id = 1325
    Variable_List = PBS_O_HOME=/home/prescott,PBS_O_LANG=en_US.UTF-8,
        PBS_O_LOGNAME=prescott,
        PBS_O_PATH=/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/usr/X11R6/
        bin,PBS_O_MAIL=/var/spool/mail/prescott,PBS_O_SHELL=/bin/bash,
        PBS_O_HOST=osg.hpc.ufl.edu,PBS_O_WORKDIR=/home/prescott,
        PBS_O_QUEUE=defaultq
    comment = Job started on Wed Feb 28 at 20:37
    etime = Wed Feb 28 20:37:21 2007
&lt;/pre&gt;

---+++ Job Output and Summary

Our example job executed a few simple programs that wrote to stdout.  In our job script, we specified the filename that contains our job&#39;s standard output as =testjob.out=.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;cat testjob.out&lt;/userinput&gt;
Wed Feb 28 20:37:21 EST 2007 
invigo.local
Wed Feb 28 20:37:41 EST 2007
&lt;/pre&gt;

We also can see that there was no standard error output from the job; our standard error output file has zero length.  Finally, we asked PBS to send us an email with a job summary when our job completed &amp;mdash; if you set up mail delivery, it should be waiting for you.

---+++ Job Post-Mortem and Accounting

Torque&#39;s accouting logs are located in =/var/spool/torque/server_priv/accounting=; there is one file per day with filenames just like for the PBS server, scheduler, and MOM.   This is the definitive source of accounting info from PBS.  You can use the [[http://pbsaccounting.sourceforge.net/][pbsaccounting package]] to generate reports and graphs from these logs.  There are other packages you can find to do this, as well, and it is even not too difficult to write your own accounting log processor to generate custom reports.

Sometimes, you may want to query what PBS did with an already completed job ID &amp;mdash; you can use the =tracejob= command.  It will pick through the PBS accounting logs and give you a timeline for the job in question.  For example, =tracejob 0= (=0= was the job ID for our test job) yields:

&lt;pre class=&quot;programlisting&quot;&gt;
Job: 0.osg.local

02/28/2007 17:31:43  A    queue=defaultq
02/28/2007 17:31:43  A    queue=batchq
02/28/2007 17:39:12  A    requestor=root@osg.local
02/28/2007 20:37:21  S    enqueuing into defaultq, state 1 hop 1
02/28/2007 20:37:21  S    dequeuing from defaultq, state QUEUED
02/28/2007 20:37:21  S    enqueuing into batchq, state 1 hop 1
02/28/2007 20:37:21  S    Job Queued at request of prescott@osg.local, owner =
                          prescott@osg.local, job name = testjob, queue =
                          batchq
02/28/2007 20:37:21  S    Job Modified at request of Scheduler@osg.local
02/28/2007 20:37:21  L    Job Run
02/28/2007 20:37:21  S    Job Run at request of Scheduler@osg.local
02/28/2007 20:37:21  A    queue=defaultq
02/28/2007 20:37:21  A    queue=batchq
02/28/2007 20:37:21  A    user=prescott group=hpcadmin jobname=testjob
                          queue=batchq ctime=1172713041 qtime=1172713041
                          etime=1172713041 start=1172713041
                          exec_host=invigo.local/0 Resource_List.ncpus=1
                          Resource_List.neednodes=1 Resource_List.nodect=1
                          Resource_List.nodes=1
02/28/2007 20:37:41  S    Exit_status=0 resources_used.cput=00:00:00
                          resources_used.mem=3304kb resources_used.vmem=16848kb
                          resources_used.walltime=00:00:20
02/28/2007 20:37:41  S    dequeuing from batchq, state COMPLETE
02/28/2007 20:37:41  A    user=prescott group=hpcadmin jobname=testjob
                          queue=batchq ctime=1172713041 qtime=1172713041
                          etime=1172713041 start=1172713041
                          exec_host=invigo.local/0 Resource_List.ncpus=1
                          Resource_List.neednodes=1 Resource_List.nodect=1
                          Resource_List.nodes=1 session=1325 end=1172713061
                          Exit_status=0 resources_used.cput=00:00:00
                          resources_used.mem=3304kb resources_used.vmem=16848kb
                          resources_used.walltime=00:00:20
&lt;/pre&gt;

---++ Final Words

Hopefully this note will help you get your Torque batch system up and running, and give you a bit of familiarity with the typical procedures and tools available.  If you have further questions, I highly recommend to look at the Admin Manual and numerous man pages included with the Torque packages, and to consult the =torqueuser= mailing list (or the [[http://www.supercluster.org/pipermail/torqueusers/][torqueuser list archives]]). 

Torque is highly configurable; in this short tutorial, we have only done enough to get you started. While what we&#39;ve done so far may be perfectly adequate for many environments, you should be aware that configuration options exist to:
   * add user and group ACLs
   * resource attributes handy for heterogenous environments
   * optimizations for job output relay
   * multiple execution queues with their own scheduling priorities
   * considerations for running parallel jobs
   * dropping in of powerful third party schedulers such as Maui
   * et cetera  
Good luck!

---++ Useful Links
It is always handy to have a few reference links at your fingertips, so I enclose a few here.  As always, Google is your friend and a wealth of information.

   * [[http://www.clusterresources.com/pages/products/torque-resource-manager.php][Cluster Resource&#39;s Torque page]]
   * [[http://www.clusterresources.com/wiki/doku.php?id=torque:torque_wiki][Torque Admin Manual]]
   * [[http://www.clusterresources.com/wiki/doku.php?id=torque:appendix:l_torque_quickstart_guide][Torque Quickstart Manual]]
   * [[http://www.altair.com/software/pbspro.htm][Altair PBS Pro]]
   * [[http://www.openpbs.org/which_pbs.html][&lt;nop&gt;OpenPBS versus PBS Pro - Which PBS is for you?]]
   * [[http://dcwww.camp.dtu.dk/pbs.html][&lt;nop&gt;OpenPBS Mini-HOWTO]]

%BOTTOMMATTER%
-- Main.CraigPrescott - 2007 Mar 03 %BR%
-- Main.ForrestChristian - 2007 Mar 08 %BR%


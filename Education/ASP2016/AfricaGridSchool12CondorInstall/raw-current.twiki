---+ Our Condor Installation

&lt;div style=&quot;margin-left: 1em; margin-right: 1em; border: 1px solid black; padding: 0.5em;&quot;&gt;
---+++ Objective of this exercise

This exercise should help you understand the basics of how Condor is installed, what Condor processes (a.k.a. daemons) are running, and what they do.
&lt;/div&gt;

---++ Login to the Condor submit computer
Before you start, make sure you are logged into =osg-ss-submit.chtc.wisc.edu=. You should have been given your name and password when you arrived at the school. If you don&#39;t know them, talk to Scot.

---++ Looking at our Condor installation

How do you know what version of Condor you are using? Try =condor_version=: 

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% condor_version
$CondorVersion: 7.9.0 Jul 10 2012 BuildID: 51900 $
$CondorPlatform: x86_64_rhap_5.7 $
&lt;/pre&gt;

There are two things you should note:

   1. This reports the &quot;CondorPlatform&quot;, which is the type of computer we built it on, _not_ the computer we&#39;re running on. It was built on Red Hat Enterprise Linux (rhap) 5.7, but you might notice that we&#39;re running on Scientific Linux 5.6, which is a free clone of Red Hat Enterprise Linux.

&lt;div style=&quot;margin-left: 1em; margin-right: 1em; background-color: #ffff66; border: 1px solid black; padding: 0.5em;&quot;&gt;
*Extra Tip: The OS version*

%TWISTY{%TWISTY_OPTS_MORE%}%
Do you know how to find the OS version? You can usually look in /etc/issue to find out:

&lt;pre style=&quot;margin-left:2em&quot; class=&quot;screen&quot;&gt;
% cat /etc/issue
Scientific Linux SL release 5.6 (Boron)
Kernel \r on an \m
&lt;/pre&gt;

Or you can run:
&lt;pre style=&quot;margin-left:2em&quot; class=&quot;screen&quot;&gt;
% lsb_release -a
LSB Version:	:core-4.0-amd64:core-4.0-ia32:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-ia32:printing-4.0-noarch
Distributor ID:	ScientificSL
Description:	Scientific Linux SL release 5.6 (Boron)
Release:	5.6
Codename:	Boron
&lt;/pre&gt;
%ENDTWISTY%

&lt;/div&gt;

Where is Condor installed? 

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% which condor_q
/usr/bin/condor_q

%RED%# It was installed using the Condor RPM %ENDCOLOR%
% rpm -q condor
condor-7.9.0-51900

% rpm -ql condor | head -10
/etc/condor
/etc/condor/condor_config
/etc/condor/condor_config.local
/etc/condor/config.d
/etc/init.d/condor
/etc/sysconfig/condor
/usr/bin/classad_functional_tester
/usr/bin/classad_version
/usr/bin/condor_check_userlogs
... output shortened ...
&lt;/pre&gt;

Condor has some configuration files that it needs to find. They are in the standard location, =/etc/condor=

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% ls /etc/condor
chtc-pool_password  chtc-submit_mapfile  condor_config	condor_config.local  config.d/
&lt;/pre&gt;

Condor has some directories that it keeps records of jobs in. Remember that each submission computer keeps track of all jobs submitted to it. That&#39;s in the local directory: 

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% condor_config_val -v LOCAL_DIR
LOCAL_DIR: /var
  Defined in &#39;/etc/condor/condor_config&#39;, line 61.

% ls -CF /var/lib/condor
execute/  spool/
&lt;/pre&gt;

The spool directory is where Condor keeps the jobs you submit, while the execute directory is where Condor keeps running jobs. Since this is a submission-only computer, it should be empty.

Check if Condor is running: 
&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
%  ps auwx --forest | grep condor_ | grep -v grep
condor    4252  0.0  0.0  97624  8064 ?        Ss   Jun05  12:43 /usr/sbin/condor_master -f -pidfile /var/run/condor/condor.pid
condor    6369  0.0  0.0  96276  6448 ?        Ss   Jun07   0:03  \_ condor_shared_port -f -p 9618
condor    6370  0.0  0.0  98504  8936 ?        Ss   Jun07   0:13  \_ condor_schedd -f
root      6372  0.0  0.0  23544  3508 ?        S    Jun07   3:35  |   \_ condor_procd -A /var/run/condor/procd_pipe.SCHEDD -L /var/log/condor/ProcLog.SCHEDD -R 10000000 -S 60 -C 1000
condor    6371  0.0  0.0  96568  6892 ?        Ss   Jun07   0:03  \_ condor_job_router -f
cndrcron 14870  0.0  0.0  95392  5536 ?        Ss   Jun15   1:43 condor_master -pidfile /var/run/condor-cron/condor_master.pid
cndrcron 14872  0.0  0.0  97096  8980 ?        Ss   Jun15   1:31  \_ condor_schedd -f
root     14873  0.0  0.0  23680  3504 ?        S    Jun15   3:57      \_ condor_procd -A /var/lock/condor-cron/procd_pipe.SCHEDD -R 10000000 -S 60 -C 107
cndrcron 15421  0.0  0.0  94012  5976 ?        S    11:20   0:00      \_ condor_starter -f -job-cluster 9 -job-proc 0 -header (9.0)  -job-input-ad - -schedd-addr &lt;127.0.0.1:37675&gt;
cndrcron 15452  0.0  0.0  94012  5976 ?        S    11:23   0:00      \_ condor_starter -f -job-cluster 7 -job-proc 0 -header (7.0)  -job-input-ad - -schedd-addr &lt;127.0.0.1:37675&gt;
cndrcron 15453  0.0  0.0  94012  5972 ?        S    11:23   0:00      \_ condor_starter -f -job-cluster 11 -job-proc 0 -header (11.0)  -job-input-ad - -schedd-addr &lt;127.0.0.1:37675&gt;
cndrcron 15524  5.0  0.0  94012  5976 ?        S    11:26   0:00      \_ condor_starter -f -job-cluster 12 -job-proc 0 -header (12.0)  -job-input-ad - -schedd-addr &lt;127.0.0.1:37675&gt;
&lt;/pre&gt;

This computer has a bit of a complicated set up because there are two versions of Condor running. Sorry for the messiness of real life! We&#39;ve used the &quot;--forest&quot; option to ps to group the Condor processes into two clumps. The second Condor in my example (the one running as the cndrcron user) is a specialized version that you can ignore for now. (Or ask Scot when you have time what&#39;s going on!)

For the first version of Condor, the one you care about, there are four processes running: the condor_master, the condor_schedd, the condor_procd, and condor_shared_port. In general, you might see many different Condor processes. Here&#39;s a list of the processes:

   * *condor_master*: This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them.
   * *condor_schedd*: If this program is running, it allows jobs to be submitted from this computer--that is, your computer is a &quot;submit machine&quot;. This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started.
   * *condor_procd:* This process helps Condor track process (from jobs) that it creates
   * *condor_collector:* This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command. At the school, it is running on a different computer, and you can figure out which one: 

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% condor_config_val COLLECTOR_HOST
cm.chtc.wisc.edu
&lt;/pre&gt;

   * *condor_negotiator:* This program is part of the Condor central manager. It decides what jobs should be run where. It is run on the same computer as the collector.
   * *condor_startd:* If this program is running, it allows jobs to be started up on this computer--that is, your computer is an &quot;execute machine&quot;. This advertises your computer to the central manager so that it knows about this computer. It will start up the jobs that run.
   * *condor_shadow:* For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance (see the standard universe later.) You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out. 
   * *condor_shared_port:* Used to assist Condor with networking by allowing multiple Condor processes to share a single network port. 

We have a [[%ATTACHURLPATH%/daemons_small.gif][graphic representation of these daemons]], drawn by Sarah Miller, age 12. 

---++ condor_q

You can find out what jobs have been submitted on your computer with the condor_q command: 

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% condor_q

-- Submitter: osg-ss-submit.chtc.wisc.edu : &lt;128.104.100.55:9618?sock=4252_f786_2&gt; : osg-ss-submit.chtc.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended
&lt;/pre&gt;

Nothing is running right now. If something was running, you would see output like this: 

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% condor_q
-- Submitter: vdt-itb.cs.wisc.edu : &lt;198.51.254.90:39927&gt; : vdt-itb.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
60256.0   bbockelm        7/6  21:01   0+00:00:00 I  0   0.0  mytest.sh         
60279.0   roy             7/7  16:55   0+00:00:01 R  0   0.0  mytest.sh         
60279.1   roy             7/7  16:55   0+00:00:01 R  0   0.0  mytest.sh         
60279.2   roy             7/7  16:55   0+00:00:01 R  0   0.0  mytest.sh         
60279.3   roy             7/7  16:55   0+00:00:01 R  0   0.0  mytest.sh         
60279.4   roy             7/7  16:55   0+00:00:01 R  0   0.0  mytest.sh         

6 jobs; 1 idle, 5 running, 0 held
&lt;/pre&gt;

The output that you see will be different depending on what jobs are running. Notice what we can see from this:

   * *ID*: We can see each jobs cluster and process number. For the first job, the cluster is 60256 and the process is 0.
   * *OWNER*: We can see who owns the job.
   * *SUBMITTED*: We can see when the job was submitted
   * *RUN_TIME*: We can see how long the job has been running.
   * *ST*: We can see what the current state of the job is. I is idle, R is running.
   * *PRI*: We can see the priority of the job.
   * *SIZE*: We can see the memory consumption of the job.
   * *CMD*: We can see the program that is being executed. 

&lt;div style=&quot;margin-left: 1em; margin-right: 1em; background-color: #ffff66; border: 1px solid black; padding: 0.5em;&quot;&gt;
*Extra credit*

%TWISTY{%TWISTY_OPTS_MORE%}%
What else can you find out with condor_q? Try any one of:

   * man condor_q
   * condor_q -help
   * [[http://www.cs.wisc.edu/condor/manual/v7.8/condor_q.html][condor_q from the online manual]]

*Double bonus points*

How do you use the -constraint or -format options to condor_q? When would you want them? When would you use the -l option? This might be an easier exercise to try once you submit some jobs.
%ENDTWISTY%
&lt;/div&gt;

---++ condor_status

You can find out what computers are in your Condor pool. (A pool is similar to a cluster, but it doesn&#39;t have the connotation that all computers are dedicated full-time to computation: some may be desktop computers owned by users.) To look, use condor_status:

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% condor_status

Name               OpSys      Arch   State     Activity LoadAv Mem   ActvtyTime

slot1@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.180  4599 50+03:14:01
slot2@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000  1024 50+03:14:02
slot3@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000  1024 50+03:14:03
slot4@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000  1024 50+03:14:04
slot5@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000  1024 50+03:14:05
slot6@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000  1024 50+03:14:06
slot7@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000  1024 50+03:14:07
slot8@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000  1024 50+03:14:00
slot9@c002.chtc.wi LINUX      X86_64 Owner     Idle     0.000   250 50+03:14:01
slot1@c003.chtc.wi LINUX      X86_64 Claimed   Busy     0.000  4599  0+02:13:34

... [snip] ...
                      Total Owner Claimed Unclaimed Matched Preempting Backfill

       INTEL/WINDOWS     2     0       0         2       0          0        0
       INTEL/WINNT61    12     0       0        12       0          0        0
        X86_64/LINUX  1983   203    1241       381       0        158        0

               Total  1997   203    1241       395       0        158        0
...
&lt;/pre&gt;

Yikes! That&#39;s a ton of computers. You will be sharing our Condor pool that is used across the UW-Madison campus. In order to ensure good response time for you, we&#39;ve allocated a couple of computers for your exclusive use. You can see them by telling Condor to only show the computers which have a specific attribute in their !ClassAds. (Think back to the lecture!) We were able to define this attribute because Condor lets us add anything we want to the !ClassAd. Try it out:

&lt;pre style=&quot;margin-left:4em&quot; class=&quot;screen&quot;&gt;
% condor_status -constraint &quot;IsOSGSchoolSlot =?= true&quot; 

Name               OpSys      Arch   State     Activity LoadAv Mem   ActvtyTime

slot10@e026.chtc.w LINUX      X86_64 Owner     Idle     0.000  1024  0+20:01:41
slot11@e026.chtc.w LINUX      X86_64 Owner     Idle     0.000  1024  0+20:01:42
slot12@e026.chtc.w LINUX      X86_64 Owner     Idle     0.000  1024  0+20:01:43
slot13@e026.chtc.w LINUX      X86_64 Owner     Idle     0.000   250  0+20:01:44
slot1@e026.chtc.wi LINUX      X86_64 Owner     Idle     0.000  6441  0+00:00:03
slot2@e026.chtc.wi LINUX      X86_64 Owner     Idle     0.000  7168  0+00:00:05
...
                     Total Owner Claimed Unclaimed Matched Preempting Backfill

        X86_64/LINUX    26    25       1         0       0          0        0

               Total    26    25       1         0       0          0        0
&lt;/pre&gt;

There should be more computers available during the school, so don&#39;t be worried if the numbers don&#39;t match. Also, that command is kind of long and you might want a shortcut. You can use the =school_status= command, which will do exactly the same thing as that command.

Note that each computer shows up multiple times, with a slotN at the beginning of the name. This is because we&#39;ve configured Condor to be able to run multiple jobs per computer. Slot refers to &quot;job slot&quot;. We do this because these are multi-core computers and they are fully capable of running more jobs at once.

Let&#39;s look at exactly what you can see:

   * *Name*: The name of the computer. Sometimes this gets chopped off, like above.
   * *OpSys*: The operating system, though not at the granularity you may wish: It says &quot;Linux&quot; instead of which distribution and version of Linux.
   * *Arch*: The architecture, such as INTEL or PPC.
   * *State*: The state is often Claimed (when it is running a Condor job) or Unclaimed (when it is not running a Condor job). It can be in a few other states as well, such as Matched.
   * *Activity*: This is usually something like Busy or Idle. Sometimes you may see a computer that is Claimed, but no job has yet begun on the computer. Then it is Claimed/Idle. Hopefully this doesn&#39;t last very long.
   * *LoadAv*: The load average on the computer.
   * *Mem*: The computers memory in megabytes.
   * *ActvtyTime*: How long the computer has been doing what it&#39;s been doing. 

&lt;div style=&quot;margin-left: 1em; margin-right: 1em; background-color: #ffff66; border: 1px solid black; padding: 0.5em;&quot;&gt;
*Extra credit*

%TWISTY{%TWISTY_OPTS_MORE%}%
What else can you find out with condor_status? Try any one of:

   * man condor_status
   * condor_status -help
   * [[http://www.cs.wisc.edu/condor/manual/v7.6/condor_status.html][condor_status from the online manual]]

Note in particular the options like -master and -schedd. When would these be useful? When would the -l option be useful? 
%ENDTWISTY%
&lt;/div&gt;


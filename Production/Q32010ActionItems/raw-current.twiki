%TABLE{ tablewidth=&quot;700&quot; columnwidths=&quot;5%, 15%, 80%, 10%, 10%&quot; cellpadding=&quot;2&quot; dataalign=&quot;left&quot; tablerules=&quot;all&quot; tableborder=&quot;2&quot; databg=&quot;#FFFFFF, #FFFFFF&quot;}%
| # | Owner | Action/Significant Item | Open Date | Close Date |
|45| Scott | GOC is planning an ITB stress test to help determine when to upgrade the BDII | 9/21/2010 | |
|44| Scott | On 16/Sep between 9:10 and 9:45 EDT network traffic between the GOC machine rooms in Indianapolis and Bloomington was blocked due to a mis-configured firewall. GOC monitoring immediately detected the problem and sent warning emails. Outside GOC users do not appear to have been affected. | 9/21/2010 | |
|43| Armen | BNL is currently draining queues to patch the latest SLC5 kernel vulnerability | 9/21/2010 | |
|42| Brian | LSST effectively completed its targeted production run of 450 images | 9/14/2010 | |
|41| All | LIGO is averaging about 1/3 of its former capacity ~150K hours/wk. | 9/14/2010 | |
|40| All | Xin confirmed that SE-only publishing is working correctly on Atlas. | 9/14/2010 | |
|39| All | The GOC is now using Puppet for installation and software management. One immediate benefit was that the last upgrade took ~9 hours on the ITB but only ~2.5 hours using Puppet. | 9/14/2010 | |
|38| Brian | It is important for sites to make sure that the &quot;Installed Capacities&quot; are listed correctly in OIM, and consistent with WLCG as these will be reported to the WLCG starting in Oct. | 9/14/2010 | |
|37| Brian | LSST is working well, averaging &gt;60K hours per day vs a target of 50K. | 9/7/2010 | |
|36| All |  Scott is working on a written process for how new Gums templates get deployed (as part of an updated VO package) | 9/7/2010 | | 
|35| Robert, Mats |  Both Engage and LIGO are hitting limits as far as production goes due to the temporary increased CMS usage. LIGO is down to a fraction of what was previously available. Engage feels the limits both in the additional length of time it takes to get jobs started as well as the length of time it takes to get jobs back as noted in the Engage throughput charts. | 9/7/2010 | |
|34| All | Issue related to fixing the process as related to understanding CERN BDII failures is still in progress. Ruth is planning to bring it up at the next WLCG management meeting. | 9/7/2010 | |
|33| All | Both LIGO and Engage noted that increases in CMS production (like last week) represent troughs in opportunistic usage. It is important for OSG to broaden resources beyond CMS | 8/31/2010 | |
|32| All | Last week the OSG hit a new Production high of 7.25 million hours. | 8/31/2010 | |
|31| Burt, Dan | The continued inability to get information via the WLCG call about issues with the CERN BDII is problematic and indicative of a WLCG process problem. One way this is impacting OSG is that without a steady stream of information, we cannot take the risk of upgrading our critical BDII software. Dan to have Ruth bring this concern up at the next WLCG management meeting | 8/31/2010 | |
|30| All | The LHC has now scaled up to collisions of 48 bunches. The plan is to be at ~350 sometime in November. | 8/24/2010 | |
|29| Dan |  The phone number needs to be changed for the Production calls as ESNet has transitioned to Reddytalk. | 8/24/2010 | |
|28| All | At 4:35 EST August 23, it was reported that no RSV_SAM messages had been received since August 20. By 10:30 EST the scripts had been reverted to their original versions and the missing data re-sent. Scott has a link to a draft report below. | 8/24/2010 | |
|27| Scott, Tony | Although the GOC has requested it on multiple occasions, there has been no response from CERN on why the CERN BDIIs were failing. The GOC will keep trying to get this info. | 8/24/2010 | |
|26| All |  OSG planning to upgrade to BDII 5.0.8 in the ITB for testing. | 8/17/2010 | |
|25| All | Glue SiteSponsorField?&lt;https://twiki.grid.iu.edu/bin/edit/Production/SiteSponsorField?topicparent=Production.Aug17,2010ProductionMeeting&gt; is single valued and not multi-valued.  Data gets stale and piles up | 8/17/2010 | |
|24| All | Burt noted two problems with the BDII software at CERN, although it is not clear if these are still in the latest version 5.0.8. Until these are fixed, OSG should not upgrade the production service. | 8/17/2010 | |
|23| Burt | Condor bug on the CMS T1 at FNAL on the calculation of the number of jobs running in Condor 7.2.1. Plan to upgrade soon. | 8/17/2010 | |
|22| Xin | Xin to start a conversation between folks configuring Condor at BNL with Condor experts at FNAL. | 8/17/2010 | |
|21| Burt, Xin | GOC has produced an updated punch list to ensure that BDIIs are working properly when they are restarted. This should prevent outages such as occurred last week. Burt, Xin to double check this list and provide feedback. | 8/17/2010 | |
|20| All | When Doug and Mine were both away on vacation, a deputy RA was not assigned to take Doug&#39;s role and at least one cert request was delayed by two weeks. Mine has updated the leave-vacation process. | 8/10/2010 | |
|19| Burt, Dan |  Burt noted that the CMS Tier1 will be handling a series of high-demand workflows for the next several weeks and will be pre-emptiing and throttling jobs from opportunistic VOs. Dan to send a note to the VO team to inform the VOs so they have proper expectations. | 8/10/2010 | |
|18| Rob | It was also noted that queries on the IS1 BDII are experiencing sporadic delays when the system is under heavy load. This is currently under priority investigation. | 8/10/2010 | |
|17| Rob, Burt, Dan | There was a BDII outage on Tuesday morning for about 45 minutes as a result of applying the LDAP security patch that caused the BDIIs to stop reporting. Two key problems were identified: 1) The BDII system update process did not have a complete test procedure in place to ensure that the BDII is reporting properly prior to modifying the other BDII system. Action: Create a punch list for testing a BDII system to ensure everything is working properly and have it reviewed by CMS and Atlas. (Rob, Burt) 2) The version of the BDII in the ITB is not the same as that in production due to system freezing at production level operations. Rob has raised this issue previously on the operations calls. A plan to determine how best to resolve this is underway. (Dan, Rob) | 8/10/2010 | |
|16| All |  DOE is switching away from the CISCO system so the phone dial-in coordinates for the production call will need to change sometime in the next several weeks. | 8/3/2010 | |
|15| Rob | Kernel updates on ReSS?&lt;https://twiki.grid.iu.edu/bin/edit/Production/ReSS?topicparent=Production.Aug3,2010ProductionMeeting&gt;? machines on Thursday, Aug. 4 2010. High Availability will be used to make sure one or the other ReSS?&lt;https://twiki.grid.iu.edu/bin/edit/Production/ReSS?topicparent=Production.Aug3,2010ProductionMeeting&gt;? server is available throughout the reboot. | 8/3/2010 | |
|14| Robert | LIGO submissions were starting up too agressively at Nebraska and overwhelming the gatekeeper. The GRIDMANAGER_MAX_JOBMANAGERS_PER_RESOURCE parameter was changed from 150 to 15 to fix this problem. | 8/3/2010 | |
|13| Burt, Xin | The dCache Gratia transfer probe that groups transfers (as opposed to recording individual transfers) has been installed and verified at Fermilab. Load on the Gratia collectors dropped by ~40% as a result of this change. Atlas will deploy the probe this week. | 8/3/2010 | |
|12| All | There is a potential DoS Vulnerability in OpenLDAP. The patch was applied to the ITB BDII, and will be applied to the production BDIIs next week. Analysis is ramping up again on the CMS and Atlas sites so we will wait to perform further BDII upgrades. | 8/3/2010 | |
|11| Armen, Suchandra | The SE-only update was tested at BNL and seems to be working. The ITB is completing tests before release into production. | 7/27/2010 | |
|10| All | Also Igor noted last week that LIGO was causing unspecified problems on the San Diego T2 and are now banned. Burt and Dan will follow up to understand in more detail what the exact issue is. | 7/27/2010 | |
|9| LIGO | LIGO E@H&lt;mailto:E@H&gt; discovered they were submitting with an older code version on the Nebraska T2 that was taxing the NSF servers. Ligo was banned for about 12 hours but this problem has now been fixed. | 7/27/2010 | |
|8| Burt | CMS job submission was low due to iCHEP. This was a good week for opportunistic VOs. | 7/27/2010 | |
|7| Dan | SBGrid continues to hit new peaks and was up to 7K simultaneous jobs this morning. | 7/13/2010 | |
|6| Abhishek | The new DZero peak last week was confirmed at 16.4 MEvents/week. | 7/13/2010 | |
|5| Rob | FNAL Gratia services will be undergoing maintenance and are expected to be down roughly from 9am to about 4pm Central on 7/14. | 7/13/2010| |
|4| Xin | The SE-only fix is now available in the VTB and is now just starting to test the SE-only | 7/13/2010 | |
|3| Abhishek | Pending verification with the DZero team, it looks like the Monte carlo production has achieved a new record rate:   16.4 M Evts/week. 112,000 wall hours/day at 91% efficiency. | 7/9/2010 | |
|2| Rob, Dan | The ITB BDII update will go into place later today to allow testing of SE-only systems. Testing &amp; documentation for adding the GIP to an SE-only install is currently in progress by the VDT &amp; Storage teams. | 7/9/2010| |
|1| Chris | All FNAL-based Gratia services will be down for up to 4 hours on Wed July 14 for OS and service upgrades. In addition, the previously announced decommissioning of the legacy redirector service will also take place at this time. | 7/9/2010| |

-- Main.SarahCushing - 06 Jul 2010

-- Main.MatsRynge - 06 Apr 2010


---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * The wide variance in data transfers at Atlas T1 (Hundreds of TB/day to near zero on some days) appears to be due to actual load variances and not a problem in Gratia. (Xin)
   * DZero now running at near peak capacity (12M Events) and not hitting any production limits. Their thank you to the CMS T1 folks for increasing the number of batch slots available. (Abhishek)
   * SBGRID hit a new peak of 70K hours/day. (Abhishek)
   * NanoHub reported a bug in DagMan after upgrade to Condor 7.4.1. A patch has been issued that NanoHub is using. (Abhishek)
   * A known bug (feature?) in Apache was re-discovered this week where Apache must be restarted in order to pick up new certs. Currently there is no work around available. (Mine, Brian) 

---++ Attendees:
   * Mats, Xin, Britta, Rob E., Brian, Suchandra, Marco, Abhishek, Rob Q., Mine, Chander, Dan
 
---++ CMS (Burt)

   * Burt out for Jury duty

---++ Atlas (Armen &amp; Xin)

   * General production status
      * During the last week ATLAS production was stable, at the level of 8k running simulation jobs. Data transfer to BNL during the week was about 35TB data, and 130TB simulation. Since March 30 LHC was delivering collisions, almost every day. Total delivered luminosity to ATLAS is about 180ub-1 (inverse millibars).
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.6M jobs, with CPU/Walltime ratio of 86%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 1.6M managed MC production, validation and reprocessing jobs 
         * average 238K jobs per day
         * failed 131K jobs
         * average efficiency:  jobs  - 93%,  walltime - 95%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate was low last week, ~50 TB/day. It ramped up to ~200TB/day in the last 4 days. 
   * Issues
      * Investigate of abnormally lower transfer rate reported by gratia last week -- no problem with gratia, just the transfer rate was low, plus some dcache srm/gridftp issues at the same time.
      * Opportunistic SE usage for D0 : not much updates. still running tests on UTA T2 site.

---++ LIGO (Britta, Rob E.)

   * Britta and Rob out. Rob to send a report tomorrow.

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Availability metrics for the last week 
      * [[http://tinyurl.com/y9ctxkg][GOC Services: BDII, MyOSG, RSV Collector, OSG Display]]
         * On April 1st, [[http://tinyurl.com/ybrpc29][both the OSG and the WLCG BDIIs on both is1 and is2 were claimed to be down by GOC&#39;s RSV monitoring as shown on MyOSG&#39;s status history]] -- GOC found a bug in the RSV probe&#39;s sorting of dates that manifested itself around 00:00 UTC on April 1st (Unintended subconscious all-fools day prank on self?!); the bug has been fixed; NOTE: Since this was not a real outage there were no alerts sent out to GOC staff by our alerting mechanism
      * [[http://tinyurl.com/y9p67ng][GOC hosted Security services managed by OSG security team]]
   * *Production release*: See [[http://osggoc.blogspot.com/2010/03/goc-service-update-tuesday-march-30th.html][blog]] for more details -- Complete
      * Upgrade took around 45 minutes total; No outages were noticed
      * BNL RT GOC-TX
         * Updates to GOC-TX, etc. were deployed on March 30th; GOC testing the deployment; 
   * *[[http://osggoc.blogspot.com/2010/03/osg-106-update-announcement.html][OSG 1.0.6 released]]*

 
---+++ Operations This Week
   * *Ticket Exchange (TX)*: 
      * GGUS
         * USCMS had question about origin of alarm tickets from training/ITB systems; We had contacted GGUS admins about this...
            * GGUS developers test alarm mechanism to US-T1 sites after each production release (usually last Wednesday of each month); changes in release date, if any, are announced in advance.
            * GGUS developers may occasionally create alarm tickets on GGUS-Train but these tickets do *not* send a text message to the relevant party; however these tickets will create corresponding GOC-Ticket-ITB tickets, and consequently corresponding support center tickets (for example: on BNL&#39;s production RT system)
      * BNL RT
         * GOC testing the deployment; we are close to declaring this ready for real production tickets.
         * [[http://docs.google.com/Doc?docid=0AQGl_1gJKDWDZGY5ZnR6anZfMjFmMmpzZmc1cA&amp;hl=en#Transition_from_email_based_ex_7272405605382354][GOC-TX  section with detailed transition proposal]]
      * FNAL Remedy
         * Further connections issues and problem with web service interface reported to Remedy developers; Not high priority at FNAL
      * We urge everyone to move to using using GOC-TX -- contact GOC if you do ticket exchange with us! (In the near future, we will also start initiating contacts to our ticket exchange collaborators]
         * GOC discussing this with several support centers since last week - stay tuned
   * *Reminder*: GOC Ticket emails will begin to appear from osg [at] tick.globalnoc.iu.edu instead of of osg [at] tick-indy.globalnoc.iu.edu -- starting mid April.
      * GOC working with collaborating SCs that do ticket exchange to try to ensure nothing breaks 


---++ Engage (Mats, John, Chris)


10 users utilized 35 sites

35003 jobs total (19186 / 15817 = 54.8% success)

37152.4 wall clock hours total (33600.6 / 3551.8 = 90.4% success)

Low job success is because a user submitted a 64 bit executable to 32 bit sites.


---++ Integration (Suchandra)
    * OSG 1.0.6 released last week
    * Currently testing OSG 1.2.9
        * In VTB testing currently
        * Should release in about 2.5 weeks
    * ITB Robot
        * Automated testing and reporting should be functioning in 2-3 days
    * Documentation
        * Pages in Trash/Trash/Integration twiki will have html comments for doc group by Thursday
        * Will be reviewing pages/ownership during 1.2.9 cycle 

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.8
      *       71 OSG 1.2.X resources (      19 are 1.2.8)
      *        8 OSG 1.0.X resources (       2 are 1.0.5)
      *        9 OSG 1.0.0 resources
      *        2 OSG 0.8.0 resources
Site Coordination meeting this Thursday 4/15 at 11am central
    * Phone: 510-665-5437, #1212
    * Adobe connect: http://osg.acrobat.com/osgsc100415/


---++ Metrics (Brian)

---++ Virtual Organizations Group (Abhishek)

   * *D0* 
      * MC Production average rose to 12 M Events/week. 134,000 wall-hours/day at 81% efficiency. 
      * Availability (and number) of batch slots at CMS T1 has increased now.
      * Possibly, no immediate need to increase number of sites available to D0.
      * Looking to increase efficiency at available sites. E.g., efficiency is:
         * poor at ce.grid.unesp.br, osg-ce.sprace.org.br, umiss001.hep.olemiss.edu
         * not satisfactory at osg.rcac.purdue.edu, prairie-fire cluster at UNL

   * *SBGrid*
      * VO-side: 
         * Peak consumption recorded at 70,000 hours/day. Piotr Sliz confirmed this from SBGrid accounting view.
         * Last week&#39;s average was 29,000 hours/day at 72% efficiency.
         * SBGrid is currently in data analysis mode, preparing for a publication. 
         * Will be able to define future targets as soon as back to the testing/production mode.
         * SBGrid hours, Nov 2009 to Apr 2010: &lt;br /&gt;
     &lt;img src=&quot;%ATTACHURLPATH%/SBGrid-OSG_hours_nov2009_to_apr2010.png&quot; alt=&quot;SBGrid-OSG_hours_nov2009_to_apr2010.png&quot; width=&#39;800&#39; height=&#39;500&#39; /&gt;  

   * *Fermi-VO*
      * Fermi-VO team in discussions with FNAL management to get accurate official estimates for subVOs. MINOS may continue to be the most active consumer (5.4 Mhours in 2009; unofficial estimate nearly 8 Mhours in 2010).
   
   * *DOSAR*
      * Workshop April 6-8. URL: http://physics.uj.ac.za/conferences/2010/DOSAR

   * *nanoHUB*
      * Problems were noticed in DAGMan after upgrading submit/workflow infrastructure to Condor 7.4.1; recurring schedd crashes; multiple copies of the same jobs were being submitted; resulted in apparent peaks in consumption.
      * Investigation between Steve C, Alain, Abhishek. Jaime Frey has supplied fix as a patched binary; being evaluated with nanoHUB now.
     
---++ Security (Mine)
   * two isolated issues at TG. no affects on OSG.
   * progress with pakiti tool. Anand is preparing an operation/deployment plan
   * progress with ITB certificate test cache. Doug is planning the plan this week. 
   * CA service at GOC is important. if this cache goes down, all sites will fail their can cause availability numbers to go down. We can modify the probe output to unknown instead of increasing the service level availability at GOC. we should reflect this in documentation as well. Mine will ask Jim Barlow.
   * Apache and Tomcat does not pick up new certificates immediately. Mine can discuss in STG to find solutions.
   * Java vulnerability and some of the patches are not available for free. Mine will look at the version we include in VDT.  
 
     


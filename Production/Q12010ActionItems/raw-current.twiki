-- Main.DanFraser - 01 Feb 2010
%TABLE{ tablewidth=&quot;700&quot; columnwidths=&quot;5%, 15%, 80%, 10%, 10%&quot; cellpadding=&quot;2&quot; dataalign=&quot;left&quot; tablerules=&quot;all&quot; tableborder=&quot;2&quot; databg=&quot;#FFFFFF, #FFFFFF&quot;}%
| # | Owner | Action/Significant Item | Open Date | Close Date |
|1| Xin, Burt, GOC, Dan | Xin has been monitoring the BDII problem with the new CE probe provided by Burt but did not see any anomalies. This seems to be a WLCG problem. This could be related to a CMS BDII problem where CERN BDII data was not getting updated. A ticket was filed with GGUS and worked on; the CERN BDIIs appear to be working now. Need to keep an eye on this and discuss strategies for monitoring. | 1/5/2010 | |
|2| | CMS noted that another site in addition to SDSC has run into zero-length CRL problems. Mine discussed the planned VDT fix that should not only fix this problem and provide better error reporting. This fix will be in the next VDT release. | 1/5/2010 | |
|3| Rob E | Dan reported that LIGO is currently down to update the error management system. Should be back online soon. | 1/5/2010 | |
|4| Mats | Engage needs the RESS RSV probe as soon as it is available. | 1/5/2010 | |
|5| Burt | Burt to ping the CMS T2s for an SE request to help D0 | 1/5/2010 | |
|6| Xin | Xin identified several T2s to help D0. Need to make sure everyone gets connected. | 1/5/2010 | |
|7| Burt | Current CMS queue of physics simulation results fulfilled. Enjoy your opportunistic cycles for now. | 1/12/2010 | |
|8| Burt | Non-CMS/CDF preemption issue @ MIT_CMS: second suggested fix now implemented. Tickets will now be closed. | 1/12/2010 | |
|9| Xin, Rob, Dan | GOC ticket 7772: Issues with WLCG BDII periodically loses information about some USATLAS Tier2 sites: continue to run the probe, see occasional info missing on both OSG and EGEE BDII server. Rob Q &amp; Dan to figure out next steps to follow up on this. | 1/12/2010 | |
|10| Chris, Dan F, Rob Q | 20 sites running VDT 1.0.0 or previous need to be reconfigured to point to the current gratia service address. List appended at bottom of report. Plan to file tickets making the change requesting that the sites either make a pointer update or a VDT upgrade (recommended). | 1/12/2010 | |
|11| Rob E | LIGO is now ramping up again after implementation of code changes to improve error handling | 1/12/2010 | |
|12| Abhishek, Mats, Dan | SBGrid made several changes and recorded a new peak of 1000+ simultaneous jobs, with 20,000 wall hours/day. | 1/12/2010 | |
|13| Abhishek, Rob | GridUNESP / DOSAR has requested the GOC to make new release of VO package | 1/12/2010 | |
|14| | Continue monitoring BDIIs | 1/19/2010 | |
|15| Burt | Had to discontinue LIGO support at MIT_CMS. Their workflow is too much I/O for the NFS configuration there. | 1/26/2010 | |
|16| Burt, Rob | Good news: roughly 3k concurrent LIGO jobs over the weekend at the FNAL T1 | 1/26/2010 | |
|17| Rob E | Rob E. is planning to switch from NFS to local storage at Caltech who had also noted an issue with NFS overloading. | 1/26/2010 | |
|18| Rob, Xin | While the BDII problem at BNL occurs rarely ~.05%, the plan is to extract and analyze the error messages and also to see if there is any correlation between BNL and GOC probes. | 1/26/2010 | |
|19| Brian | Brian to send an email describing the proposed naming change to Xin and Burt. | 1/26/2010 | |
|20| BDII | A bug was finally found and fixed in the BDII at CERN that resulted in stale entries. This was first noted by Xin over 2 months ago | 2/2/2010 | |
|21| Rob | GOC&#39;s monitoring of the BDIIs has demonstrated that everything is working appropriately. Problems were correlated to site outages or downtimes. Plan to close ticket 7772. The current script is too complicated to use on an ongoing basis for monitoring, but it was agreed that some type of ongoing monitoring should be implemented and run at the GOC. | 2/2/2010 | |
|22| Rob | A problem with GGUS-GOC ticket exchange system has been fixed as the transition was made to implement a web services based system that interfaces directly between the two systems. | 2/2/2010 | |
|23| Robert | LIGO hit a new record of 160K hours/day thanks in part to Burt&#39;s increasing the disk quota on the FNAL T1, and also due to some storage optimizations made by Robert. | 2/2/2010 | |
|24| Burt | The number of simultaneous jobs that can be run at FNAL is limited by a Condor 7.3 limitation to ~3000. Upgrading to 7.4 should improve this. | 2/2/2010 | |
|25| Abhishek, Dan | SBGRID is trying multiple solutions (now looking at PANDA). It would be better if we can try and focus them to complete one strategy before diversifying.  | 2/2/2010 | |
|26| | Atlas and CMS production is still low as we await the LHC turn on later this month. | 2/9/2010| |
|27| Xin | Atlas needs a technical contact from DZero to continue working on the opportunistic storage request. Dan to follow up. | 2/9/2010 | |
|28| Rob E | LIGO reached the Rank of #1 for the first time with over 2.5 Million hours/wk | 2/9/2010 | |
|29| | A power outage at FNAL on Tuesday caused a disruption in all computational services at FNAL for several hours; there was a corresponding delay until production services could be fully restored. | 2/9/2010 | |
|30| Rob, Dan, Chander | There was a 3-day delay in the forwarding of two Atlas Alarm tickets while the GOC was upgrading its ticketing links between GGUS and BNL and automatic routing was not functional. Automatic routing was restored and tested on Monday. Additional change control processes are being explored to prevent similar problems in the future. | 2/9/2010 | |
|31| Abhishek | SBGRID scaled to meet its goal of ~3000 job submissions but failure rates are &gt;50% and they are evaluating Panda. | 2/9/2010 | |
|32| Burt | Fermi T1 now at Condor 7.4.1; Burt is incrementally increasing the opportunistic jobs limit beyond 5500 (previous limit was 3500). Will keep increasing as long as the system remains stable. | 2/16/2010 | |
|33| Rob | Gratia data collection will be delayed for about six hours on Thursday while the database is transferred to new hardware. | 2/16/2010 | |
|34| Rob | Production service updates at the GOC are being frozen (exceptions considered on a case by case basis) to provide maximum stability as the LHC comes online and computing ramps up. | 2/16/2010 | |
|35| Mine | Plans are being made to support the IGTF format changes in OpenSSL?&lt;https://twiki.grid.iu.edu/bin/edit/Production/OpenSSL?topicparent=Production.Feb16,2010ProductionMeeting&gt;. The goal is to provide a seamless transition path for OSG services. Since security affects so many different services, an ITB cycle of testing may not be sufficient. The Production Coordinator has requested a test plan that STG, Operations, and Production can review. | 2/16/2010 | |
|36| Burt | Tier 1 opportunistic limit at 7100 jobs but opportunistic usage will drop as large global workflows begin next week | 2/23/2010 | |
|37| Xin | D0 in discussion with UTA on opportunistic storage | 2/23/2010 | |
|38| Rob E | LIGO currently running at rank 1 but as graphs show below this is largely due to the Tier-1; production is lower on other sites as they are starting to receive higher priority workloads. | 2/23/2010 | |
|39| Rob Q | First part of Gratia addition of higher capacity processing nodes (in parallel) was successful; Thursday is the planned switch-over to the new nodes exclusively | 2/23/2010 | |
|40| Abhishek | Fermi-VO lost some integration and testbed services as a result of the power outage. These will be moved to other facilities at FNAL. CDF production was affected by FNAL power outages. | 2/23/2010 | |
|41| Abhishek, Brian | D0 jobs having problems with resolving $OSG_WN_TMP. | 2/23/2010 | |
|42| Abhishek | SBGRID having difficulties at 4 large sites -- UFL, Purdue/RCAC, Buffalo/CCR, Caltech/Ultralight. &quot;Either due to the type of error or our submission configuration we only get back the Globus error code (17), and no other details. This makes it virtually impossible to debug.&quot; | 2/23/2010 | | 
|43| Burt, Armen | LHC: In commissioning phase. Beam splash seen; ramp up to collisions at 900 GeV?&lt;https://twiki.grid.iu.edu/bin/edit/Production/GeV?topicparent=Production.Mar2,2010ProductionMeeting&gt;, then up to 7 TeV?&lt;https://twiki.grid.iu.edu/bin/edit/Production/TeV?topicparent=Production.Mar2,2010ProductionMeeting&gt; in a few weeks | 3/2/2010 | |
|44| Brian | John Weigand has sent out tickets to several of the sites that are not reporting Gratia data, with some suggestions about how to fix the sites e.g. restart Gratia. All of these worked. Plan to continue with other sites (~14) that are not reporting. | 3/2/2010 | |
|45| Dan, Rob | As Abhishek &amp; Brian noted, the Gratia transition did not go as smoothly as predicted -- there was significant downtime during the change over to the new system. that was noticable by the VOs in their reports. The database is still not caught up. Dan/Rob to discuss with Keith how best to keep the GOC (and others) informed in such situations. | 3/2/2010 | |
|46| Brian | As a result of the Gratia change over to the new system, the DOE Display is showing some strange data (long strings of 0&#39;s). Brian is investigating. | 3/2/2010 | |
|47| All | First 7 Tev (center of mass) collisions at the LHC planned for end of March. | 3/16/2010 | |
|48| Xin | Atlas production returning to normal levels | 3/16/2010 | |
|49| Xin |  D0 should be set to run on UTA, ready to test | 3/16/2010 | |
|50| Metrics | Brian to follow up with John W on sites not reporting | 3/16/2010 | |
|51| All | New VDT security release 1.2.8 planned shortly. This will be an opportunity for sites running 1.0.x to upgrade (especially important if they are running Condor). Sites that upgrade will automatically be configured to point to the correct Gratia collector. | 3/16/2010 | |
|52| Dan, Abhishek, Rob E, etc | Rob E. is completing a document that describes the problems LIGO has encountered to get jobs running on many different sites. This will be used (in conjunction with other data) to inform possible strategies for helping other VOs not need to repeat this effort. | 3/16/2010 | |
|53| Rob | GOC production freeze scheduled to be lifted. Next production changes planned for Mar 30. | 3/16/2010 | |
|54| Brian | Problem with DOE DIsplay showing long strings of 0&#39;s was corrected | 3/16/2010 | |
|55| All | Machine: 7 TeV collisions achieved! | 3/30/2010 | |
|56| All | No data transfers were reported on Atlas over the weekend. Xin to work with the Gratia team to identify where the problem is. | 3/30/2010 | |
|57| All | GGUS tickets now being routed through GOC-TX (web services), running smoothly so far | 3/30/2010 | |
|58| All | GOC Ticket emails will begin to appear from osg [at] tick.globalnoc.iu.edu instead of of osg [at] tick-indy.globalnoc.iu.edu -- starting mid April.  GOC will work with collaborating SCs that do ticket exchange to try to ensure nothing breaks | 3/30/2010 | |
|59| Rob | The BDII monitors reported stale data in the BDII for a short period of time although the BDII logs did not indicate any problems. The Production Coordinator has requested that the GOC continue looking into this | 3/30/2010 | |
|60| John | Throughput limitations being explored by the Engage team appear to be due to specific user requested resources (e.g. memory requests) that limit the number of suitable sites on the OSG. | 3/30/2010 | |
|61| Abhishek, Dan | DZero production seems to be limited for the first time by the number of slots available on the OSG. Possible strategies for improving this being discussed | 3/30/2010 | |

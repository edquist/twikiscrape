-- Main.DanFraser - 05 Jul 2011
---++ Action/Significant Items:
   * There were major system wide network issues at IUPUI on Saturday morning. Scott is working with the IUPUI folks to understand the cause of the issue. (See the note below)
   * All the ITB tests passed for the new CA caches and these will be transitioned to production next week. Once in place we will start transitioning select sites to use these new caches. 
   *  The Operations team announced two new services: !GratiaWeb and &quot;Planet OSG&quot;

---++ Attendees:
   * Armen, Britta, Suchandra, Marco, Scott T., Mine, Dan
 
---++ CMS (Burt)

---++ Atlas (Armen &amp; Xin)

   * General production status
      * LHC had another productive week. ATLAS got 225pb-1 delivered, with total luminosity now for ATLAS 1.725fb-1. US ATLAS production was quite stable at the average level of about 12k running jobs. Majority of the jobs were simulation jobs, continuing the Geant4 re-simulation campaign.  
   * Job statistics for last week.      
      * Gratia report: USATLAS ran 2M jobs, with CPU/Walltime ratio of 91%. 
      * Panda world-wide production report (real jobs): 
         * completed 1.5M managed group, MC production, validation and reprocessing jobs 
         * average 220K jobs per day
         * failed 155K jobs 
         * average efficiency:  jobs - 91%
      * Real Jobs processed by US sites for last week, reported from PanDA monitor 
         * 990K 
   * Data Transfer statistics for last week
      * Data transfer rate was 200TB~400TB last week at BNL T1. 
   * Issues
      * OSG opportunistic access to USATLAS sites: 
         * HCC: roadblock cleared for SLAC, SLAC should be up soon; NET2 T2 still needs to find cycles to enable it. 
         * Engage: no update last week
      * update of LFC in VDT/OSG client packages : preliminary version under testing

---++ LIGO (Britta, Robert E.)

---+++ Gratia Reports

   * Current week&#39;s total usage: 5 users utilized 29 sites
      * 34102 jobs total (25817 / 8285 = 75.7% success)
      * 245544.5 wall clock hours total (182518.0 / 63026.4 = 74.3% success)
   * Previous week&#39;s total usage: 4 users utilized 35 sites
      * 36069 jobs total (25139 / 10930 = 69.7% success)
      * 297023.9 wall clock hours total (206287.7 / 90736.2 = 69.5% success)

---+++ LIGO / E@OSG

   * Recent Average Credit (RAC): 1,029,687.13430, Last Week: 1,249,519.95052
   * E@H rank based on RAC: 2 (-1)
   * E@H rank based on accumulated credits: 3 (+-0)


---+++ LIGO / INSPIRAL

   * Caltech Glidein Frontend testing:
      * Troubleshooting Binary Inspiral fails

---+++ LIGO/PULSAR
   * Caltech Glidein Frontend testing:
      * 50000 job dag submitted to frontend, running over 32 OSG sites
      * fails at UFlorida-HPC, UMissHEP

   * One 100,000 job dag running at UFlorida_PG, UCSDT2, LIGO_CIT, Nebraska
   * Twelve 50,000 job dags running at  12 OSG sites: USCMS-FNAL-WC1-CE3, GridUNESP_CENTRAL, LIGO_UWM_NEMO, Purdue-RCAC, Firefly, 
      UFlorida-PG, UCSDT2,  Nebraska, LIGO_CIT, SPRACE, STAR_BNL, CORNELL

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * [[http://tinyurl.com/27fknc6][GOC Services Availability/Reliability]]
   * [[http://myosg.grid.iu.edu/miscstatus/index?datasource=status&amp;count_sg_1=on&amp;count_active=on&amp;count_enabled=on][Current Status]]
   * Network outages in Indianapolis caused outages for OIM and TWiki, degraded performance for other services. As of 11:30 EDT, this is what is known:

Starting Saturday morning around 6:30am, we began experiencing intermittent issues with the router serving the IUPUI data center, requiring manual intervention to resolve.  We immediately engaged our vendor, Juniper, working with them throughout the weekend to identify and attempt to resolve the issue.  Sunday morning around 10am, with assistance from Juniper, we broadly identified a specific type of traffic that instigated the events and took measures to limit the traffic sources.  Since that time, the network has been stable. 

This traffic is not malicious, nor customer sourced, but instead normal network monitoring traffic used across campuses.  Juniper is now working to determine the correlation between that traffic and the events that ensued.

We will continue to work with Juniper, providing them necessary data to assist in identifying root-cause and help them come to a long term solution.

   * Production release, [[http://osggoc.blogspot.com/2011/07/goc-service-update-tuesday-july-26th-at.html][ Release notes]]
      * Operating system updates, reboot required.
      * OIM 2.36, external library updates, Installed capacity report fixed, cosmetic changes.
      * MyOSG 1.37, updated BDII information page, updated various URL, minor changes due to FF and Chrome.
      * GOC ticket 1.40, updated various URL, template updates, minor changes due to FF and Chrome.
   * !InCommon HTTP/SSL certificates Canceled, rescheduled for 9/Aug
   * New services announced: !GratiaWeb and &quot;Planet OSG&quot;

---+++ Operations This Week
   * ITB release
      * Cosmetic changes to OIM
      * !InCommon certificates installed two weeks ago on ITB machines
      * cometd installed on ticket-itb. Allows user to see who else is viewing/editing a ticket.
      * assignee and ticket id logic changes/corrections 
   * Testing of new CA distributions underway
   * Next week !TWiki-docteam to move from Indianapolis to Bloomington (There are no twiki-itb instances in Indianapolis)
      * Practice run for moving TWiki and OIM later in August
   * GOC hosted Glidein [[http://tinyurl.com/3hymbr4][monitoring]] in place on MyOSG-ITB

---++ Engage (Mats, John)


---++ Integration (Suchandra)


---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.20
      *       96 (1) OSG 1.2.X resources (      12 are 1.2.20)
      *        2 (1) OSG 1.0.X resources (       0 are 1.0.6)
      *        2 (0) OSG 1.0.0 resources
      *        0 (0) OSG 0.8.0 resources
Site coordination meeting this week, 8/4
   * Preparation for the Summer workshop
      * Agenda presentation
      * Requirement discussion
   * Trash/Trash/SiteCoordination.SitesCoord110804
   * Phone: # 866-740-1260, code 8349885#
   * Adobe connect: http://osg.acrobat.com/osgsc110804/ 
OSG Summer workshop, Aug 9 and 10 2011
   * https://indico.fnal.gov/conferenceDisplay.py?confId=4531
   * https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/SiteCoordination/SummerWorkshopTutorials2011


---++ Virtual Organizations Group (Chander)


---++ Security (Mine)
   * No major security incidents or vulnerabilities. 
   * Follow up work with Canadian incident. We will find out how many site admins checked the announcement. We will call a few sites to understand how they interpret the announcement. 
   * New CA caches: all tests passed in ITB. We are ready for moving to production next week. One important point: we did not test rpm packages and yum/apt-get methods in ITB. We tested the vdt scripts with CA tarballs in ITB. Since we will control which sites we will transition first, we will start with sites that are using vdt scripts and the ca tarballs. In the meantime, we should work with GOC to create a new rpm packages for the new ca caches. 
   * There was a test request from doegrids: doegrids adding multiple crl locations int their distribution points. We were ready for the tes last week, but we could not get teh new crl_url file and a sample end user cert from DOEgrids. We could not get a response from Mike or Dhiva. Maybe they are on vacation. 
   * OSG Summer forum. Anand will attend and present at users forum. 

---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

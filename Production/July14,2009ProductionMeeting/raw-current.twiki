-- Main.DanFraser - 08 Jul 2009
---++ Attendees (to be updated after meeting):
   * Xin, Armen, Britta, Mats, Brian, Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Miron, Dan

---++ Questions from the Executive Director
   * At the Council meeting today US ATLAS (Michael E, Richard M)  reported on scalability issues with the Storage Elements during Step09 and the support of the  analysis jobs that they are pushing through the Tier-2s. Can we hear about this at the production meeting please
      * One problem: when analysis jobs compete with storage jobs for tape access, there can be major bottlenecks. Will continue the discussion on the next production call. 
   * Do we have a canonical list of US LHC Tier-3s and which are now accessible to OSG? Is there a list of &quot;expected&quot; additions over the next 12 months please? It would be very useful to be able to get a list of Tier-3s easily from the GOC. 
      * Will follow up at the next T3 Liason meeting. (Dan)
   * I understand the GEO code is a code shared between VIRGO and LIGO; For this and the E@H do we understand the expectations for published results  and how we support achieving them in the needed time frame?
      * According to Britta/Kent, GEO and E@H use the same code base although some modifications have been made from the original German E@H to support GRAM2. There is an agreement in place to process VIRGO data as part of the E@H effort. 

---++ CMS (Burt)
   * We ran about 140 khours/day last week. We are still running &quot;skim jobs&quot; at the Tier 1 (and worldwide -- glideinWMS sustained 10K concurrent jobs with ~100 Kjobs/day).  CPU/wallclock was 50% last week (65% excluding skims).
   * Step09 Postmortem: I haven&#39;t reviewed it yet
   * Storage: No numbers yet -- Tier 1 gratia data is still catching up! (We&#39;re up to June 30 now..)
   * Concerns about RSV 3: it does not seem yet ready for production 

---++ Atlas (Armen &amp; Xin)

   * Reprocessing tasks of the last cosmic run are done at BNL T1. General production stays stable at 5~6 k running jobs. 
   * job statistics for last week. 
      * Gratia report: USATLAS ran 655K jobs, with CPU/Walltime ratio of 86%
      * PanDA world-wide production report (real jobs):
         * completed successfully 676K managed MC production, validation and reprocessing jobs
         * average  ~96K jobs per day
         * failed 163K jobs
         * average efficiency: 81% for jobs and 90% for walltime

---++ LIGO (Britta)

* ITB validation: 
   * E@OSG 
      * validated at: LBNL, FERMIGRID, BNL, UC_ITB
      * LIGO_CIT in progress, CIT_ITB_1 needs admin attention
   * BINARY INSPIRAL
      * No site validated
      *  CIT_ITB_1, UC_ITB, LIGO_CIT need site admin attention
      *  FERMIGRID in progress
      *  LBNL down, TS in progress
      * BNL_ITB TS in progress


* Gratia Reports:
   * Current week&#39;s total usage: 3 users utilized 17 sites;
      * 9491 jobs total (8438 / 1053 = 88.9% success)
      * 34250.4 wall clock hours total (29988.9 / 4261.5 = 87.6% success);
   * Previous week&#39;s total usage: 3 users utilized 17 sites;
      * 8282 jobs total (7582 / 700 = 91.5% success);
      * 39502.6 wall clock hours total (35494.3 / 4008.3 = 89.9% success);
   
   *  E@H statistics: 
      * Recent Average Credit (RAC): 170,393.37228 ; Last week: 152,378.43727
      * E@H rank based on RAC: 7 (-1)
      * E@H rank based on accumulated Credits: 25 (+0)

   * GEO600-1.3.x code
      * Fully migrated to new code
      * Running at 17 sites

   * Details 
      * Remaining problems at:
         * gpn-husker ? ticket open, I am trouble shooting
         * MIT_CMS, GOC ticket open, waiting for e-mail address to organize telecon
         * SBGrid-Harvard-Exp: (Condor) Error in submit file
         * UTA_DPCC deployment fail: svn
         * Purdue_Caesar
         * UCLA_Saxon_Tier3 - no out gooing network connections
         * UCSDT2 (?) app does not start
 
---++ Integration (Suchandra)
   * Almost done with service validation
      * Waiting for cache update to test rsv fixes
      * Waiting to test gratia changes as well
   * Currently going through vo validation process
      * Have green flag from DZero
      * Waiting for responses and testing from other VOs
   * Will try to complete VO testing in next 10 days
    

---++ Site Coordination (Marco)
   *Site Administrators telecom on Thursday 7/23 (ITB slot):
      * Adobe Connect test
      * brief on OSG 1.2
      * presentation of sessions and requirements for OSG Site Administrator Workshop
   * Site update status (from !MyOSG):
      * Label differ &quot;OSG-x.y&quot; &lt; 1.0.1, &quot;OSG x.y&quot; after
      * Most recent version is OSG 1.0.4
      * 1 OSG 1.1.4 resource
      * 36 OSG 1.0.X resources (18 are 1.0.4)                                                                                                                                                                  
      * 50 OSG 1.0.0
      * 5 OSG 0.8.0
      * 0 OSG 0.6

---++ Engagement (Mats)


12 users utilized 27 sites;

42573 jobs total (38421 / 4152 = 90.2% success);

152976.9 wall clock hours total (129980.5 / 22996.4 = 85.0% success);


Expired proxy caused some job failures.

$OSG_APP discussion is ongoing. Many different views - will probably not
be resolved (at least not anytime soon). Trash/Engagement will build on compute
nodes, installing into $OSG_APP. Sites which mount $OSG_APP read only
will be ignored. Sofware availablity is advertised so this should not
affect failure rates.


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)

   * OSG 1.2 pre-release validation by VOs -- ALICE, ATLAS, CDF, CMS, DES, D0, DOSAR, Engage, Fermilab-VO, LIGO, nanoHUB, NYSGrid, SBGrid, and STAR are participating.

      * URL: https://twiki.grid.iu.edu/bin/view/Trash/ReleaseDocumentationSiteValidationOSG12#VO_validation   
      * Dzero has successfully completed validation and has given a green flag.
      * LIGO, nanoHUB, Engage-VO are updating the feedback pages. Site issues are being actively tracked and resolved.
      * ALICE, CDF, Fermilab-VO, SBGrid have started and have plans to give more updates soon.
      * STAR is looking to start as soon as effort is available.
      * NYSGrid specifics are needed, and awaited.
      * DOSAR has completed a first round of validation and is looking to retry.
      * DES is lacking in effort, and may eventually give a green flag based on peers evaluation.
      * ATLAS validation is in progress. CMS is getting ready. Recommendation: For LHC preparedness, it may be beneficial to also validate chain components. E.g., RSV, GIP, CEMon, BDII, SRM-client.   

   * !IceCube -- Goal is to start a gradual scale-out from GLOW to other OSG sites. Had a technical meeting with !IceCube team and Tanya after last week&#39;s VO forum. Storage requirements becoming clearer now. !IceCube&#39;s needs are more than the capacities of SQUID and OSG_DATA, but considerably less to introduce SRM overhead. Starting to evaluate using glideinWMS and possibly SRM at UCSD site. In contact with GLOW team, and will get Miron&#39;s input.

---++ Grid Operations Center (Rob Q.)

---+++ Operations This Week
   * *Request to site-administrators*: [[http://osggoc.blogspot.com/2009/07/mportant-please-restart-tomcat-on-your.html][Please restart tomcat on your CEs]] - we are still seeing several CEs not reporting to is1.grid.iu.edu - note that removal of temporary rsync from is2 could potentially remove your CE from the BDII!
   * Carry-forward from last week: Yum repository network reconfiguration - very ephemeral outage expected (mid week)
   * We will be testing contact information update reminders for OIM with select testers.
   * BDII Issues on Wednesday July 8th. [[https://twiki.grid.iu.edu/bin/view/Operations/BDIIRootCauseAnalysis][Root Cause Analysis]]

---+++ Future Events
   * VORS Turn Down Scheduled for August 3rd - If you know anyone who uses VORS, please remind them.
   * August 6th and 7th - Site Administrators Meeting in Indianapolis
   * September Machine Room Move in Bloomington September 19 2009 - All GOC Services in Bloomington will be down. IUPUI will still be handling BDII traffic. 

---++ Security (Mine)

-- Main.MarciaTeckenbrock - 05 Nov 2010

---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * The CMS T1 noted that when they tried to upgrade to the latest OSG CE version 1.2.15, CEMon failed to publish data so they failed back to 1.2.8. Burt is working with Parag to try and resolve this. 
   * There was an Atlas critical alarm at about 3:30 am on Sunday morning against the T1 dCache system. In addition to getting the problem resolved (in two hours), this demonstrated that the emergency communication channels are working properly. (Rob)
   * Atlas continues to have issues with the CERN BDIIs and Sam BDIIs where BNL (and some Atlas Tier-2) data is stale or unavailable. It is unclear if this problem is related to case sensitivity issues or another problem altogether. The data appears fine in the OSG BDIIs. Action is for Dan to start an email activity with the CERN BDII developers to help track down the problem. In parallel, Rob is planning to examine the &quot;-debug&quot; output from the BDIIs that will show when case sensitive issues appear. 
   * A new BDII (IS3) running v5 was added in parallel to the production BDIIs according to the transition plan; it will not added into the BDII round robin until OSG has the all clear from Atlas and CMS. There was an issue however where IS3 stopped accepting new entries after a few hours and needed to be restarted. The GOC team is investigating this problem with the developers. (Rob)
   * On Nov 4 &amp; 5 LIGO was running a significant number of jobs on the CMS T1, but Gratia reported no activity on the T1 for this period. Dan to follow up with the Gratia team. 

---++ Attendees:
   * Xin, Armen, Robert E., Suchandra, Burt, Marco, Rob Q., Scott T., Chander, Dan
 
---++ CMS (Burt)
   * LHC: Now running Pb+Pb, on schedule.
   * Tier 1 in all-day downtime yesterday
   * 323 khour/day, 89% success

---++ Atlas (Armen &amp; Xin)

   * General production status
      * LHC moved to heavy ion physics last week. Preparations started Thursday, and the first collision over the weekend - very fast and smooth transition. Since yesterday stable data-taking. ATLAS data reprocessing campaign moving quite nicely, without major problems. Production at US is stable at the level of ~10k running jobs. T1(s) are doing the data reprocessing, and T2(s) at the moment are finishing Geant simulation, and later will move to reprocessing of those simulated samples. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.4M jobs, with CPU/Walltime ratio of 85%. 
      * Panda world-wide production report (real jobs): 
         * completed 1M managed/group MC production, validation and reprocessing jobs.
         * average 142K jobs per day
         * failed 95K jobs
         * average efficiency: 
            * jobs     - 91%
            * walltime - 95%
      * Failure rate was high on Monday and Tuesday, probably from the power outage at BNL.  
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was 400~500TB/day.
   * Issues
      * BDII issues : GOC ticket 9496 (stale info) and 9526 (SAM BDII missing BNL SE info)

---++ LIGO (Britta, Robert E.)


---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * [[http://tinyurl.com/27fknc6][Reliability/Availability of GOC Services]]
   * [[http://tinyurl.com/35zl55c][Reliability/Availability of Security Services]]
   * ITB release [[http://osggoc.blogspot.com/2010/11/goc-service-update-tuesday-november-9th.html][Release notes]] are available.
   * Second top level BDII at the GOC meeting, a [[http://mypage.iu.edu/~steige/rept.pdf][document]] to be presented at the blueprint meeting exists. 
   * dCache on ATLASDATADISK and ATLASMCDISK, failed ~2:45 EST, see ticket [[https://ticket.grid.iu.edu/goc/viewer?id=9522][9522]] for details.
   * BNL-ATLAS is missing from SAM-BDII, see GGUS ticket [[https://gus.fzk.de/ws/ticket_info.php?ticket=64039][64039]] or GOC ticket [[https://ticket.grid.iu.edu/goc/viewer?id=9527][9527]] 
   * Gratia and !ReSS (Represented by Fermigrid Ops) - No issues

---+++ Operations This Week
   * Production Release ([[http://osggoc.blogspot.com/2010/11/goc-service-update-tuesday-november-9th.html][Notes]])
   * is3 will rsync with backup, effectively adding 3rd server to GOC BDII, testing by hearty users can begin.
      * This will not be added into the Production RR until CMS/ATLAS have approved it. 
   * Rob Q., Scott T. at the blueprint meeting, Tue., Wed. Both back in the office Thu. and available for emergencies. 
   * Gratia and !ReSS
      * Kernel, VDT and Condor update on ReSS machines, 11/10/2010, High availability will keep services available throughout.


---++ Engage (Mats, John)

11 users utilized 43 sites;

30165 jobs total (26552 / 3613 = 88.0% success);

140315.7 wall clock hours total (126794.9 / 13520.8 = 90.4% success);

Trash/Engagement infrastructure issues with disk space. Not very serious, but a lot of jobs had to be restarted.

ExTENCI team members (running under the Engage VO) have opened a set of tickets against sites. Some issues came from multi-VO membership and sites using edg-gridmap. 


---++ Integration (Suchandra)
   * About to start new test cycle (OSG 1.2.16)
      * [[http://vdt.cs.wisc.edu/releases/2.0.0/release-p23.html][Changes]]
      * Moving to VTB soon 
      * Would like to get release before thanksgiving holidays
   * Testing RSV pre-release for OSG 1.2.17

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.14
      *       95 (2) OSG 1.2.X resources (      14 are 1.2.14)
      *        4 (0) OSG 1.0.X resources (       0 are 1.0.6)
      *        4 (0) OSG 1.0.0 resources
      *        0 (-1) OSG 0.8.0 resources
This week 11/11 there will be a site coordination meeting
   * SiteCoordination.SitesCoord101111
   * Phone: # 866-740-1260, code 8349885#
   * Adobe connect: http://osg.acrobat.com/osgsc101111/ 


---++ Virtual Organizations Group (Marcia)

---+++ ALICE 

   * There had been some GIP issues (accounting info may be understated) that needed to get resolved. A solution was put forth by NERSC, but Jeff Porter will find out whether this has been implemented.

---+++ CDF

   * CDF has been running production jobs at KISTI, making good progress; KISTI is going to add another hundred nodes dedicated to CDF.
   * However, the Running Experiments group at Fermilab reported glidein problems at the site. Stephan Lammel has asked Rick to run stress tests before KISTI can be reinstated. [General question: Are stress tests for !FermiGrid only or for OSG in general?]
   * Chander will facilitate a meeting including Gabriele, Steve Timm, Igor, and both CDF teams to help solve various issues.
 
---+++ General

   * Security team is seeking input from !VOs for Galois on needs and problems document (See http://grid2.galois.com/needs-and-problems.html




---++ Security (Mine)

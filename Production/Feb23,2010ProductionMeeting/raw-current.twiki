-- Main.DanFraser - 09 Feb 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * Tier 1 opportunistic limit at 7100 jobs but opportunistic usage will drop as large global workflows begin next week (Burt)
   * D0 in discussion with UTA on opportunistic storage (Xin)
   * LIGO currently running at rank 1 but as graphs show below this is largely due to the Tier-1; production is lower on other sites as they are starting to receive higher priority workloads. (Rob E.)
   * First part of Gratia addition of higher capacity processing nodes (in parallel) was successful; Thursday is the planned switch-over to the new nodes exclusively (Rob Q).
   * Fermi-VO lost some integration and testbed services as a result of the power outage. These will be moved to other facilities at FNAL. CDF production was affected by FNAL power outages. (Abhishek)
   * D0 jobs having problems with resolving $OSG_WN_TMP. (Abhishek, Brian)
   * SBGRID having difficulties at 4 large sites -- UFL, Purdue/RCAC, Buffalo/CCR, Caltech/Ultralight. &quot;Either due to the type of error or our submission configuration we only get back the Globus error code (17), and no other details.  This makes it virtually impossible to debug.&quot; (Abhishek)


---++ Attendees:
   * Xin, Armen, Britta, Rob E., Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Dan
 
---++ CMS (Burt)
   * Computing: 70 khour/day, 87% success.
   * Facilities: power issues at the Tier 1, again
   * Tier 1 opportunistic limit at 7100 jobs; but
   * Large global workflows begin next week -- opportunistic usage will drop

---++ Atlas (Armen &amp; Xin)

   * General production status
      * Reprocessing of real and simulation data is now finished. Merging jobs are still running, in addition to simulation jobs. Overall production was at the level of 3k running jobs. Distribution of the reprocessed datasets is in progress. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2.4M jobs, with CPU/Walltime ratio of 67%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 756K managed MC production, validation and reprocessing jobs 
         * average 108K jobs per day
         * failed 128K jobs
         * average efficiency:  jobs  - 86%,  walltime - 87%
   * Data Transfer statistics for last week
      * BNL T1 data transfer is about the same as previous weeks, &gt;100TB/day average, peaking ~200TB/day. 
   * Issues and GOC Tickets
      * Opening more USATLAS T2 sites to D0 VO as opportunistic storage:  Mark from USATLAS and Joel from DZERO are in contact now.

---++ LIGO (Britta, Rob E.)

---++++ Weekly OSG Efficiency by VO for 21 February 2010

|  *Njobs*  |  *Delta*  |  *Wall*  |  *Delta*  |  *CpuToWall*  |  *Delta*  |  *%Effi*  |  *Delta*  |
|    88,732 | 21,928  | 1,033,550.7 |    343,986  |      0.60      |     -0.12 |    60  |   -12  |


   * Current week&#39;s total usage: 6 users utilized 37 sites;
      * 88953 jobs total (53806 / 35147 = 60.5% success);
      * 1058062.6 wall clock hours total (882427.1 / 175635.5 = 83.4% success);
   * Previous week&#39;s total usage: 4 users utilized 37 sites;
      * 64980 jobs total (45351 / 19629 = 69.8% success);
      * 713487.7 wall clock hours total (620271.2 / 93216.5 = 86.9% success);

---++++ LIGO / INSPIRAL

   * Just returned from SciMon duty at LIGO Hanford observatory
   * Learned how to obtain s6 data in almost real time (latency: 20 mins) and other detector related techniques

---++++ LIGO / E@H

   * production levels still very low on some major resources during weekdays:
      * !GridUNESP_CENTRAL (down due to switch problem )
      * BNL_ATLAS_1 ( up to 40 jobs running )
      * BNL_ATLAS_2 ( up to 40 jobs running  )
      * UFlorida-HPC ( -75%, due to high priority jobs )
      * LIGO_UWM_NEMO ( -75%, due to high priority jobs )
      * Purdue-RCAC ( -75%, due to high priority jobs )
      * prairiefire ( -90% )
   * production level up on:
      * USCMS-FNAL-WC1-CE3 ( increased optimistic resource limit by 2k)
      * Firefly

&lt;img align=&quot;left&quot; src=&quot;http://boincstats.com/charts/chart_uk_bo_object_new_users_1789660.gif&quot;/&gt;

&lt;BR&gt;

&lt;img align=&quot;left&quot; src=&quot;http://t2.unl.edu/gratia/bar_graphs/facility_hours_bar_smry?vo=ligo&quot;/&gt;
      

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Last week&#39;s availability metrics
      * [[http://tinyurl.com/ycogjev][GOC Services: BDII, MyOSG, RSV Collector, OSG Display]] 
      * [[http://tinyurl.com/yaan8zf][GOC hosted Security services managed by OSG security team]]
   * [[http://osggoc.blogspot.com/2010/02/new-release-113-of-ca-certificates.html][OSG CA Distribution 1.13]] released on Feb 18th 2010
      * Also [[http://osggoc.blogspot.com/2010/02/osg-ca-distribution-release-112-goc.html][OSG CA Distribution 1.12]] released on Feb 16th 2010
   * [[http://osggoc.blogspot.com/2010/02/osg-ticket-exchange-11-bug-patched.html][GOC-TX 1.1 - Bug Fix release]] - Fixed bug that was causing initial assignment update by GOC staff on a ticket assigned to GGUS to not show up. ([[https://ticket.grid.iu.edu/goc/viewer?id=8114#1265985224][An example of the problem]] - this update did not make it to [[https://gus.fzk.de/ws/ticket_info.php?ticket=55511][GGUS ticket]])
   * [[http://osggoc.blogspot.com/2010/02/upcoming-gratia-maintenance.html][Gratia Maintenance - Part 1]]
      * Not related to RSV collector hosted by the GOC, which remains unaffected!
      *  IT took longer than thought because of power outage. Other than that they didn&#39;t have any issues.
   * [[http://osggoc.blogspot.com/2010/02/fnal-power-outage.html][FNAL Power outage]] - and follow up notifications: [[http://osggoc.blogspot.com/2010/02/fnal-power-outage-update.html][Update 1]], [[http://osggoc.blogspot.com/2010/02/osg-docdb-access-problem.html][Update 2]]
      * All production services are up. ITB is off line working to bring back up. Not all the storage services up. They are at reduced capacity. ETA of back on-line is not ready at this time. Please let us know when UCMS tier 1  when its back to full capacity.

---+++ Operations This Week
   * [[http://osggoc.blogspot.com/2010/02/upcoming-gratia-maintenance.html][Gratia Maintenance - Part 2]]
   * *GOC-TX*
      * With USATLAS RT: Close to being ready to move to new system.. Still no ETA, though, yet on the move because of production upgrade freeze
      * With FNAL Remedy: No updates
   * *GOC Debian repo*
      * LIGO testing was succeess
   * New VO Package with FNAL Changes, DayaBay addition and port changes for GRASE and NYSGRID VOMS will be sent to ITB later today.

---+++ Future Events
   * Next GOC Services Update Tentatively schedule for ITB March 23rd and into Production on March 30th 


---++ Engage (Mats, John, Chris)


---++ Integration (Suchandra)
   * OSG 1.2.7 release
      * Still in testing and waiting for ATLAS validation to finish
   * ITB testing infrastructure
      * Added 2 new sites to queue, 2 sites being worked on
      * Testing glexec on FNAL 
      * Gathering statistics on pilots and sample jobs
   * Documentation
       * Currently pruning documentation of old and out of date pages
       * Will discuss further and start looking at pages during ITB meeting on thursday


---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.6
      *       70 OSG 1.2.X resources (      16 are 1.2.6)
      *       10 OSG 1.0.X resources (       2 are 1.0.5)
      *       11 OSG 1.0.0 resources
      *        2 OSG 0.8.0 resources
         * OU_OCHEP_SWT2, tier2-01.ochep.ou.edu , Contact: Horst Severini
         * UIC_PHYSICS mstr1.cluster.phy.uic.edu , Contact: John Wolosuk
Site Coordination meeting this Thursday 3/4 at 11am central 
   * This time one the first week because of the All Hands meeting the following week
   * Phone: 510-665-5437, #1212
   * Adobe connect: http://osg.acrobat.com/osgsc100304/
   * Tentative agenda
      * Final planning for the joint Tier 3 session (Rob Snihur)
      * discuss the CRL problem: http://osgsec.blogspot.com/  (Igor Sfiligoi)
      * Other security report/announcements
      * New sites (specially Tier3) 

---++ Virtual Organizations Group (Abhishek)

   * D0 
      * Average 6 M Events/week; 40,000 wall-hours/day at 73% efficiency. 
      * Recovery from previous week&#39;s FCC/FNAL power outage completed around last Thursday.
      * In discussion with Mark/UTA on opportunistic SE access.

   * CDF 
      * Affected by power outage. Recovery hopefully complete now.
   
   * Fermi-VO 
      * Grid service-nodes up quickly after outage. Production resumed.
      * Trash/Trash/Integration and testbed nodes may remain offline for extended period.
      * Running at reduced power capacity; worker-nodes not affected.
      * New sub-VO forming this week: Muon Accelerator program. 
      
   * ALICE
      * Visited CERN to discuss setting up NERSC/LLNL site. 
      * Funds expected to arrive in summer. Getting ready to ramp-up again. 
      * Testbed up and functional. 
      * Hardware scheduled to arrive around March 1st, then will run scaling tests with OSG-AliEn interface.
      * Focus will be to test scalability of VO-Box operations on OSG.
      
   * !GlueX 
      * Working with Richard Jones to get jobs from other VOs on the !GlueX site UConn-OSG. 
      * Have asked D0, SBGrid, NYSGrid. 
      * Local CE and SE are now stable and running a steady stream of local jobs.
      * Some pending issues; status to be confirmed this week.
         * $OSG_LOCATION/gratia/var/logs directory filled up.
         * Current GIP status on !MyOSG is unclear.
         * Need jobs from more VOs. =Resolved=
         * Weekly Gratia report for UConn-OSG site incorrectly shows 0 wall hours consumed.
      * D0 jobs having problems with resolving $OSG_WN_TMP. Brian &amp; Abhishek in discussion with Joel &amp; Richard.

   * !GridUNESP
      * Problem with local switch in main cluster which has caused problems with LIGO jobs. 
      * Working with vendor. 
      
   * !NYSGrid 
      * Looking to upgrade current infrastructure over the summer from 13TF to 40-60TF. 
      * Made progress in setting up &quot;hub/portal&quot; with !HubZero and getting access through hub to !BlueGene resources at Stonybrook. 

   * SBGrid
      * Using MatchMaker as the primary mode. Submission rate currently moderate, ~1200 jobs.  Had a peak of ~3000 simultaneous jobs; success rate 30-50%. 
      * Current issues pointed out by SBGrid:
         * &lt;i&gt;&quot;Many OSG sites still have issues with either *rsync* or *curl*.  Since staging in files with every job is a bad idea (so we&#39;ve been told), whether using Condor mechanisms or !GridFTP, we are instead attempting *either* to pre-stage data using rsync for common binaries and data, *or* curl within a job to fetch data from a web server.  We&#39;ve had problems with rsync not being available or with the process of updating files in $OSG_{APP,DATA}/sbgrid via rsync.  With curl, in many cases an old version is the first in the path and it doesn&#39;t support options we require, *or* we get curl 18 errors (interrupted transfers).&quot;&lt;/i&gt;
         * &lt;i&gt;&quot;4 large sites -- UFL, Purdue/RCAC, Buffalo/CCR, Caltech/Ultralight -- are all failing to run our jobs.  Either due to the type of error or our submission configuration we only get back the Globus error code (17), and no other details.  This makes it virtually impossible to debug. have not entered a GOC ticket for this.  A google search does not show up any good explanation for what a Globus error code 17 is.&quot;&lt;/i&gt;
            * This can be related to condor configuration not being found. Have asked SBGrid to check $CONDOR_CONF.
         * &lt;i&gt;&quot;Many sites do not catch the exit code of the job executable and return it properly.  This means that many failed jobs are not retried, if they run at such a site.  I can implement &quot;post&quot; scripts that will check for this, but really this is not fun and does not seem like the right solution.  I have tried to enter GOC tickets for sites where I&#39;ve noticed this, but it is a bigger issue that, to me at least, seems like it should be monitored by OSG centrally and sites that don&#39;t do this properly should be pushed by OSG to fix themselves.&quot; &lt;/i&gt;
 

---++ Security (Mine)

   * Testing the new IGTF distribution against our CA package generation scripts. We will make a plan on how to test in ITB against VDT stack. We are far from that yet
  

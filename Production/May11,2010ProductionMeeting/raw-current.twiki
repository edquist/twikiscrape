-- Main.DanFraser - 30 Apr 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * There was a BDII outage (caused by a CEMON collector failure) that affected the visibility of the CMS T1 and other sites last Tuesday evening. After hours response from the GOC team was excellent. Primary causes are still being investigated although it looks to have been caused by a bug in NSCD. Also exploring monitoring requirements to detect future failures in the BDII end-to-end architecture. The failed BDII node has temporarily been taken out of the system for diagnostics. More details below in Burt and Rob&#39;s reports. (Rob, Burt)
   * A draft report from the Root Cause Analysis of the Gratia problem detected by the OSG Display on Apr 20 is being circulated and iterated on. Plan to have a final report available soon. (Dan, Rob, Brian)
   * Dan noted that error rates in the number of CMS wall clock hours seem abnormally high (sometimes ~50%). Brian noted that it is difficult to determine if this represents a real problem, since pilot jobs increase the difficulty in tracking errors. Brian investigating.
   * In preparing for todays production meeting Dan realized that he forgot to send out last weeks production report. It is now available at https://twiki.grid.iu.edu/bin/view/Production/May4%2c2010ProductionMeeting.

---++ Attendees:
   * Mats, Xin, Armen, Brian, Burt, Abhishek, Rob Q., Mine, Dan
 
---++ CMS (Burt)
   * Up to 2.6 inverse nanobarns inverse luminosity.  (Still only colliding 2x2 bunches)
   * Job statistics for last week: 168 khours/day, 66 kJobs/day, 77% success 
   * Operations were adversely affected by BDII outage.  Tier 1 and Tier 2s have staggered update times so that older CEMon releases will be robust against hanging collector
   * Dialog will continue on making SLA definitions more clear

---++ Atlas (Armen &amp; Xin)

   * General production status
      * ATLAS production was at the average level of 7-8k running jobs. Mainly were simulation jobs. Production load is down now in preparation for the second round of reprocessing. Over the week data distribution from last reprocessing was going on. LHC had commissioning over the weekdays last week. Also some small technical problems. Good luminosity over the weekend and yesterday.
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.7M jobs, with CPU/Walltime ratio of 83%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 843k managed MC production, validation and reprocessing jobs 
         * average 120K jobs per day
         * failed 111K jobs
         * average efficiency:  jobs  - 88%,  walltime - 92%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate is 300~500TB/day last week. 
   * Issues
      * Opportunistic SE usage for D0 : no updates from D0 test 

---++ LIGO (Britta, Rob E.)
--+++ Gratia Reports
   * Last week&#39;s total usage: 4 users utilized 40 sites
      * 376279 jobs total (125852 / 250427 = 33.4% success)
      * 2649282.3 wall clock hours total (2166518.3 / 482763.9 = 81.8% success)
   * Current week&#39;s total usage: 5 users utilized 35 sites
      * 62680 jobs total (26174 / 36506 = 41.8% success);
      * 895682.0 wall clock hours total (672090.6 / 223591.4 = 75.0% success);
---+++ LIGO / E@H
   * Recent Average Credit (RAC):  1,288,948.61025, Last week: 1,574,770.42532
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0) 

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
 
   * Availability metrics for the last week 
      * [[http://tinyurl.com/2fnlv2z][GOC Services: BDII, MyOSG, RSV Collector, OSG Display]]
      * [[http://tinyurl.com/24hmhb7][GOC hosted Security services managed by OSG security team]]
   * !CEMon collector on is2.grid.iu.edu died on Tuesday evening. This led to issues with BDII resources appearing in both BDIIs. [[http://osggoc.blogspot.com/2010/05/some-osg-ces-facing-intermittent-cemon.html][Notification]]
      * [[https://ticket.grid.iu.edu/goc/viewer?id=8530][Ticket Details]]
      * Issues found with NCSD using CPU time.
   * GOC Services ITB Release on May 4th 
      * [[http://osggoc.blogspot.com/2010/05/goc-service-update-tuesday-may-11th-at.html][Change Log]]
      * [[http://tinyurl.com/24nxbpf][Top Level BDII Monitoring]]
      * Mostly minor bug fixes

---+++ Operations This Week

   * Scheduled Maintenance of is2
      * is2.grid.iu.edu will be removed from RR and diagnosis began this morning. [[http://osggoc.blogspot.com/2010/05/scheduled-maintenance-is2gridiuedu-goc.html][Maintenance Notification]].
   * GOC Services Production Release on May 11th
      * [[http://osggoc.blogspot.com/2010/05/goc-service-update-tuesday-may-11th-at.html][Change Log]]
      * Mostly minor bug fixes
   * CERN Closed for Holiday Thursday and Friday


---++ Engage (Mats, John, Chris)

12 users utilized 33 sites;

34354 jobs total (32432 / 1922 = 94.4% success);

113542.8 wall clock hours total (48413.6 / 65129.3 = 42.6% success);

The low success rate is due to a failed SE at Duke, which a lot of long running jobs tried to write to at the end of the jobs. When the writes failed, so the the jobs. Duke is working on repairing the SE, and the jobs have been stopped until the repairs are completed.

I will not be able to make the call today due to a conflict.


---++ Integration (Suchandra)
   * Continuing work on ITB Robot enhancements
   * Also working on closing existing ITB trouble tickets (went from 33 to 22 over the last week) and feature requests
   * Waiting the VDT changes for xrootd and bestman before VTB testing will start

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.9
      *       80 (1) OSG 1.2.X resources (       7 are 1.2.9)
      *        6 (0) OSG 1.0.X resources (       1 are 1.0.6)
      *        7 (1) OSG 1.0.0 resources
      *        1 (0) OSG 0.8.0 resources
Site Coordination meeting this Thursday 5/13 at 11am central
   * Phone: 510-665-5437, #1212
   * Adobe connect: http://osg.acrobat.com/osgsc100513/ 
   * Special topic will be Job workflows in OSG with:
      * Parag talking about Glidein WMS
      * Jose talking about Panda


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)

   * D0
      * D0 monte-carlo event production at 7.8 million events per week.
      * Nearly 70,000 wall-hours/day at 80% efficiency.
      * Sites related issues:
         * Nebraska and Purdue SE&#39;s gave very low efficiency last week.  
         * Retrying UTA SE this week; heard back from Cornell SE, waiting for more details. 
      * Correction in last week&#39;s wall-hours consumption: 
         * Actual: 120,000 wall hours/day at 87% efficiency ~ 13.3 Million Events/week.
  
   * GLUE-X 
      * Site has nearly 384 cores; half capacity occupied with local users, half OSG. Running at capacity.
      * GLUE-X is interested in offering opportunistic storage to other VOs.
         * D0 will give it a try.
      * Pointed out 2 bugs: in !MyOSG and GIP. Abhishek working to reach closure.
         * https://ticket.grid.iu.edu/goc/viewer?id=8343
  
   * !SBGrid
      * Team on travel this week. 
      * Starting to investigate !GlideinWMS. Continuing to use OSG/Engage !MatchMaker for production.


---++ Security (Mine)

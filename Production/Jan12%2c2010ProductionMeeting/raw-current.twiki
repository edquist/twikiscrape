-- Main.DanFraser - 05 Jan 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * Current CMS queue of physics simulation results fulfilled.  Enjoy your opportunistic cycles for now! (Burt)
   * Non-CMS/CDF preemption issue @ MIT_CMS: second suggested fix now implemented. Tickets will now be closed. (Burt)
   * GOC ticket 7772: Issues with WLCG BDII periodically loses information about some USATLAS Tier2 sites: continue to run the probe, see occasional info missing on both OSG and EGEE BDII server. Rob Q &amp; Dan to figure out next steps to follow up on this. (Xin, Rob, Dan)
   * 20 sites running VDT 1.0.0 or previous need to be reconfigured to point to the current gratia service address. List appended at bottom of report. Plan to file tickets making the change requesting that the sites either make a pointer update or a VDT upgrade (recommended). (Chris, Dan F, Rob Q)
   * LIGO is now ramping up again after implementation of code changes to improve error handling (Rob E)
   * SBGrid made several changes and recorded a new peak of 1000+ simultaneous jobs, with 20,000 wall hours/day. (Abhishek, Mats, Dan)
   * !GridUNESP / DOSAR has requested the GOC to make new release of VO package (Abhishek, Rob)

---++ Attendees:
   * Chris, Mats, Xin, Armen, Britta, Rob Ee, Suchandra, Burt, Marco, Abhishek, Rob Q., Chander, Dan
 
---++ CMS (Burt)
   * Computing: 114 khour/day, 92% success.  CPU/wallclock at 74%.
   * Storage: 726 TB xfer (T1), 66 TB (others)
   * BDII sync issues at EGEE continue
   * Current queue of physics simulation results fulfilled.  Enjoy your opportunistic cycles for now!
   * May be running some intense skim runs at the Tier 1 over the next week
   * Non-CMS/CDF preemption issue @ MIT_CMS: second suggested fix now implemented.  We consider this work done.

---++ Atlas (Armen &amp; Xin)

   * General production status
      * During the last week USATLAS production was quite stable at the level of 8~10K running jobs, mainly MC jobs. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.8M jobs, with CPU/Walltime ratio of 81%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 648K managed MC production, validation and reprocessing jobs 
         * average 93K jobs per day
         * failed 44K jobs
         * average efficiency:  jobs  - 94%,  walltime - 94%       
   * Data Transfer statistics for last week
      * Transfer rate stays the same as previous weeks. BNL T1 transferred ~75 TB/day data last week, with peak at 150 TB/day.  
   * USATLAS sites are asked to finish the upgrade to SL5 and OSG 1.2 by the end of January. 
   * Issues and GOC Tickets
      * GOC ticket 7772: Issues with WLCG BDII periodically loses information about some USATLAS Tier2 sites: continue to run the probe, see occasional info missing on both OSG and EGEE BDII server. 
      * Opening more USATLAS T2 sites to D0 VO as opportunistic storage:  trying to contact Joel from D0...

---++ LIGO (Britta)

  * Gratia reports:
 
   * Current week&#39;s total usage: 3 users utilized 33 sites
      * 2675 jobs total (2601 / 74 = 97.2% success);
      * 34.9 wall clock hours total (28.8 / 6.1 = 82.5% success);
   * Previous week&#39;s total usage: 4 users utilized 30 sites;
      * 2424 jobs total (2372 / 52 = 97.9% success);
      * 30.4 wall clock hours total (25.7 / 4.7 = 84.5% success);
 
   * E@H reports
      * Recent Average Credit (RAC): 78,298.67015, Last Week: 170,585.60004
      * E@H rank based on RAC: 10 (-7)
      * E@H rank based on accumulated Credits: 13 (-2)
   
   * Robert is ramping up after implementation of code changes to improve error handling
   * Robert is working on code changes required to expand to Fermilab sites and Sprace

 
---+++ Binary Inspiral
    
    * LIGO-Pegasus face-to-face meeting was held yesterday

    * 3 day test work-flow on Firefly: Gap in data error
      * suspected to be bug in LIGO code
      * Duncan Brown was alerted   
      * Test run on a different  3-day data set  succeeded (run time ~ 4 days)
      * One Week work-flow submitted, so far no failed jobs

---++ Engage (Mats, John, Chris)

5 users utilized 38 sites;

116329 jobs total (95672 / 20657 = 82.2% success);

461422.7 wall clock hours total (408150.6 / 53272.1 = 88.5% success);

Waiting for ReSS RSV probe to be finished and deployed.


---++ Integration (Suchandra)
   * OSG 1.2.5 released
   * VTB 1.1.48 released, next testing cycle should be fairly quick
   * Moving forward on ITB panda testing
     * Added two new sites and should have 3 sites using framework for testing before OSG 1.2.6 release

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.5
      *       62 OSG 1.2.X resources (       4 are 1.2.5)
      *       10 OSG 1.0.X resources (       1 are 1.0.5)
      *       18 OSG 1.0.0 resources
      *        2 OSG 0.8.0 resources
         * OU_OCHEP_SWT2, tier2-01.ochep.ou.edu , Contact: Horst Severini
         * UIC_PHYSICS mstr1.cluster.phy.uic.edu , Contact: John Wolosuk
   * Site coordination phone meeting Thursday 1/14, 11:00am Central:
      * Tentative agenga
         * OSG Tier 3 documentation
         * Suggestion to be more supportive (future meetings, activities)
         * Security advisories
      * https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/SiteCoordination/SitesCoord100114
      * Phone: 510-665-5437, #1212
      * Adobe connect: http://osg.acrobat.com/osgsc100114/ 

---++ Metrics (Brian)


---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Last week&#39;s status history 
      * [[http://tinyurl.com/yejwd2j][GOC Services: BDII, MyOSG, RSV Collector]] - NOTE: After delays due to holidays, etc. GOC has fixed minor bug that was causing service availability to be miscalculated in isolated cases.
      * [[http://tinyurl.com/ybo4sjd][GOC hosted Security services managed by OSG security team]]
   * [[http://osggoc.blogspot.com/2010/01/osg-125-release-notice.html][OSG 1.2.5 was released last week]]
   * Relevant to only WLCG Interop sites: SAM team has indicated there was a broker problem between Jan 8th 11:00 and Jan 9th 13:00 (UTC) that may be causing bad LCG availability numbers; GOC will resend RSV records, and SAM team will recompute numbers.
   * On continued examination of the case where sites stopped reporting properly to one of the collectors we&#39;ve decided to wait to hear more from the gratia developers before pinging the 40-some affected sites with tickets.

---+++ Operations This Week
   * *[[http://osggoc.blogspot.com/2009/12/goc-service-update-tuesday-january-12.html][Production release Tuesday - Jan 12th 14:00 UTC]]*
      * OIM 1.12 (Updated footer link for reporting bugs, Updated the !DivRep framework to latest version, Modified certificate requirement verbiage)
      * !MyOSG 1.13 (Updated the footer link for reporting bugs, Removed the invalid link in misc page, Changed the h3 header background - and adjusted CSS accordingly, Set &quot;Now&quot; as the default for status history and links that leads to status history pages, Changed verbiage on Resource Group home page, Fixed the typo reported by Brian on Resource Group filter, Added filter by &quot;Support Center&quot; for VO page ([[https://ticket.grid.iu.edu/goc/viewer?id=7848][related ticket]]))
      * Ticket 1.12 (Updated the h3 background, Updated the footer link for reporting bug)
      * No service outages are expected during this upgrade but the GOC reserves 1 hour for unforeseen problems.
   * GOC continuing to pursue the case of [[https://ticket.grid.iu.edu/goc/viewer?id=7913][some CEs stopping to upload data to central collector]] - according to resident CEMon expert (Parag), it may have been a combination of default configuration and timeouts; expect confirmation in the near future.
      * GOC will add is2 back into DNS RR for is.grid.iu.edu
      * Purdue Ceaser Retired

---+++ Outstanding Tickets
   * [[https://ticket.grid.iu.edu/goc/viewer?id=7772][7772]] - ATLAS BDII Disappearances and BNL Monitoring
   * [[https://ticket.grid.iu.edu/goc/viewer?id=7878][7878]] - MIT CMS Change Request
   * [[https://ticket.grid.iu.edu/goc/viewer?id=7837][7837]] - WQCG-Harvard-OSG C,C++ Compiler Installation Request

---++ Virtual Organizations Group (Abhishek)

   * D0 MC production minimal; waiting on new D0 application software release. 
      * Workload very low in past 3 weeks; 3 M, 0.5 M, 0.02 M Evts/week.     
           
   * SBGrid/NEBioGrid 
      * Immediate focus: throughput of how fast MM can match jobs to handle SBGrid&#39;s submission rate.
      * Upgraded Condor to v7.4 to resolve file descriptor limit issue.
      * Upgraded MM to v0.8, to use changes made by Mats in MM&#39;s logic.
      * Increased job lifetime to 2 hours.
      * Had a new peak of 1000+ simultaneous jobs, with 20,000 wall hours/day.
      * Background: https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/SBGrid_NEBioGrid_OSG  
    
   * Fermi-VO
      * MINOS subVO: production has ramped up; 40,000 hours/day.
      * CDMS, !MiniBooNE, PATRIOT subVOs: also doing production at moderate scale; less than 1000 hours/day.
        
   * !GridUNESP / DOSAR 
      * VO successfully started; liaison with DOSAR.
      * Currently waiting for GOC to make new release of VO package.
      * Experiences: https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/DOSAR_GridUNESP_OSG
         
   * !IceCube
      * Work ongoing to integrate and evaluate HTTP cache / Squid for data staging.
      * Background: 
         * Proof of principle done in Oct&#39;09.
         * Limited data-access model.
         * Total usage across 6 sites; 4,000 wall hours; 600 jobs at 50% efficiency.

---++ List of Sites that Need Gratia Pointers updated:
&lt;verbatim&gt;
athena.rit.albany.edu           should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
atlas.bu.edu                    should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
capiscum.hep.wisc.edu           should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
caraway.hep.wisc.edu            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
cassia.hep.wisc.edu             should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
cayenne.hep.wisc.edu            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
cluster04.syr.edu               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
cms-osg.phys.utk.edu            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
cress.hep.wisc.edu              should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
cumin.hep.wisc.edu              should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
davinci.nersc.gov               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
dayabay01.hep.wisc.edu          should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
fester.utdallas.edu             should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
grid1.oscer.ou.edu              should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
grid.rc.rit.edu                 should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
grow-phed.its.uiowa.edu         should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
heroatlas.fas.harvard.edu       should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
iogw1.hpc.ufl.edu               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
iut2-grid6.iu.edu               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
jacin03.nersc.gov               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
julius.rcac.purdue.edu          should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
kansasosg.phsx.ku.edu           should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
lnx6211.lns.cornell.edu         should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
login02.hep.wisc.edu            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
login03.hep.wisc.edu            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
login04.hep.wisc.edu            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
luma.hep.wisc.edu               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
m3.alliance.unm.edu             should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
magic.cse.Buffalo.EDU           should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
msu-osg.aglt2.org               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
nys1.cac.cornell.edu            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
osgce64.hepgrid.uerj.br         should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
osg-ce.sprace.org.br            should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
osg-gate.rice.edu               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
OSG-GRID1.umsl.edu              should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
ouhep00.nhn.ou.edu              should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
ouhep0.nhn.ou.edu               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
ouhep1.nhn.ou.edu               should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
quiver.mrl.ucsb.edu             should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
saxon.hosted.ats.ucla.edu       should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
smufarm.physics.smu.edu         should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
tier2-01.ochep.ou.edu           should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
tier2-02.ochep.ou.edu           should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
tp-grid3.ci.uchicago.edu        should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
uct2-grid6.uchicago.edu         should be contacting gratia-osg-prod.opensciencegrid.org port 80 (preferable for efficiency) or 8880 (if site policy prevents 80)
&lt;/verbatim&gt;

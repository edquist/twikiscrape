-- Main.DanFraser - 05 Oct 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * There was a problem with the Indianapolis-based BDII (is2.grid.iu.edu) between the hours of 18:30 UTC and 21:45 UTC. The issue was traced back to a IP renumbering project (as part of the maintenance) which had unforeseen affects on the internal DNS and LDAP. As soon as the issue was identified by the GOC, the maintenance was stopped and service was rolled back to the pre-maintenance configuration. 
   * Burt and Xin have given approval to begin the transition to BDII v5. Scott T. presented a transition plan that will introduce additional (virtualized) BDIIs into the round robin and allow for power user testing before incorporating these into production. In the future, as demand increases, this will enable the GOC to easily add extra virtual servers. (See below for more info.)
   * LIGO is now up to 90% of its August peak. (Robert)  
   * Doug Olsen is transitioning to a position with Magellan and will no longer be working as part of the OSG Security team.

---++ Attendees:
   * Xin, Armen, Britta, Robert E., Brian, Suchandra, Burt, Rob Q., Scott T., Mine, Chander, Dan
 
---++ CMS (Burt)
   * 306 khour/day, 84% success
   * LHC: delivered &gt; 22 pb-1 so far.  Following program to increase bunch numbers every 3 fills; technical stop this week to remove RF obstruction.

---++ Atlas (Armen &amp; Xin)

   * General production status
      * Physics data-taking restarted over the weekend after the LHC technical stop. Another week to go. Try to increase number of bunches up to ~400. Plan to accumulate another 40-50pb-1 of data. Currently ATLAS has 36pb-1 of data. ATLAS production was very stable during the last weeks. Last weekj was at the level of 9-10k running jobs all the time. At the moment still the plan is to start the reprocessing tomorrow. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.5M jobs, with CPU/Walltime ratio of 87%. 
      * Panda world-wide production report (real jobs): 
         * completed 1,252,877 managed/group MC production, validation and reprocessing jobs.
         * average 178,982 jobs per day
         * failed 132,134 jobs
         * average efficiency: 
            * jobs     - 90.5%
            * walltime - 96.7%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was 300~500TB/day.
   * Issues

---++ LIGO (Britta, Robert E.)

---+++ Gratia Reports
   * Last week&#39;s total usage: 3 users utilized 31 sites
      * 67310 jobs total (34075 / 33235 = 50.6% success)
      * 697147.7 wall clock hours total (653197.4 / 43950.3 = 93.7% success)
   * This  week&#39;s total usage: 4 users utilized 31 sites
      * 81978 jobs total (38281 / 43697 = 46.7% success)
      * 633983.0 wall clock hours total (539431.7 / 94551.3 = 85.1% success)

---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,488,124.84372, Last week: 1,292,173.41628
   * E@H rank based on RAC: 1 
   * E@H rank based on accumulated credits: 3 

---+++ LIGO / INSPIRAL
   * GLIDEINS
      * Submitted ten Glidein work-flows at FF to test scaling
         * Troubleshooting segfaults and exec call fails 
            * possibly due to wrong pegasus configuration 
            * testing fix
      * two glidein test work-flow at LIGO_CIT succeed
      * glidein test work-flow a UMissHEP: testing with adjusted pegasus configuration 
         
   * FILE TRANSFERS
      * tested code changes to enable srm-mkdir call
      * Further code enhancements in progress
      * TTU: site down, certs expired
      * UCSDT2: execute permission of pegasus executable lost in translation, contacted Terrence
      * UMissHEP: fork jobs run, md5sum check jobs in queue for days

   * TESTS WITH SRM SETUP
      * work-flows at CIT_CMS_T2, Firefly are running
      * Nebraska: symlinking of data fails-&gt; tmpltbank jobs cannot find the data and fail
          * Missing data on Nebraska - fixed, resubmitted work-flow

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * [[http://tinyurl.com/368urd2][Reliability/Availability of GOC Services]]
   * [[http://tinyurl.com/34m9qvq][Reliability/Availability of Security Services]]
      * looking into results from SECMON_CRL, may be related to Brazil CA outages.
   * ITB release - [[http://osggoc.blogspot.com/2010/10/goc-service-upgrade-tuesday-october_19.html][Release Notes]]
      * Including a TWiki version with X509 authentication. We hope this will speed up the registration process, as we will automate it from OIM. This will require an extensive bit of work in ensuring that user OIM information is correct and complete before we can release it into production.
   *  Gratia and !ReSS (Represented by Fermigrid Ops)
      * No issues this week. 

---+++ Operations This Week
   * [[http://osggoc.blogspot.com/2010/10/goc-service-upgrade-tuesday-october_19.html][Service Upgrade]] release notes
   * RHEL5 updates
   * WLCG Top Level BDII Project Team
      * We are hoping to host a top level BDII, there will be a meeting to discuss
      * The GOC has had a great deal of success in tests with a patch to fix some of the reliability problems of the most recent BDII release.
      * First Meeting was this morning (October 26th, 10:00 Eastern Time)
         * 866.740.1260  U.S. &amp; Canada Toll-Free
         * 303.248.0285  International
         * Access Code:   8266135
   * [[https://docs.google.com/document/edit?id=1sYZyZcPtWD3ZXp6-YQF7Wzty_9wJuQRbokDcSWgd7b4&amp;hl=en#][BDII Upgrade plan]] document exists
   *  Gratia and !ReSS (Represented by Fermigrid Ops)
      * No issues this week. 


   * None attending. Keith reports no issues. 
---++ Engage (Mats, John)

No production issues. I will not be able to attend the call today.

16 users utilized 43 sites;

26602 jobs total (118953 / 7649 = 94.0% success);

254174.9 wall clock hours total (231425.8 / 22749.1 = 91.0% success);

---++ Integration (Suchandra)
   * Released OSG 1.2.15 last week
      * Done by Scot following release steps
      * Confirmation of backup for releases
   * Next test cycle possibly starting next week

---++ Site Coordination (Marco)


---++ Metrics (Brian)


---++ Virtual Organizations Group (Marcia)

---+++ CHARMM (Tim Miller)

   * Currently about half of mutant proteins for the current project are running. Tim expects completion of all 17 mutants by the end of November.
   * Help from Jose and Maxim on finding sites has been helpful. (See list of current sites being used at [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/VOGroupMeeting20101021][https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/VOGroupMeeting20101021]]

---+++ SBGrid (Ian Stokes-Rees)

   * A large number of pilot jobs are sitting at sites doing nothing. Ian will send out data.
   * Also seeing job failures at sites.

---+++ !GridUNESP 

   * Changing from Condor to pbs improved average success rates of jobs from 60 to 95%.

---+++ CDF (Rick St. Denis)

   * Problem at KISTI: 40Hz of queries to pbs from !glideinWMS.  Pbs crashes every few hours. They have a cron checking every 5 min and doing a restart. Want to know what to tune where and whether there is a deeper architectural issue.
   * KISTI is a priority for CDF, as they run their production services off site. Gabriele is in Korea this week.
   * CDF also sees the pilot job sitting error that !SBGrid mentioned. 


---++ Security (Mine)
   * Internal staff transition. Doug is moving out of security duties. Anyone who has an ongoing security project with Doug should let us know. We are planning for transitioning the RA work to GOC. When our plans are more clear, we will inform our community. At the moment Doug continues his work as the RA. I will update you the changes immediately so if there are some slowness in certificate work you can announce to your communities.
   

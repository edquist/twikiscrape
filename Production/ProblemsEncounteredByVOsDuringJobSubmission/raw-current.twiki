%TOC%

---++Purpose
This document is being used to create a centralized list of problems encountered by different VOs when they submit jobs to the OSG. The purpose of this document is to use this information in developing a strategic approach for tackling/testing for as many of these problems as is reasonable. A more broadly scoped list of problems directly reported by VOs is available  [[Trash/Trash/VirtualOrganizations.DirectFeedbackOnProductionProblemsOrBottlenecks][here]].

---++Text Color Interpretation
   * %BLUE% Blue text represents problems that are solved (or rendered irrelevant) with either OSGMM or that could be added to a Pilot based system. (Some tests would need to be added to the Pilot systems to bring equivalency with OSGMM.) %ENDCOLOR%
   * %AQUA% Aqua text represents problems that are solved by OSGMM, but currently not by Pilots %ENDCOLOR%
   * %ORANGE% Orange text represents problems that are not addressed by either OSGMM or Pilots %ENDCOLOR%
   * Regular black text represents &quot;other&quot; issues

---++ Problems encountered when selecting/targeting an optimal subset of sites
   1 %BLUE% Which information system and which mechanism to use for retrieving a list of all OSG sites? (nanoHUB, SBGrid) %ENDCOLOR%
   1 %BLUE% Which workflow management system (and which logical tests) to use for probing all sites? (nanoHUB, SBGrid) %ENDCOLOR%
   1 %BLUE% Which logic/system/mechanism to use for selecting a &#39;suitable targeted subset&#39; of sites from all sites across OSG? (!CompBioGrid, nanoHUB, SBGrid, Glue-X) %ENDCOLOR%
   1 %BLUE% Which mechanisms to use for dynamically ranking all sites within this targeted subset of sites? (!CompBioGrid, nanoHUB, SBGrid, Glue-X) %ENDCOLOR%
   1 %ORANGE% Fermilab VO relies on !ReSS. Some of the information is not readily available via the GLUE Schema 1.3 and thus not via !ReSS. For instance, the GLUE schema only specifies the total memory which is available on a given worker node, but it gives no information if, for instance, the batch system will kill a job if it uses &gt;900MB of memory. (Fermilab VO) %ENDCOLOR%
   1 %ORANGE% How to understand Preemption Policies of sites? Currently there is only a binary yes or no, with no way to tell how long a VO&#39;s job hasto finish, or which VOs get pre-empted, etc. There are some other fields not present in the GLUE schema, i.e. scratch disk space available per node, which Fermilab VO had to add as customized attributes. (SBGrid, Fermilab VO, nanoHUB) %ENDCOLOR%

---++ Problems encountered when accessing remote sites
   1 %BLUE% Do sites that advertise access to specific VOs actually allow that VO to run?
   1  Is GRAM working?
   1 Is the Grid-proxy expired?
   1 For users registered as members of more than one VO, is the access properly mapped at sites? (SBGrid, !NEBioGrid) %ENDCOLOR%

---++ Problems encountered when trying to deploy software (either on a WN or on a Gatekeeper)
   1 %AQUA% Is !GridFTP up and available to be used to stage-in scripts for building software stacks? (LIGO) %ENDCOLOR%
   1 %BLUE% Are outgoing connections available?
   1 Is $OSG_APP defined, mounted, and provisioned, and does it have the correct permissions set?
   1 Is $OSG_APP mounted read-only on the worker nodes?
   1 Is $OSG_APP over quota? (Is the size reasonable, sometimes it is underestimated?) %ENDCOLOR%
   1 %AQUA% Are development tools available (configure, make, C &amp; C++ compilers)? (LIGO)
   1 Are development tools available (configure, make, C &amp; C++ compilers) only on the head node? %ENDCOLOR%

---++ Problems encountered when submitting a job
   1 %BLUE% Does the output location for the job exist and have the correct permissions? %ENDCOLOR%
   1 %BLUE% Is the output location over quota? %ENDCOLOR%
   1 %BLUE% Is !GridFTP available to stage the job?
   1 Is Condor-G working correctly on the submit host? (Is the configuration correct?)
   1 Is the jobmanager working?
   1 Is the Condor grid_monitor working? %ENDCOLOR%

---++ Problems encountered when running a job
   1 %BLUE% Is $OSG_DATA defined, mounted, and provisioned, and does it have the correct permissions set?
   1 Is $OSG_DATA over quota? (again reasonable use is often underestimated)
   1 Is $OSG_WN_TMP defined, mounted, and provisioned, and does it have the correct permissions set?
   1 Is $OSG_WN_TMP over quota? %ENDCOLOR%
   1 %ORANGE% Is the worker node architecture compatible with the software that was built on the head node? %ENDCOLOR%
   1 %BLUE% Are outgoing connections being blocked by a firewall?
   1 Are hostnames being resolved? %ENDCOLOR%

---++ Problems encountered when managing persistent data (using SRM)
   1 %ORANGE% Which sites on OSG provide persistent opportunistic storage? (D0, SBGrid, !IceCube, Glue-X) %ENDCOLOR%

---++ Problems encountered when accounting usage and counting errors
   1 How to resolve anomalies in reporting of errors, and thus error in wasted wall hours, when using Pilot based job management? (GEANT4, SBGrid, and other VOs using Pilots)
   1 Is OSG Accounting error-proof? E.g., how to understand wall hours consumption of partially executed workflows or evicted jobs? (SBGrid)

---++ How are Job error messages handled?
   1 %BLUE% GridJobStatus: Done; Globus error 8 %ENDCOLOR%
   1 ...

---++ Additional thoughts from VOs
   1 %BLUE% &quot;The various discussions I had during the AHM and your presentation made me think that the pilot job should do more than just verify the OSG environment. I think the user should be able to configure the pilot in such a way that it verifies whatever requirements the user&#39;s application has. The pilots that are started on behalf of the user should also keep a record of errors which in turn could be used to create statistics. Both features are currently present in our E@OSG software.&quot; (Robert E., LIGO) %ENDCOLOR%

---++ OSGMM Site Verification Checks

Fatal / fork (head node)

   * $HOME disk usage over 30 GB
   * $OSG_APP not defined
   * $OSG_DATA not defined
   * $OSG_WN_TMP not defined

Advertised (non-fatal) / fork (head node)

   * Is $OSG_APP writable?
   * Is $OSG_DATA writable?
   * $GLOBUS_LOCATION path

Fatal / jobmanager (worker node)

   * $OSG_APP not defined
   * $OSG_DATA not defined
   * $OSG_WN_TMP not defined
   * $OSG_GRID not defined

Advertised (non-fatal) / jobmanager (worker node)

   * Is $OSG_APP/$VO_NAME/jobenv.sh sourceable?
   * Is $OSG_GRID/setup.sh sourceable?
   * GB disk free for $OSG_DATA
   * GB disk free for $OSG_APP
   * GB disk free for $OSG_WN_TMP
   * Is globus-url-copy installed?
   * Is wget installed?
   * Does the site allow outbound network traffic?
   * Is $OSG_APP writable?
   * Is $OSG_DATA writable?
   * Is $OSG_WN_TMP writable?
   * $GLOBUS_LOCATION path
   * CPU bitness
   * Number of cores on the node
   * Total memory
   * Memory per core
   * Is the site using pool accounts?
  
The above tests are the standard ones shipped with OSGMM. VOs can add their own. Using Trash/Engagement as an example, we use the non-fatal tests for advertising available software (installed by the OSGMM maintenance jobs), and run more functional tests such as verifying that globus-url-copy can copy a file, and by that testing if for example the CRLs are updated correctly on the worker nodes.

---++ Results: Recommendations and Issues (for discussion)

If the OSG pilot based projects (e.g. !GlideinWMS and PANDA for starters) could incorporate the site verification and site maintenance features of OSGMM, one could make a strong case that these should be the recommended models for new VOs to use on the OSG.
   1 It is recommended that a common site verification test group be created (and included in the VDT) for general use As a baseline, it should incorporate a subset of the tests (TBD) currently in the OSGMM site verification suite. 
   1 It is recommended that the site verification tests be incorporated into the pilot projects.
   1 It is recommended that Pilot projects also incorporate the ability to run site maintenance jobs that verify and build (when needed) requested software components needed by VOs to run. 
   1 Opportunistic storage remains a problem and should be tackled separately, although this may eventually require some new components to be included to the site verification and or site maintenance harness. 

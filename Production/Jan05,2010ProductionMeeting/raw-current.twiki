-- Main.BrittaDaudert - 04 Jan 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * Xin has been monitoring the BDII problem with the new CE probe provided by Burt but did not see any anomalies. This seems to be a WLCG problem. This could be related to a CMS BDII problem where CERN BDII data was not getting updated. A ticket was filed with GGUS and worked on; the CERN BDIIs appear to be working now. Need to keep an eye on this and discuss strategies for monitoring. (Xin, Burt, GOC, Dan)
   * CMS noted that another site in addition to SDSC has run into zero-length CRL problems. Mine discussed the planned VDT fix that should not only fix this problem and provide better error reporting. This fix will be in the next VDT release.
   * Dan reported that LIGO is currently down to update the error management system. Should be back online soon. (Rob E.)
   * Engage needs the RESS RSV probe as soon as it is available. (Mats)
   * Burt to ping the CMS T2s for an SE request to help D0. 
   * Xin identified several T2s to help D0. Need to make sure everyone gets connected. (Xin)
---++ Attendees:
   * Mats, Xin, Armen, Britta, Brian, Suchandra, Burt, Marco, Abhishek, Mine, Chander, Miron, Dan
 
---++ CMS (Burt)
   * CRL bug affected UCSD and Florida
      - Somehow some CRLs became zero-length on two different CMS sites.  CRL update then does not update those CRLs anymore.
   * LIGO jobs filled $OSG_DATA at Florida; LIGO user was informed.
   * Computing: 84 khour/day, 95% success. CPU/wallclock at 68%.
   * Storage: 276 TB xfer (T1), 42 TB (others)
   * OSG: Nearly all at 1.2 (only Rutgers and OSU @ 1.0)
   * Non-CMS/CDF preemption issue @ MIT_CMS: second suggested fix not yet implemented at MIT.
   * D0 opportunistic storage -- no recruits yet (could only make the tail end of the T2 mtg today).

---++ Atlas (Armen &amp; Xin)

   * General production status
      * During the last two weeks USATLAS production was quite stable at the level of 8K running jobs. Reprocessing was finished by the end of 2009, as planned. BNL T1 processed ~50% of all jobs.  The reprocessing jobs are short in CPU but long in stage-in and stage-out, making cpu/walltime ratio lower, reflected in the following job stats. 
   * Job statistics for last two weeks. 
      * Gratia report: USATLAS ran 2.9M jobs, with CPU/Walltime ratio of 31 for the last week of 2009, and 81% for the first week of 2010. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 2M managed MC production, validation and reprocessing jobs 
         * average 291K jobs per day
         * failed 226K jobs
         * average efficiency:  jobs  - 89%,  walltime - 90%       
   * Data Transfer statistics for last week
      * Transfer rate stays the same as previous weeks. BNL T1 transferred ~75 TB/day data last week, with peak at 150 TB/day.  
   * Issues and GOC Tickets
      * GOC ticket 7772: Issues with WLCG BDII periodically loses information about some USATLAS Tier2 sites. Got the new ldap query from Burt, running it now on ganglia.
      * Opening more USATLAS T2 sites to D0 VO as opportunistic storage:  We will try to contact Joel from D0, figuring out details about configuration etc.

---++ LIGO (Britta)

   * Gratia reports:
   * Current week&#39;s total usage: 4 users utilized 30 sites;
      * 2424 jobs total (2372 / 52 = 97.9% success);
      * 30.4 wall clock hours total (25.7 / 4.7 = 84.5% success);
   * Previous week&#39;s total usage: 4 users utilized 33 sites;
      * 2641 jobs total (2571 / 70 = 97.3% success);
      * 87.8 wall clock hours total (64.7 / 23.1 = 73.7% success);
 
   * E@H reports
      * Recent Average Credit (RAC): 170,585.60004
      * E@H rank based on RAC: 10 (-7)
      * E@H rank based on accumulated Credits: 11 (+0)
   
   * Robert is working on code changes required to expand to Fermilab sites and Sprace

 
---+++ Binary Inspiral
    * 3 day test work-flow on Firefly: Gap in data error

---++ Integration (Suchandra)
   * OSG 1.2.5 release delayed due to last minute issues
   * Considering making an interim release of xrootd/bestman components for ATLAS
  
 
---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.4
      *       58 OSG 1.2.X resources (      23 are 1.2.4)
      *       10 OSG 1.0.X resources (       0 are 1.0.5)
      *       19 OSG 1.0.0 resources
      *        2 OSG 0.8.0 resources
         * OU_OCHEP_SWT2, tier2-01.ochep.ou.edu , Contact: Horst Severini
         * UIC_PHYSICS mstr1.cluster.phy.uic.edu , Contact: John Wolosuk

---++ Virtual Organizations Group (Abhishek)

---+++ VOs with High Activity

   * D0 MC reported good production in mid-Dec, followed by a drop. 
      * Workload very low in past 2 weeks; 3 M, 0.5 M Evts/week.
      * Nearly 11.4 M Evts in mid of Dec&#39;09; was new 6-month peak; OSG view was 9 M Evts; discrepancy was resolved.
      * MIT situation looks better, as reported by D0 in late Dec&#39;09.
      * [Carried over items:
         * ATLAS: Need for more SEs.
            * D0 can benefit from more opportunistic SEs at ATLAS T2 sites. 
            * Currently 2 SEs: MSU, MWT2-IU.
         * CMS: CE&#39;s rate of preemption. 
            * Possibly, MIT T2 applied the fix to CE. (Burt may have more accurate status).
            * Dan Bradley&#39;s solution: use &lt;u&gt; !LastHeardFrom &lt;/u&gt; instead of the more popular &lt;u&gt; !CurrentTime &lt;/u&gt;.
            * https://ticket.grid.iu.edu/goc/viewer?id=7814
         * CMS: Need for more SEs.
            * D0 can benefit from more opportunistic SEs at CMS T2 sites. 
            * Currently 3 SEs: Purdue, UCSD, UNL]
           
   * SBGrid/NEBioGrid 
      * Meeting and plan discussed on Dec 16 &#39;09.
      * Immediate goal: 
         * To increase job efficiency at moderate job volume. Then, increase job volume. 
         * Target sustained peak 3000 jobs running simultaneously. Increase to 6000 later. Target 1-2 times jobs queued as running. 
      * Full details: https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/SBGrid_NEBioGrid_OSG  
      * Immediate issue: Possibly, SBGrid submit infrastructure is co-located on same hardware as CE/Gatekeeper headnode. 
      * Current status:
         * ~200 simultaneous jobs. 
         * Working with Mats to resolve OSG-MM issues.
         * New version of MM v0.8 is now being installed, rank calculation to be tweaked.
      
   * GEANT4
      * Biannual EGEE-based exercise ran on OSG in Dec&#39;09. 
      * Scale was low; more analysis and report later this month. 
      * Problem: Discrepancy in wasted wall hours due to Pilots.
         * 50% in OSG resource-view, 0% (full success) in Geant4-view.
         * Reason likely to be OSG-side issue: Exit-code discrepancy due to Pilots.

   * CDF 
      * Successfully upgraded new portal to !GlideinWMS; working well.

   * Fermi-VO
      * Communication channel in Dec&#39;09 site upgrade; not discussed beforehand.
   
---+++ VOs with Limited Activity
      
   * !GridUNESP / DOSAR
      * Brought up !GridUNESP as community grid VO.
      * Infrastructure configured with OSG software stack (site side, then VO side).
      * Charter for !GridUNESP made public.  
      * Experiences: https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/DOSAR_GridUNESP_OSG
      * First !GridUNESP researchers submitted MPI jobs through full infrastructure, running on OSG.

         
   * !IceCube
      * Proof of principle completed in Oct&#39;09. Limited data-access model.
      * Total usage across 6 sites; 4,000 wall hours; 600 jobs at 50% efficiency.
      * Progress in Oct-Dec&#39;09 slow; !IceCube team was on travel to South Pole.
      * Work restarted in Dec&#39;09 to integrate HTTP cache / Squid for data staging.
 
---++ Security (Mine)
   * Sent out a separate email describing the CRL problem encountered at San Diego. 

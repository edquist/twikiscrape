-- Main.DanFraser - 04 Sep 2012
---++ Action/Significant Items:
   * We collectively dropped the ball on the manual CA release update notification for RPM based sites. This could result in reduced uptime accounting for the WLCG sites if the sites do not incorporate this CA release within 8 days for 3.1.8+ sites or 2 days for others.  Follow up:
      * Marco is working on a draft twiki page that explains how to manually download the latest CA release and is drafting an email for review.
      * Tim, Rob, and others to review first thing tomorrow morning.
      * Rob/Dan will make sure this process keeps moving and culminates with a note to sites tomorrow morning informing the version 3.0+ sites of what they need to do to complete the manual update. 
      * Tim to have a design document for the fix for this problem by next Tue. so this work can be completed, tested, and available before the next CA release in one month. 

---++ Attendees:
   * Tim, Suchandra, Marco, Rob Q., Mine, Chander, Dan
 
---++ CMS (Tony)


---++ Atlas (Armen &amp; Xin)

   * Armen and Xin are out this week and next.
   * General production status
      * LHC operation last week was overall stable. This week is machine development period. Current collected luminosity for ATLAS is ~15.4 fb-1.
      * US ATLAS production during the last week was quite stable, at the average level around 18-20K running jobs, mostly simulation type.
   * Job statistics for last week.      
      * Gratia report: 933K pilot jobs run on USATLAS sites, with CPU/walltime ratio of 91%
      * Real Jobs processed by US sites for last week, reported from PanDA monitor 
         * 1.2M
   * Data Transfer statistics for last week
      * Data transfer rate was around ~350TB/day at BNL T1. 
   * Issues

---++ Grid Operations Center (Rob Q.)
---+++ Operations Last Week
   * [[http://tinyurl.com/9oh5bzh][GOC Services Availability/Reliability]]
   * [[http://myosg.grid.iu.edu/miscstatus/index?datasource=status&amp;count_sg_1=on&amp;count_active=on&amp;count_enabled=on][Current Status]]
   * [[http://osggoc.blogspot.com/2012/10/goc-service-update-tuesday-october-9.html][ITB Update Change Log]]
   * CVMFS Mailing List Announcement - [[http://osggoc.blogspot.com/2012/10/new-osg-mailing-list-for-cvmfs.html][Link]]
   * No non-emergency changes to Operations Services on Nov 20th, December 25th, or January 8th. 
   * VO Package release to ITB
      * !SuperB Role Addition
      * Fermigrid Sub VO LBNE Removed (Now a full fledged VO)
   * !FermiGrid Ops
      * Still chasing down some unknown VO reporting
   * WMS Glide In Factory
      * glideinWMS software team helped us determine high load issues was due to an unexpected python behavior.
         * For each forked process using popen, python attempts to close all file descriptors by default.
         * We had set high max file descriptor limits (~50k) for the gfactory user previously to accommodate the Atlas testing.
         * Reducing limits (back to default 1k) on ITB cut the load on the machine in half.  This weekend I reduced the limits on UCSD and GOC Production.

---+++ Operations This Week
   * [[http://osggoc.blogspot.com/2012/10/goc-service-update-tuesday-october-9.html][Production Update Change Log]]
   * Upcoming Campus Infrastructures Community Workship - [[https://indico.fnal.gov/conferenceDisplay.py?confId=5927][Link]]
   * Place Holder for 2013 AHM - [[http://pti.iu.edu/osgahm-2013][Link]]
   * Discussions of OASIS (CVMFS) Hosting Ongoing 
   * !FermiGrid Ops
      * Still working on schema change and upgrade
   * WMS Glide In Factory
      * Plan to upgrade glideinWMS ITB to get patch fixing the tarball selection, and will make Condor 7.8.4 tarballs available for glideins.


---++ Campus Infrastructures / HTPC (Dan, Brooklin)

   * Campus Infrastructures workshop scheduled for Nov 14-15 in Santa Cruz. Registration, lodging and travel details, and a preliminary agenda are available at  https://indico.fnal.gov/conferenceDisplay.py?confId=5927.  While we hope most can attend in person there will be a video service for remote participants.

---++ Software (Tim)


---++ Integration (Suchandra)
   * Xrootd gsi tests completed successfully on SL5 without errors
      * Xrootd GSI broken on SL6
   * Investigating new GUMS release

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 3.1.10 / 1.2.30
      *       51 (-2) OSG 3.x (       1 are 3.1.10)
      *       50 (-3) OSG 1.2.X resources (       2 are 1.2.30)
      *        1 (-1) OSG 1.0.X resources (       0 are 1.0.6)
      *        0 (0) OSG 1.0.0 resources


---++ User Support (Chander, Mats)

At the OSG Council call today (Oct 9), SBGrid noted that they are not getting as many opportunistic cores as has been the case in the past.  This also reminds us of the remark from GLOW that they have great difficulty in getting more than 100K hours per day.  Thus there is some issue here that may need to be reviewed.  The OSG Council is asking the OSG Project to examine the broad area of opportunistic availability for VOs and determine the current state, problems, and future directions.  We need to frame the questions that need to be addressed.  Chander proposes to work with Dan and Lothar to review this matter and determine next steps.

---++ Security (Mine)
   * Emergencies/Vulnerabilities
      * Nothing new
   * Operations
      * OSG PKI GA Training completed. 
      * Pakiti technical work is completed. Sent to VDT team for inclusion in OSG VDT. 
      * New OSG CA bundle came out today. 



---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings


-- Main.RobQ - 09 Oct 2012

-- Main.SarahCushing - 26 Apr 2010

%TABLE{ tablewidth=&quot;700&quot; columnwidths=&quot;5%, 15%, 80%, 10%, 10%&quot; cellpadding=&quot;2&quot; dataalign=&quot;left&quot; tablerules=&quot;all&quot; tableborder=&quot;2&quot; databg=&quot;#FFFFFF, #FFFFFF&quot;}%
| # | Owner | Action/Significant Item | Open Date | Close Date |
|1| Xin | The wide variance in data transfers at Atlas T1 (Hundreds of TB/day to near zero on some days) appears to be due to actual load variances and not a problem in Gratia. | 4/6/2010 | |
|2| Abhishek | DZero now running at near peak capacity (12M Events) and not hitting any production limits. Their thank you to the CMS T1 folks for increasing the number of batch slots available. | 4/6/2010 | |
|3| Abhishek | SBGRID hit a new peak of 70K hours/day. | 4/6/2010 | |
|4| Abhishek | NanoHub?&lt;https://twiki.grid.iu.edu/bin/edit/Production/NanoHub?topicparent=Production.Apr6,2010ProductionMeeting&gt; reported a bug in DagMan?&lt;https://twiki.grid.iu.edu/bin/edit/Production/DagMan?topicparent=Production.Apr6,2010ProductionMeeting&gt; after upgrade to Condor 7.4.1. A patch has been issued that NanoHub?&lt;https://twiki.grid.iu.edu/bin/edit/Production/NanoHub?topicparent=Production.Apr6,2010ProductionMeeting&gt; is using. | 4/6/2010 | |
|5| Mine, Brian | A known bug (feature?) in Apache was re-discovered this week where Apache must be restarted in order to pick up new certs. Currently there is no work around available. | 4/6/2010 | |
|6| All | Burt is out with Jury Duty on Tuesdays for the next few months but agreed to find a replacement person to represent CMS Grid services until he is back. | 4/13/2010 | |
|7| Xin | Atlas data rates were consistent between Ganglia and Gratia and verifyied the increase to over 400 TB/day a few days ago. There is still some concern about the rates dropping to near zero about 12 days ago. Will continue to watch data transfer rates closely at BNL and cross-compare any anomalies between Ganglia and Gratia. | 4/13/2010 | |
|8| Rob | The GOC will transition to the Web Services mechanism for ticket transfers with RT (BNL) tomorrow at 10am  | 4/13/2010 | |
|9| Rob | CMS Tier-3 tickets are now being routed through the newly setup CMS T3 support center.  | 4/13/2010 | |
|10| All |  Three sites made the transition from OSG 1.0 to 1.2 this week. | 4/13/2010 | |
|11| Xin, Brian | The CERN BDIIs exceeded an internal 5MB limit and info became unavailable when BNL-Atlas added two additional CEs to its resource group. The problem was restored when the CEs were pulled back. Brian is working with Atlas to upgrade to the multi-CE configuration but this requires a GIP upgrade as well. Tests are underway to temporarily upgrade the Gratia binaries since a full upgrade will need to wait until Atlas downtime. This should reduce the data to less than 3MB, even with the new CEs. | 4/20/2010 | |
|12| Rob | CERN BDII operations are not currently considered as a critical service although both CMS and Atlas consider their dependency on BDII to be critical, with CERN requiring 30 minute response time. This is being addressed by WLCG management. | 4/20/2010 | |
|13| All | There was a Gratia outage for over 20 hours. Brian discovered this by watching the DOE display. There were no reports from the Gratia team. outage was restored when the backup Gratia DB was switched to the active one. Need to have a post-mortem of this outage and importantly the communications channel. The DOE display handled this outage correctly and gracefully as designed. | 4/20/2010 | |
|14| Dan | One of the ReSS?&lt;https://twiki.grid.iu.edu/bin/edit/Production/ReSS?topicparent=Production.Apr27,2010ProductionMeeting&gt; servers failed this week, but properly failed over, so there was no downtime. Causes are being investigated. | 4/27/2010 | |
|15| Brian, Rob | LIGO was asked to reduce pressure on NFS servers at Nebraska. Rob E mentioned that he will run a different code version on Nebraska as well as the Fermi T1. | 4/27/2010 | |
|16| Brian, Tony | SBGRID jobs were causing trouble on the Fermi T1 and at Nebraska and were temporarily banned on both sites. | 4/27/2010 | |
|17| Rob, Tony | LIGO also making sure that boinc no longer pings Google when running E@H.&lt;mailto:E@H.&gt; This was causing problems on the Fermi T1  | 4/27/2010 | |
|18| Dan, Tony | Data transfer data from Fermi T1 is not showing on Gratia chart for past 3-4 days. Dan asked Tony to take a look. | 4/27/2010 | |
|19| Marco | Three more sites upgraded to OSG 1.2.x. | 4/27/2010 | |
|20| Abhishek | New peak in DZero jobs this week, 13.3 Million Events, 120K hours last week | 5/4/2010 | |
|21| Dan | Root Cause Analysis of Gratia problem on April 20 affecting the OSG Display is ongoing. Main problem is with a known problem in mySQL | 5/4/2010 | |
|22| Abhishek, Dan | SBGRID overloading issues at FNAL and UNL have been resolved, jobs running normally again | 5/4/2010 | |
|23| Rob, Burt | There was a BDII outage (caused by a CEMON collector failure) that affected the visibility of the CMS T1 and other sites last Tuesday evening. After hours response from the GOC team was excellent. Primary causes are still being investigated although it looks to have been caused by a bug in NSCD. Also exploring monitoring requirements to detect future failures in the BDII end-to-end architecture. The failed BDII node has temporarily been taken out of the system for diagnostics. More details below in Burt and Rob&#39;s reports. | 5/11/2010 | | 
|24| Dan, Rob, Brian |  A draft report from the Root Cause Analysis of the Gratia problem detected by the OSG Display on Apr 20 is being circulated and iterated on. Plan to have a final report available soon. | 5/11/2010 | |
|25| Brian | Dan noted that error rates in the number of CMS wall clock hours seem abnormally high (sometimes ~50%). Brian noted that it is difficult to determine if this represents a real problem, since pilot jobs increase the difficulty in tracking errors. Brian investigating. | 5/11/2010 | |
|26| All | A jump in the number of CMS error rates (by hour) as noted last week were investigated by Brian and found to be internal to CMS. This reflects a change in the job mix as data is increasing from the experiment and users are running more grid jobs. Errors are roughly 50% stage out errors, 25% configuration errors, 13% output errors, and 12% other. Atlas error rates are not as visible since everything is bundled in pilots that usually complete complete successfully although jobs sometimes fail. | 5/18/2010 | |
|27| Rob| The CEMON collector outage last week was attributed to a known problem with the NCSD after it has been running for long periods of time O(months) . The IS2 BDII was brought back online on Monday with a couple of patches, one to the BDII and one to the kernel. Plan is to switch NCSD to automatically restart itself every 24 hours at the next change cycle. | 5/18/2010 | |
|28| John | Engage noted that there is a problem with enabling users on OSG who need large (~600GB) databases co-located. (John) This is not a short term issue. | 5/18/2010 | |
|29| Abhishek, Dan, Rob E, Jim, etc | We now have a centralized collection of data from different VOs that characterize many of the problems they encounter when attempting to get jobs running across the OSG. Next step is to try and understand the issues and determine which can be tackled from a systematic perspective. https://twiki.grid.iu.edu/bin/view/Production/ProblemsEncounteredByVOsDuringJobSubmission | 5/18/2010 | |
|30| All | Atlas has requested the ability for sites with SE&#39;s but not CE&#39;s to be able to register to the BDII. Tony is starting to look at this from the GIP side. | 5/24/2010 | |
|31| Marco | Problem with sites not reporting to the correct Gratia collector is winding down. FermiGrid operations is planning to see if there are any stragglers. | 5/24/2010 | |
|32| Dan, Rob Q | GOC monitoring will now alert when 10% of sites have dropped out of the BDII as a high level monitoring mechanism for the CEMON collector. It is also important to monitor/know when a T1 goes offline. | 5/24/2010 | |
|33| Tony, Burt | Tony is working on the requirements for enabling an SE in GIP without necessarily having a CE. Meanwhile, Burt has set up a dummy CE for Princeton as a work around. | 6/1/2010 | |
|34| All | In preparation and gathering results for ICHEP (July 29), it is anticipated that there will be increased usage from Atlas and CMS. CMS has requested that the facility be maintained in as stable of a condition as possible. | 6/1/2010 | |
|35| Brian, Dan | Plan to create a Gratia plot that shows how opportunistic usage is varying on a site by site basis as competition for opportunistic usage is noticeably increasing. | 6/1/2010 | |
|36| Armen | There were some issues reported on Atlas sites this week involving storage filling up at some sites. Several sites were temporarily banned from BNL and data transfers were temporarily halted. Atlas runs a centralized data distribution mechanism but the centralized delete mechanism is not working as efficiently. Work is ongoing to address this issue | 6/8/2010 | |
|37| Rob | There was a Gratia outage on June 3 that was discovered first thing in the morning. This was due to a RedHat?&lt;https://twiki.grid.iu.edu/bin/edit/Production/RedHat?topicparent=Production.Jun8,2010ProductionMeeting&gt; / MySQL?&lt;https://twiki.grid.iu.edu/bin/edit/Production/MySQL?topicparent=Production.Jun8,2010ProductionMeeting&gt; problem where MySQL?&lt;https://twiki.grid.iu.edu/bin/edit/Production/MySQL?topicparent=Production.Jun8,2010ProductionMeeting&gt; can take longer to restart than RedHat?&lt;https://twiki.grid.iu.edu/bin/edit/Production/RedHat?topicparent=Production.Jun8,2010ProductionMeeting&gt; allows. This problem has been fixed. No data was lost. The FNAL monitoring / alarm system worked correctly. | 6/8/2010 | |
|38| Rob | Scott Teige will be Arvind&#39;s replacement at the GOC. | 6/8/2010 | |
|39| Rob | It was decided not to upgrade the CEMON Collector since stability has been requested by CMS, and no urgent reasons were found in the release notes to justify an upgrade at this time. | 6/8/2010 | |
|40| Dan | We continue to operate with minimal changes to allow teams to get their data for iCHEP. | 6/15/2010 | |
|41| All | As noted in last weeks report, there was an issue with a T3 site filling its storage system and effectively creating a DOS attack on the BNL T1. The problem was correctly handled as the T1 simply blacklisted the site. There are still a few questions remaining however and Armen has kindly agreed to look into this problem and report at the next Production meeting. | 6/15/2010 | |
|42| Rob | There was a slowdown of one of the ReSS? servers over the weekend between 6/11 and 6/14, ReSS? service remained available due to high availability features. An auto-correction script was updated which should prevent slowdowns like this in future. | 6/15/2010 | |
|43| Rob | Gratia machines will delay kernel update pending new Gratia release forthcoming in a couple of weeks, which will partially address issues of the reporting database being behind the collector database during housekeeping. | 6/15/2010 | |
|44| Mine | The &quot;production&quot; version of Pakiti has been released. The security team will begin work toward deploying for CMS T3 sites. | 6/15/2010 | |
|45| Rob | There is a problem with sites dropping out of the Sam-BDII at CERN. It is not known if this impacts other top-level BDIIs, but the problem scope is larger than just OSG as non-OSG sites are having the same problem. While the primary work on this is happening at CERN, Rob is planning to add the SAM-BDIIs to his monitoring system, and is also preparing to run a high intensity test against the SAM-BDIIs that we used earlier on known BDIIs. This should provide good comparison data that may be helpful in debugging. | 6/22/2010 | |
|46| Burt, Dan | Did not have representation from CMS this week. Dan to follow up with Burt on getting a replacement. | 6/22/2010 | |
|47| All | The GIP team released an alpha version that will enable sites to register with just an SE. This is currently being tested by Atlas. | 6/22/2010 | |
|48| Burt | There was a production problem noted last week by some of the VOs that was traced to the GlideinWMS?&lt;https://twiki.grid.iu.edu/bin/edit/Production/GlideinWMS?topicparent=Production.Jun29,2010ProductionMeeting&gt; factory running a development release of Condor, in which there was a bug in the detection of GT2 vs GT5 sites. This has been fixed by rolling back to an earlier version, but we need to plan for the future and consider operating a Glide-inWMS factory as a &quot;production&quot; service. | 6/29/2010 | |
|49| Dan |  There has been significant progress on the problem of adding SEs to the BDII without a CE. The first version will be for Bestman SEs. Testing of the end to end solution is targeted to begin on July 6. Xin suggested that there will be some T3s that may be interested in testing this when the new SE package is available. | 6/29/2010 | |
|50| Mine | Pakiti server is ready and installed from the source code. Anand is working with Rob to ensure it meets our expectations. | 6/29/2010 | |
|51| Abhishek | UTA SE is now being used by D0. Hooray, Hooray. | 6/29/2010 | |
|52| Marco | There will be a Site Administrators meeting on Aug 10-11 at Vanderbilt. | 6/29/2010 | |

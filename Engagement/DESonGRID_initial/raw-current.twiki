---+Running Standalone DES Processing on the Grid

Here are older instructions for running [[DES_SingleEpochOverview][first or final cut DES processing]]
on fermigrid using the local nfs (bluearc) for storage. The software and procedures
are still being worked out.
&lt;pre&gt;
  A. Make sure the input directory is where we expect, and get list of
     valid exposures using is_science_exposure.py.

  B. Make new output directory:

     uberftp fnpcosg1.fnal.gov
     cd /des/orchestration/DTS/dst
        # May need to &#39;cd /des/&#39; then &#39;cd orchestration&#39; etc to give
        # bluearc a chance to mount this.
     mkdir finalcut_20121102_outputA

  C. Near the top of des.csf, change
       &#39;exposures&#39; to have the file name produced in Step A,
       &#39;num_exposures&#39; to have the number of lines in this file,
       &#39;output_dir&#39; to be the directory created in Step B, and
       &#39;pipeline&#39; to be the processing pipeline needed.

  D. Make a directory to run from, and copy the needed
     files there. These are:

         run_desB.sh
         des.csf
         exposureZZZZ.txt    # filename from Step A
         submit_finalcut_multiexpo.wcl
         submit_firstcut_multiexpo.wcl
         get_filter.py

  E. Get set up to use condor:

          voms-proxy-init -valid 48:00  -voms des:/des/production

       To see information about a proxy, the command is

         voms-proxy-info -all

       To delete the local copy of the proxy, the command is

          voms-proxy-delete

  F. Start the jobs running with &#39;condor_submit des.csf&#39;
     To check on the status, the command is

       condor_q

     or

       condor_q CLUSTER_NUMBER

     If you need to stop the jobs, the command is

         condor_rm CLUSTER_NUMBER

     Can also watch

       /blue-orch/mslyz_des_test/e939/LOCK/LOCKS

     to see how the data transfers are going.
&lt;/pre&gt;

-- OSG/FNAL User Support

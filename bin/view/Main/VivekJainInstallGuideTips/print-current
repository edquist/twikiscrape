<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> VivekJainInstallGuideTips &lt; Main &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Main/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Main/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Main/VivekJainInstallGuideTips?_T=16 Feb 2017" type="application/x-wiki" title="edit VivekJainInstallGuideTips" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/bin/view/Main/WebRss" />    
<base href="https://twiki.opensciencegrid.org/bin/view/Main/VivekJainInstallGuideTips"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#FFEFA6;
	}
	.patternBookView {
		border-color:#FFEFA6;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">



</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> -- <a href="/bin/view/Main/VivekJain" class="twikiLink">VivekJain</a> - 05 Jul 2005
<p />
<p />
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Introduction">Introduction</a>
</li> <li> <a href="?cover=print#Installing_the_cluster"> Installing the cluster</a> <ul>
<li> <a href="?cover=print#Setting_up_the_ROCKS_cluster"> Setting up the ROCKS cluster</a>
</li> <li> <a href="?cover=print#OSG_GateKeeper"> OSG-GateKeeper</a>
</li></ul> 
</li> <li> <a href="?cover=print#Following_the_OSG_Install_Guide"> Following the OSG Install Guide</a> <ul>
<li> <a href="?cover=print#Pre_installation_Checklist"> Pre-installation Checklist</a> <ul>
<li> <a href="?cover=print#Firewalls"> Firewalls</a>
</li> <li> <a href="?cover=print#User_accounts"> User accounts</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installation_Procedure"> Installation Procedure</a> <ul>
<li> <a href="?cover=print#Certificates"> Certificates</a>
</li></ul> 
</li> <li> <a href="?cover=print#Configuration_and_setup_of_OSG_C"> Configuration and setup of OSG CE Services</a> <ul>
<li> <a href="?cover=print#Configuring_Trash_ReleaseDocumen"> Configuring ReleaseDocumentationMonALISA</a>
</li> <li> <a href="?cover=print#Authorizing_other_users"> Authorizing other users </a>
</li> <li> <a href="?cover=print#Configuring_MIS_CI"> Configuring MIS-CI</a>
</li> <li> <a href="?cover=print#Configuring_CONDOR"> Configuring CONDOR</a> <ul>
<li> <a href="?cover=print#Setup_the_Gatekeeper_on_the_inte"> <font color="#0000ff"> Setup the Gatekeeper on the internal network </font></a>
</li> <li> <a href="?cover=print#Exporting_directories_from_the_G"> <font color="#0000ff"> Exporting directories from the Gatekeeper </font></a>
</li> <li> <a href="?cover=print#To_start_CONDOR_at_boot"> <font color="#0000ff"> To start CONDOR at boot </font></a>
</li> <li> <a href="?cover=print#Other_CONDOR_configuration_files"> <font color="#0000ff"> Other CONDOR configuration files to be changed </font></a>
</li></ul> 
</li></ul> 
</li></ul> 
</li> <li> <a href="?cover=print#Testing_some_of_this_software"> Testing some of this software</a>
</li></ul> 
</div>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Introduction"></a> Introduction </span></h2>
<p />
Here are some <strong>tips/comments</strong> for setting up a grid node as per instructions in the OSG Install Guide. <strong>I installed OSG-ITB software version 0.1.6</strong>
<p />
When I started this project, I had very, very limited knowledge about system adminstration, so these comments may be useful for novices (like me). Feel free to <a href="mailto&#58;vj&#64;bnl&#46;gov">e-mail</a> me. A lot of people helped, especially Terence, Alain, Horst, Nate to name a few.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Installing_the_cluster"></a> Installing the cluster </span></h2>
<p />
We first set up the cluster using <a href="http://www.rocksclusters.org/Rocks/" target="_top">ROCKS 3.3</a> (Makalu). This includes the ROCKS headnode and compute nodes. We then set up the Gatekeeper node; this node contains all the VDT software. 
<p />
<h3><a name="Setting_up_the_ROCKS_cluster"></a> Setting up the ROCKS cluster </h3>
<p />
The ROCKS installation documentation is very well written. Since we were setting up a test grid node, we used some old CPU's which were lying around. I mention this because, we ended up booting the compute nodes using a floppy disk (the CD drives were not working and the NIC cards were too old to know about a PXE boot). We downloaded the relevant software from <a href="http://www.rom-o-matic.net/" target="_top">here</a> - you have to know the type of your NIC.
<p />
For installing the ROCKS cluster, we only used the base roll and the Kernel+HPC roll. All other software came from VDT.
<p />
The ROCKS headnode is multi-homed. One NIC is connected to the external world and the other to the internal network. The latter has the IP address - 10.1.1.1. We have three compute nodes. When we go to a bigger cluster, we will probably have to do more sophisticated things for setting up NFS.
<p />
Make sure that <em>eth0</em> is connected to the internal network (<em>eth1</em> is connected to the outside world).
<p />
<h3><a name="OSG_GateKeeper"></a> OSG-GateKeeper </h3>
<p />
<a name="MyGatekeeper"></a> Here we took a box and installed Scientific Linux 3 on it. Initially, we had connected this only to the external world, but it was recommended by <a href="/bin/view/Main/TerrenceMartin" class="twikiLink">Terrence</a> that we also connect it to the same private network as the ROCKS headnode; the GK's internal card is connected on the "same side" of the switch as the ROCKS compute nodes. 
<p />
The internal NIC had a IP address of 10.1.1.2. For the DNS address, we used 10.1.1.1, i.e., the ROCKS headnode. It is necessary to have the GK talk directly to the compute nodes - makes the operation of CONDOR more robust. I will talk more about this <a href="/bin/view/Main/VivekJainInstallGuideTips#MyHeadnode" class="twikiCurrentTopicLink twikiAnchorLink">here</a>.
<p />
Make sure that <em>eth0</em> is connected to the internal network (<em>eth1</em> is connected to the outside world).
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Following_the_OSG_Install_Guide"></a> Following the OSG Install Guide </span></h2>
<p />
Now come my comments/tips on the install guide - <span class="twikiNewLink">Trash/Trash/Integration.OSGCEInstallGuide-0.1.6<a href="/bin/edit/Trash/Trash/Integration/OSGCEInstallGuide-0/1/6?topicparent=Main.VivekJainInstallGuideTips" rel="nofollow" title="Trash/Trash/Integration.OSGCEInstallGuide-0.1.6 (this topic does not yet exist; you can create it)">?</a></span>. For the most part, the guide is well documented. Some stuff needs to be better explained, at least for a novice.
<p />
<h3><a name="Pre_installation_Checklist"></a> Pre-installation Checklist </h3>
<p />
Most of the stuff listed here is pretty straightforward and well documented. Some comments:
<p />
<h4><a name="Firewalls"></a> Firewalls </h4>
<p />
   If you follow the link, you will come to the separate section on <span class="twikiNewLink">Firewalls<a href="/bin/edit/Trash/Trash/Integration/OSGCEInstallGuide-0/1/6?topicparent=Main.VivekJainInstallGuideTips" rel="nofollow" title="Firewalls (this topic does not yet exist; you can create it)">?</a></span>. As it is currently written, it appears that information regarding setting environment variables like GLOBUS_TCP_PORT_RANGE only applies to the case of host-based firewalls. This is not <strong>true</strong>. It applies equally to the case where you have an external firewall "surrounding the nodes". 
<p />
You need to set the variables <strong>GLOBUS_TCP_PORT_RANGE</strong> and <strong>GLOBUS_TCP_SOURCE_RANGE</strong> (both have the same range, <em>e.g.,</em> 40000-41000).
<p /> <ul>
<li> In <strong>/etc/xinetd.d/gsiftp,gsiftp2,globus-gatekeeper</strong> - both variables need to be set
</li> <li> Wherever you have installed the VDT software, below that will be sub-dirs, globus and vdt <ul>
<li> In <strong>globus/etc/globus-job-manager.conf</strong>, you need to set <strong>GLOBUS_TCP_PORT_RANGE</strong>
</li> <li> In <strong>vdt/etc/vdt-local-setup.sh (.csh)</strong>, you can set both these varibles (<em>won't hurt</em>)
</li></ul> 
</li></ul> 
<p />
Remember to change the configuration files on the firewall to allow traffic on these ports, as well as the other ports specified in the documentation. 
<p />
<h4><a name="User_accounts"></a> User accounts </h4>
<p />
I installed the VDT software under <strong>/usr/local/grid</strong> and the plan is to export this to the ROCKS headnode and compute nodes. Thus, I put the user accounts in /usr/local/grid/users/. To make a new user account, do
<p />
useradd -d /usr/local/grid/users/&lt;username&gt; &lt;username&gt;
<p />
Remember, to set up the user accounts on the ROCKS headnode too. Here, it doesn't matter where the home directory is. Just use the command,
<p />
useradd &lt;username&gt;
<p />
The only requirement is that the user ID's should match on the two nodes.
<p />
<h3><a name="Installation_Procedure"></a> Installation Procedure </h3>
<p />
Again the documentation is quite self-explanatory. Some comments:
<p />
<h4><a name="Certificates"></a> Certificates </h4>
<p />
<a name="MyCertificates"></a> Once you get your User certificate, you will also need other certificates which are for the host as well as for services on the host. In some cases, it is better to wait for the certificate before proceeding. I needed a host certificate as well as service certificates for LDAP and http. It may save time if you were to apply for all these certificates at once (<font color="#0000ff"> Is this possible? Need to check </font>).
<p />
To apply for the certificates, go to <a href="https://pki1.doegrids.org" target="_top">https://pki1.doegrids.org</a> and follow instructions.
<p />
<h3><a name="Configuration_and_setup_of_OSG_C"></a> Configuration and setup of OSG CE Services </h3>
<p />
Again the documentation is quite self-explanatory. Some comments:
<p />
<h4><a name="Configuring_Trash_ReleaseDocumen"></a> Configuring <span class="twikiNewLink">Trash.ReleaseDocumentationMonALISA<a href="/bin/edit/Trash/Trash/Integration/MonALISA?topicparent=Main.VivekJainInstallGuideTips" rel="nofollow" title="Trash.ReleaseDocumentationMonALISA (this topic does not yet exist; you can create it)">?</a></span> </h4>
<p />
The Trash.ReleaseDocumentationMonALISA daemon monitors the state of the ROCKS compute nodes through the ROCKS headnode. The latter uses Ganglia for collecting information. It was suggested that it would be better for this communication to occur via the internal network. So, when you are configuring Trash.ReleaseDocumentationMonALISA, you will be asked the host name on which Ganglia is running. Instead of giving the fully qualified domain name, I used ahepg1h.local (my ROCKS headnode is called <em>ahepg1h.tld</em>). Even if you don't answer correctly, you can go back and edit the file <strong>$VDT_LOCATION/MonaLisa/Service/VDTFarm/vdtFarm.conf</strong>.   Make sure in the following line, it says ahepg1h.local instead of the FQDN
<p />
*PN_vdt{monIGangliaTCP, ahepg1h.local, 8649}%30
<p />
On the ROCKS headnode (<em>ahepg1h</em>), you have to edit <strong>/etc/gmond.conf</strong> and set <strong>trusted_hosts</strong> to point to the Gatekeeper node. Again, you want to use the internal network. I say <strong>trusted_hosts osg-gk.local</strong> - on the internal network, the gatekeeper is known as <strong>osg-gk</strong>, even though to the external world, it is known as <em>aheposgk.tld</em>. I'll talk about how to set this up <a href="/bin/view/Main/VivekJainInstallGuideTips#MyHeadnode" class="twikiCurrentTopicLink twikiAnchorLink">here</a>.
<p />
<h4><a name="Authorizing_other_users"></a> Authorizing other users </h4>
<p />
Since I have a test node, for now I am using the simplest scheme, <strong>Grid3</strong>, to authorize others users. Basically, I have grid-mapfile which gets updated regularly with user names.
<p />
You also need to do chkconfig --add edg-gridmapfile-upgraded, so that the daemon comes up at boot time.
<p />
<h4><a name="Configuring_MIS_CI"></a> Configuring MIS-CI </h4>
<p />
When I originally configured OSG and other packages, I had set the variable <strong>$WNTMP</strong> to be <strong>/usr/local/grid/tmp</strong>, which is exported to all nodes. I later realized that this is not necessary and that $WNTMP can point to a directory which is available only on the ROCKS compute nodes. So, I re-ran <strong>$VDT_LOCATION/monitoring/configure_osg.sh</strong>, and set this variable to be <strong>/state/partition1/tmp</strong> (visible only on the ROCKS compute nodes).
<p />
I think because originally I had set it to be /usr/local/grid/tmp, MIS-CI didn't know about the change. We discovered this because Gridcat started to look at the MIS-CI information and was reporting the wrong location. <a href="mailto&#58;jrosheck&#64;indiana&#46;edu">John</a> suggested I do the following steps:
<p /> <ul>
<li> cd $VDT_LOCATION  and then <strong>. setup.sh</strong>
</li> <li> cd MIS_CI and then <strong>./configure-misci.sh</strong>
</li> <li> When I had originally configured MIS-CI, I had started a cron job, the database should get updated automatically. However, one can force the issue by doing, <strong>cd sbin and ./run-mis-ci.sh</strong>
</li></ul> 
<p />
<p />
<h4><a name="Configuring_CONDOR"></a> Configuring CONDOR </h4>
<p />
This took the longest time to tweak. <a href="/bin/view/Main/TerrenceMartin" class="twikiLink">Terrence</a> made many suggestions and we finally got it working. Here are the steps I followed. I did all this tweaking after I had set up the system and discovered that CONDOR wasn't working
<p />
<h5><a name="Setup_the_Gatekeeper_on_the_inte"></a> <font color="#0000ff"> Setup the Gatekeeper on the internal network </font> </h5>
<p />
<a name="MyHeadnode"></a> As mentioned previously, the OSG Gatekeeper needs to communicate with both the ROCKS headnode (for <span class="twikiNewLink">Trash.ReleaseDocumentationMonALISA<a href="/bin/edit/Trash/Trash/Integration/MonALISA?topicparent=Main.VivekJainInstallGuideTips" rel="nofollow" title="Trash.ReleaseDocumentationMonALISA (this topic does not yet exist; you can create it)">?</a></span>) and the ROCKS compute nodes (for CONDOR). It was suggested that we set it up so that it uses the internal network as much as possible. Some details are <a href="/bin/view/Main/VivekJainInstallGuideTips#MyGatekeeper" class="twikiCurrentTopicLink twikiAnchorLink">here</a>.
<p />
On the ROCKS headnode,
<p /> <ul>
<li> Make a file, <strong>/var/named/rocks.domain.local</strong> and put the line "osg-gk A 10.1.1.2" in it.
</li> <li> cd /var/named
</li> <li> dbreport dns &gt; rocks.domain
</li> <li> Make a file, <strong>/var/named/reverse.rocks.domain.local</strong> and put the line "2.1.1 PTR osg-gk.local." in it.
</li> <li> dbreport dns reverse &gt; reverse.rocks.domain
</li> <li> /etc/init.d/named reload
</li></ul> 
<p />
These steps allow osg-gk.local, osg-gk, 10.1.1.2 to be resolved. Remember, that for the Gatekeeper, the DNS server is the ROCKS headnode.
<p />
<h5><a name="Exporting_directories_from_the_G"></a> <font color="#0000ff"> Exporting directories from the Gatekeeper </font> </h5>
<p />
I installed the VDT software below <strong>/usr/local/grid</strong>. Some of the software needs to be visible from the ROCKS compute nodes, so we have to export these sub-dirs.
<p />
   On the <em>Gatekeeper</em>,
<p /> <ul>
<li> Edit <strong>/etc/exports</strong> and add "/usr/local/grid 10.0.0.0/255.0.0.0(rw,no_root_squash)" - not clear if no_root_squash is necessary. We put it in while trying to fix a bug.
</li> <li> Export via "exportfs -r"  (refreshes the kernel)
</li> <li> "service nfs start" (Start NFS on the Gatekeeper) 
</li> <li> If you also do "chkconfig nfs on" and "chkconfig nfslock on", it will keep NFS on all the time
</li></ul> 
<p />
On the <em>ROCKS headnode</em>,
<p /> <ul>
<li> Edit <strong>/etc/auto.master</strong> and add the line "/usr/local/grid /etc/auto.osg -- timeout 600"
</li> <li> Edit <strong>/etc/auto.osg</strong> and add lines like "app osg-gk.local:/usr/local/grid/app". Similarly add more lines where <em>app</em> is replaced by <em>data,scratch,users,condor</em>
</li> <li> Make the directory <strong>/usr/local/grid</strong>
</li> <li> Issue the command "cluster-fork mkdir /usr/local/grid" - this makes the sub-dir on all the ROCKS compute nodes
</li> <li> Since the ROCKS headnode communicates to the compute nodes via 411, restart this service. Issue the following commands,  <ul>
<li> "make -C /var/411 clean"
</li> <li> "make -C /var/411"
</li> <li> "cluster-fork /opt/rocks/bin/411get" - forces the compute nodes to get the latest 411 information
</li> <li> "cluster-fork service autofs reload"
</li></ul> 
</li></ul> 
<p />
<h5><a name="To_start_CONDOR_at_boot"></a> <font color="#0000ff"> To start CONDOR at boot </font> </h5>
<p />
To ensure that CONDOR starts up at boot, 
<p /> <ul>
<li> Make the directory <strong>/etc/condor/</strong> 
</li> <li> Issue the command, "ln -s /usr/local/grid/condor/etc/condor_config /etc/condor"
</li></ul> 
<p />
<h5><a name="Other_CONDOR_configuration_files"></a> <font color="#0000ff"> Other CONDOR configuration files to be changed </font> </h5>
<p />
I have to admit that somewhere along the way I sort of strayed off the path where CONDOR would have done a lot of work I describe below, especially that relating to the installation on the compute nodes. I am trying to re-trace my steps to sort this out.
<p />
We also need to edit a few CONDOR configuration files on the <font color="#ff0000"> Gatekeeper </font>
<p />
Edit <strong>/usr/local/grid/condor/etc/condor_config</strong>
<p /> <ul>
<li> Set "RELEASE_DIR = /usr/local/grid/condor"
</li> <li> Set "LOCAL_DIR = $(RELEASE_DIR)/local.$(HOSTNAME)"
</li> <li> Set "LOCAL_CONFIG_FILE = $(LOCAL_DIR)/condor_config.local"
</li> <li> Set "CONDOR_HOST = osg-gk.local"
</li> <li> Set "UID_DOMAIN =albany.edu"
</li> <li> Set "TRUST_UID_DOMAIN" = True
</li> <li> Set "FILESYSTEM_DOMAIN" = True
</li> <li> Set "USE_NFS = True"  (can't hurt)
</li></ul> 
<p />
Edit <strong>/usr/local/grid/condor/local.aheposgk/condor_config.local</strong>. I think this file gets created when we initially ran CONDOR as per instructions in the Install guide (Remember, I did all this tweaking after I had set up the system and discovered that CONDOR wasn't working).
<p /> <ul>
<li> Set variables <em>CONDOR_HOST, RELEASE_DIR, LOCAL_DIR, UID_DOMAIN, FILESYSTEM_DOMAIN, USE_NFS_</em> as above.
</li> <li> Set "NETWORK_INTERFACE = 10.1.1.2" (do this at the very top of the file, just after CONDOR_HOST)
</li> <li> Set the variable "DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, SCHEDD" (I didn't want the gatekeeper to run jobs, so I removed STARTD from the list
</li></ul> 
<p />
After all this tweaking, do "condor_reconfig" on the Gatekeeper. Re-reads the config. file(s).
<p />
Wait, we are not done yet. We need to make local config files for the ROCKS compute nodes. I did it by hand (to test the system), but one can do issue a cluster-fork command from the ROCKS headnode and do all compute nodes at once. In any case, I did the following.
<p />
On one of my <font color="#ff0000"> ROCKS compute nodes (compute-0-0.local) </font>, I did the following,
<p /> <ul>
<li> In /usr/local/grid/ do "mkdir condor/local.compute-0.0/" (<font color="#0000ff"> Do I have to make them on all compute nodes by hand? </font> <font color="#008000"> No. If I had configured CONDOR "correctly", I shouldn't need to do this by hand </font>)
</li> <li> "touch condor_config.local" within this sub-dir
</li> <li> Issue the command, "/usr/local/grid/condor/condor_configure --install_dir = /usr/local/grid/condor/ --type=execute --central-manager=osg-gk.local --owner=condor" - this sets up the configuration file (<font color="#0000ff">This step could be done by cluster-fork, but I am not sure. </font> <font color="#008000"> Again, if I had configured CONDOR correctly, I shouldn't need to do this by hand </font>)
</li> <li> Issue the command, "EXPORT CONDOR_CONFIG = /usr/local/grid/condor/etc/condor_config" followed by "condor/sbin/condor_master" - This turns CONDOR on this node
</li> <li> For the other nodes, I did, "cp -rp local.compute-0-0/ local.compute-0-1/", etc. and start CONDOR on those nodes. (<font color="#0000ff"> Is there a better way? </font> <font color="#008000"> Yes, I have to figure it out </font>)
</li></ul> 
<p />
This should get it working...
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Testing_some_of_this_software"></a> Testing some of this software </span></h2>
<p />
I set up another machine where I installed VDT-Client software using the command 
<p />
pacman -get <a href="http://www.cs.wisc.edu/vdt/vdt-136-cache:VDT-Client" target="_top">http://www.cs.wisc.edu/vdt/vdt-136-cache:VDT-Client</a> 
<p />
I put my User Certificate on this machine and made the .pem files as explained on the website mentioned <a href="/bin/view/Main/VivekJainInstallGuideTips#MyCertificates" class="twikiCurrentTopicLink twikiAnchorLink">above</a>
<p />
Make sure that this machine does not have a firewall, or the Gatekeeper will not be able to talk to it.</div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Main<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span><a href="/bin/view/Main/TWikiUsers" class="twikiLink">TWikiUsers</a> &gt; <a href="/bin/view/Main/VivekJain" class="twikiLink">VivekJain</a><span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>VivekJainInstallGuideTips</span> <br />    
Topic revision: r13 - 07 Feb 2017 - 19:16:30 - <a href="/bin/view/Main/BrianBockelman" class="twikiLink">BrianBockelman</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
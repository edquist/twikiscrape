---+!! Overview of Single Epoch Processing for the Dark Energy Survey

Systems for processing Dark Energy Survey telescope
data are being developed mainly by the
[[https://desweb.cosmology.illinois.edu:8443/confluence/display/PUB/The+Dark+Energy+Survey+Data+Management+Project][DESDM]] project.

This page gives a high level overview of the processing
that just a single exposure (&quot;epoch&quot;) needs.

%TOC%

---++ Goals

The goal of this project is to study how practical it is to run the
single exposure processing on Grid worker nodes.

The main steps of this project are:

   1. Building and adapting the image processing applications for
   standard Grid platforms.
   1. Adapting the image processing applications to work in single exposure 
   mode. This includes modifying the remap step.
   1. Adapting the prototype orchestration scripts from NCSA to properly
   pass information across computational steps.
   1. Porting the single exposure pipeline to run on a Grid cluster. Scaling
   up the usage of the Grid to 300 images at a time, either pre-fetched or 
   transferred upon request.
   1. Measuring standard computational metrics when running on the Grid. 
   These metrics include application run time, time delays for data 
   transfers, performance of local peripherals, etc.


---++ High-level Description of Processing

---+++ Inputs

   1. An image from the telescope. This takes up about 2GB.
   1. A bias image -- 2 GB.
   1. A flat image -- 2 GB.

   #2 and #3 only change once every night or two.

    Details:
   * One CCD image is 4k x 4k x (2 bytes or 4 bytes),
        which works out to 32 or 64 MB.  One exposure
        consists of 62 CCD images, which works out to a
        total of 2 GB or 4 GB.

   * There are 5 different bands (grizY), but each
        band is done as a separate exposure.

   * Taking an exposure (with DECAM) takes ~100
        seconds. The filter has to be swapped for the
        next exposure.


---+++ Outputs

   1. A reduced image with SCAMP headers  (4GB).
       This is to help check the processing.
   1. A remapped image (4GB).
   1. A small table from PCM (~100MB).

---+++ Time Estimates

  The processing for one exposure should be on the order of
  10 hours (?).

  Usually about 300 exposures should be processed every day if the sky
  is clear enough that the telescope is in Standard Survey Mode. If a
  few images take longer that&#39;s ok.

  If there are a few thin clouds then the telescope
  will be in Supernova Mode, since Standard Survey Mode
  needs excellent conditions. Supernova Mode generates
  only about 100 images a night, but it is important
  that these be processed before the next night so that
  any supernovas can be followed up on.

---+++ Processing

&lt;verbatim&gt;
   calibration    processing
   data needed    steps
              -----
       
   Bias           imcorrect
   Flats          crosstalk
                     These process the raw data into &quot;reduced images&quot; or &quot;reds&quot; for short.
                     Each pixel becomes a 4 byte float.
                
   known          sextractor #1 -- detect bright objects
   objects        scamp -- matches bright objects with known objects. Then we know
   UNSO-B                  what the CCD is looking at. Scamp writes this data into the
   catalog                 FITS headers (doesn&#39;t change the pixels). The output is
                           &quot;reduced+SCAMP headers&quot; and also a few numbers
                           that say how to rotate and stretch the image to match
                           the fixed coordinate system.
                
                  remap
                    Align the chips so that they are on a fixed coordinate system.
                    &quot;RA&quot; is the x-axis, &quot;DEC&quot; is the y-axis.

                  Full sextractor -- measures and detects all the objects in the images.
                     Computationally intensive step. ~5 minutes for each chip (?) =&gt; 5 hours.

                  PSF calculation

  photometric     PCM  -- Photometric Calibration Module
  solution            Produces a table of objects with columns like these:         
                        run id, object id, ra, dec, magnitude
&lt;/verbatim&gt;


---+++ Notes

  Human intervention is needed to fix at least scamp
  failures. Some questions about whether processing
  should continue if scamp fails on only a few chips.

  Have to return the logs to help debugging.

  After single epoch processing, will do coadd
  processing. Also need to run science code like the
  weak lensing processing.

  Can&#39;t do just one chip at a time, since some steps
  need a few chips. For example, crosstalk correction
  needs 12. The full sextractor can be done separately
  for each chip. DAGMAN could be useful for splitting
  up the processing if this takes a long time.

  Could write small scripts to interact with the
  database when needed.

---+++ Running on the Grid

There is a preliminary system for [[DESonGRID_initial][running many of these steps on the grid]].

---+++ Limited Public Access to DES SV data
[[https://twiki.grid.iu.edu/bin/view/Trash/Trash/VirtualOrganizations/DESUserAccess][Limited Access to DES Data for November, 2012 SV run]]


  ---The Dark Energy Survey and OSG/FNAL User Support


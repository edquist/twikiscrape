

---+ Modeling the Perfect Fluid

Duke University physicist Steffen A. Bass uses big chunks of computing time to study the behavior of some of the universe’s most fundamental particles—quarks and gluons, which existed in an unbound state up until about a micro-second after the Big Bang. 

Bass and fellow researchers Berndt Mueller, a Duke physics professor, visiting assistant professor Hanna Petersen, graduate student Christopher Coleman and undergraduate physics major Vivek Bhattacharya have performed numerical simulations of experiments at CERN’s Large Hadron Collider (LHC) and Brookhaven National Laboratory’s Relativistic Heavy Ion Collider (RHIC) in an effort to recreate that instant when temperatures and pressures in the young universe were greater than those found in the hottest stars—so great that quarks and gluons, particles that are always bound together, were for a fleeting instant unbound and free flowing, forming what physicists call the Quark-Gluon Plasma (QGP). 
The computer simulations run by the group of Professors Bass and Mueller often require as many as 10,000 model runs to follow for all the possible interactions and trajectories of the quarks and gluons that occur when ions are smashed together at nearly the speed of light to create conditions that make the QGP possible. The team first used the Open Science Grid in 2007 after learning about it through RENCI, which leads the OSG Engage Virtual Organization. 

Since then the Engage team has collaborated closely with the researchers to provide high throughput computing expertise and access to the latest hosted Open Science Grid infrastructure. This includes a dedicated Engage VO submit host running Condor, GlideinWMS and Pegasus WMS. Engage team members help identify the best ways to access advanced OSG resources, design next generation usage models including multi-core applications, and help the team troubleshoot issues. Lessons learned in the process in turn are communicated to the OSG community and help inform its work.
The researchers use a new hydrodynamic model to simulate the flow of the unbound quarks and gluons.  In the last year, they’ve racked up more than 4 million CPU hours on OSG resources recreating the conditions of experiments at RHIC and LHC and testing new statistical techniques for modeling and visualizing large complicated data sets. According to Bass, OSG resources are well suited to running their simulations, since the thousands of model runs can run simultaneously on distributed computing resources.

“The scale of the simulations we are doing now would not have been possible if RENCI had not introduced us to the OSG,” said Bass. “Through the OSG Engage Virtual Organization, we’ve been able to work with experts who understand our needs and who make accessing and using OSG resources as uncomplicated as possible. Thanks to these resources, we are able to do calculations that were not possible before and to think about problems in new ways.”

The scientific results of their work has been significant: the QGP state that they’ve modeled and that has been observed at RHIC and LHC is not a weakly interacting, gas-like system as physicists once expected, but a strongly interacting liquid-like system with near zero shear-viscosity. In Bass’s words, “It is nature’s most perfect fluid.”

Through a National Science Foundation Cyber-enabled Discovery and Innovation (CDI) award, the team is pioneering new ways of analyzing their massive data sets, including visualizing the interactions among particles (in collaboration with experts in the University of North Carolina computer science department) and applying new approaches to quantify, analyze and reduce uncertainty in their complex models. 

Typically, computers will run a model thousands of times to control for uncertainties, a process that eats up CPU hours and becomes impractical as data sets grow larger and scientists seek to simulate more complicated, multidimensional problems with unknown outputs. The Duke research team is developing a new computational approach that involves building a statistical approximation of a computer model, which can emulate the output of computer simulations using fewer model runs. 

“We are looking at ways to better manage HPC resources and take away some of the manual work that is still required,” said Bass. “The package of tools we are developing could be useful to any scientist who is modeling the behavior of complex systems.”


-- Main.RuthPordes - 08 Dec 2011

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> Phenomenology &lt; Engagement &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Engagement/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Engagement/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Engagement/Phenomenology?_T=16 Feb 2017" type="application/x-wiki" title="edit Phenomenology" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/bin/view/Engagement/WebRss" />    
<base href="https://twiki.opensciencegrid.org/bin/view/Engagement/Phenomenology"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#D0D0D0;
	}
	.patternBookView {
		border-color:#D0D0D0;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">



</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <h1><a name="Running_Phenomenology_Codes_on_O"></a>  Running Phenomenology Codes on OSG </h1>
<p />
The <a href="http://www.slac.stanford.edu/grp/th/th.html" target="_top">SLAC Theory Group</a>
has started working with OSG User
Support to try running some phenomenology codes on OSG
as a proof-of-principle.
<p />
The theory group works with
the following experiments within SLAC:
<a href="http://atlas.web.cern.ch/" target="_top">ATLAS</a>,
<a href="http://www-public.slac.stanford.edu/babar/" target="_top">BaBar</a>,
<a href="http://cdms.berkeley.edu/" target="_top">CDMS</a>,
<a href="http://fgst.slac.stanford.edu/" target="_top">FGST</a>,
BICEP/SPUD,
and Super-B. More broadly, it works with
<a href="http://cms.web.cern.ch/" target="_top">CMS</a>,
<a href="http://lhcb.web.cern.ch/" target="_top">LHCb</a>,
<a href="http://www-cdf.fnal.gov/" target="_top">CDF</a>,
<a href="http://www-d0.fnal.gov/" target="_top">D0</a>,
<a href="http://h1.desy.de/" target="_top">H1</a>,
<a href="http://www.lnf.infn.it/kloe/" target="_top">Kloe</a>,
<a href="http://www.rssd.esa.int/index.php?project=Planck" target="_top">Planck</a>,
PAMELA/HESS,
<a href="http://www.gsi.de/" target="_top">GSI</a>,
<a href="https://www.jlab.org/" target="_top">Jlab</a>, and
<a href="http://www.bnl.gov/rhic/" target="_top">RHIC</a>.
<p />
<p />
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Job_Requirements"> Job Requirements</a>
</li> <li> <a href="?cover=print#Basic_Idea_for_Data_Handling"> Basic Idea for Data Handling</a>
</li> <li> <a href="?cover=print#Using_iRODS_and_Running_Multi_Co"> Using iRODS and Running Multi-Core Jobs</a>
</li> <li> <a href="?cover=print#Needs_for_MPI_Jobs_from_Septembe"> Needs for MPI Jobs (from September Meeting)</a>
</li></ul> 
</div>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Job_Requirements"></a> Job Requirements </span></h2>
<p />
We will initially be running a pair of applications,
<a href="http://projects.hepforge.org/sherpa/dokuwiki/" target="_top">Sherpa</a>
and
<a href="http://today.slac.stanford.edu/feature/2008/theory-one-loop-computations.asp" target="_top">Blackhat</a>,
that do multiparticle QCD calculations.
<p />
In actual use they will produce about 2 or 3 GB of data
stored in a root NTuple, and take about 8 to 12 hours
to run.
<p />
They are independent Monte Carlo jobs whose output
files will record the random number seed, so no special
parallelization is needed. If some jobs fail, we don't
have to resubmit them.
<p />
There will be very roughly 500 jobs submitted at
one time for the proof-of-principle phase.
<p />
The executables will be 2 or 3 GB also, and should
be prestaged at the sites. The executable
shouldn't change too often--on the order of once
per year.
<p />
For testing, the output data will be more like
160KB, and the executable and input data are in a
50MB file.
<p />
Ideally we should be able to do a single
exploration ("pilot") run to figure out how many
jobs will be needed, and then submit all the jobs.
The variable number of jobs may be difficult to
handle with Condor DAGMAN.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Basic_Idea_for_Data_Handling"></a> Basic Idea for Data Handling </span></h2>
<p />
We would direct jobs to sites that have a hadoop
file system available. Each job can copy its output
data to its site's file system, usually via a POSIX
interface, before exiting. Then, later, the user can
retrieve and delete the data from the site using srm
or gridftp.
<p />
There are
<a href="https://twiki.grid.iu.edu/bin/view/Trash/ReleaseDocumentationOSGStorageDiscoveryTool" target="_top">OSG Discovery Tools</a> 
that indicate which sites have storage elements and (at least nominally) how much room
they have.
<p />
As an example
<pre class="screen">
   get_srm_storage_element_id --vo engage --free_online_size 100000 --show_site_name
</pre>
<p />
The User Support team wrote custom scripts to implement a workflow
that uses SRM to store output data. It was mostly serviceable but
had limited functionality and could lose track of data.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Using_iRODS_and_Running_Multi_Co"></a> Using iRODS and Running Multi-Core Jobs </span></h2>
<p />
The User Support group has implemented a workflow that
uses <span class="twikiNewLink">iRODS<a href="/bin/edit/Trash/Trash/VirtualOrganizations/IRODSOSG?topicparent=Engagement.Phenomenology" rel="nofollow" title="iRODS (this topic does not yet exist; you can create it)">?</a></span> to stage
the application to $OSG_APP, and to store the output data
from the application. This would replace the "basic" handling
mentioned above. We are also trying to run this workflow
on sites that support
<a href="/bin/view/Documentation/HighThroughputParallelComputing" class="twikiLink">HTPC</a>.
<p />
Here's a draft of the instructions:
<p />
<pre>
Running Multi-Core Jobs Using iRODS for Data Handling

0. Setup the environment:

     voms-proxy-init -valid 48:00 -voms osg:/osg/Pheno
     source /opt/irods_client/setup_irods.sh

   It's important that the proxy not expire before the
   jobs are done, otherwise iRODS won'd be able to save
   the output data.

1. Create a tar file with the application, and move it
   to the iRODS store at each site:

      tar cfh test_application.tar test_application
      iput -R  osgAppGridFtpGroup test_application.tar
      irepl-osg -f /osg/home/pheno/test_application.tar -G osgAppGridFtpGroup
      for site in $(ilsresc-osg -G osgAppGridFtpGroup | perl -ne '/Group: osg Resource: (\S+)/ and print "$1 "'); do ibun-osg  -f
test_application.tar -R $site -G osgAppGridFtpGroup; done

   When this step is done, can use

      ils -l /osg/home/pheno

   to see the untarred files on the submit host. From
   the jobs, the files are accessible using normal file
   system operations at

         $OSG_APP/osg/irods/pheno

   Other notes:
     Use -f with iput to overwrite a file that is already there.
     Can use -a with ibun to do the operations
     asynchronously with email notification.
     The tar file shouldn't be compressed.

   Files do not get directly uploaded to Tusker. iRODS
   puts them at GLOW, and then relies on cvmfs to move
   them to Tusker which should take about an hour.


2. Create the submit file. It should have lines like
   this

       x509userproxy = /tmp/x509up_uZZZZ
       +UsesiRODS=True
       +SiteList = "GLOW,UCSDT2,Firefly,Tusker,Nebraska,prariefire"
       Requirements = (stringListMember(GLIDEIN_ResourceName,SiteList) == True)
       +RequiresWholeMachine = True
       +ProjectName = "Pheno"

   The ZZZZ in the x509userproxy is your unix user id
   available with the 'id' command.

   The five listed sites are all set up for iRODS and
   should work for multicore jobs, although so far our
   fully-featured test jobs have only run at Tusker.

   We also have an example of a run available.

        We are in the process of testing a
        DESIRED_HTPC_Resources attribute. It's similar
        to SiteList above, but should direct glideins
        to only the listed sites.

3. Create a wrapper that will run the job. The condor
   jobs should no longer use "transfer_output_files"
   for anything big. The necessary lines for iRODS on
   the worker node could look something like this:

     OUTPUT=tp_outputA_$1_$2.txt
     export JOB_ID=$1_$2
     $IRODS_PLUGIN_DIR/icp $OUTPUT irodse://pheno@gw014k1.fnal.gov:1247?/osg/home/pheno/$OUTPUT
     if [ $? -eq 0 ]
     then
        echo "`date` icp success"
        rm -f $OUTPUT
     else
        echo "`date` icp failure"
     fi

   This "if statement" doesn't delete the output file
   if the icp fails, which allows condor to bring
   that file back.

   It might be best for the worker node to make a
   single tar file to hold all the files to put in
   iRODS.

4. Submit the jobs

      condor_submit test18_pheno.condor

   and wait for them to run and finish.

5. Look at and possibly download the output and remove it:

     ils -l /osg/home/pheno
     iget /osg/home/pheno/tp_outputA_863567_9.txt
     irm -f /osg/home/pheno/tp_outputA_863567_9.txt
</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Needs_for_MPI_Jobs_from_Septembe"></a> Needs for MPI Jobs (from September Meeting) </span></h2>
<p />
Would like to try running MPI jobs. Ideally there would be
about 128 jobs of 32 cores that run for 23 hours each, or
approximately 100K hours. Would also be ok to have 500
jobs of 8 cores for 23 hours each. May be ok to spread
out runs over about 3 days.
<p />
-- OSG User Support</div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Engagement<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>Phenomenology</span> <br />    
Topic revision: r6 - 07 Feb 2017 - 18:20:18 - <a href="/bin/view/Main/BrianBockelman" class="twikiLink">BrianBockelman</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
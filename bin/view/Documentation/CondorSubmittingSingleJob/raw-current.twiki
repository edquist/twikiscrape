%DOC_STATUS_TABLE%

---+!! *Submitting a Single Job Using Condor-G*
%TOC{depth=&quot;3&quot;}%

---+ About this Document
In this guide, we will send a single &quot;hello world&quot; Condor-G job to a remote CE.  This will demonstrate the basics of using Condor-G: writing a job submit file, submitting it, and checking its status.

More complex examples in follow-up documents show how to [[CondorSubmittingMultipleJobs][submit multiple jobs]] and [[CondorSubmittingMultipleComplexJobs][submit multiple complex jobs]] (including IO transfer).

---+ Requirements
In order to complete this document, you will need:
   * A DOE Grid [[Documentation.CertificateUserGet][User Certificate]]
   * Membership in a [[Documentation.WhatIsVO][Virtual Organization]]
   * Access to a machine with the [[Documentation.Release3.InstallOSGClient][OSG-Client]] (or [[ReleaseDocumentation.ClientInstallationGuide][OSG-Client 1.2, Pacman version]]) installed. Remember to install also Condor-G if optional and to setup the environment if needed.
   * Know the endpoint you are submitting to, as covered in the [[FindAvailableResource][finding available resources]] document.

%STARTINCLUDE%

---+ Procedure

---++&quot;Hello World&quot; Condor Job

&lt;!-- Using HTML because Twiki lists would have been too complex given the level of nesting --&gt;
&lt;ol&gt;
  &lt;li&gt;Before you submit a job to condor, you need to write a simple script to execute!  We provide one below that will print out a few environment variables, setup the worker node client, and sleep for a bit:
&lt;br/&gt;
%TWISTY{
mode=&quot;div&quot;
showlink=&quot;Show simple script file...&quot;
hidelink=&quot;Hide simple script file&quot;
showimgleft=&quot;%ICONURLPATH{toggleopen-small}%&quot;
hideimgleft=&quot;%ICONURLPATH{toggleclose-small}%&quot;
}%
&lt;pre class=&quot;file&quot;&gt;
#!/bin/sh
echo &quot;Hello from `hostname`&quot;
echo &quot;Going to source the file $OSG_GRID/setup.sh&quot;
source $OSG_GRID/setup.sh
echo &quot;Resulting environment:&quot;
printenv
echo &quot;Output of lcg-cp --help (lcg-cp is a commonly used SRM client):&quot;
lcg-cp --help
echo &#39;Directories in \$OSG_APP&#39;
ls $OSG_APP
date
sleep 120
date
&lt;/pre&gt;
%ENDTWISTY%
&lt;br/&gt;
Name this script =mytest.sh= and set it executable with =chmod +x mytest.sh=.
  &lt;/li&gt;
  &lt;li&gt;Create a file named =condorg_test.submit= that contains the following (we will cover this line-by-line below):
&lt;br/&gt;
%TWISTY{
mode=&quot;div&quot;
showlink=&quot;Show condor submit file...&quot;
hidelink=&quot;Hide condor submit file&quot;
showimgleft=&quot;%ICONURLPATH{toggleopen-small}%&quot;
hideimgleft=&quot;%ICONURLPATH{toggleclose-small}%&quot;
}%
&lt;pre class=&quot;file&quot;&gt;
Universe        = grid
grid_resource = gt2 red.unl.edu:/jobmanager-condor
Executable      = mytest.sh
Output          = job_test.output
Error           = job_test.error
Log             = job_test.log
queue
&lt;/pre&gt;
%ENDTWISTY%
Replace the endpoint =red.unl.edu:/jobmanager-condor= with the endpoint you selected from the [[FindAvailableResource][finding available resources guide]].
   &lt;/li&gt;
   &lt;li&gt;Create a proxy that indicates which VO you are part of:
 &lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% voms-proxy-init --voms cms:/cms
&lt;/pre&gt;
Replace =cms:/cms= as appropriate for your VO.
   &lt;/li&gt;
   &lt;li&gt;Submit the =condorg_test.submit= job:
&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% condor_submit condorg_test.submit
&lt;/pre&gt;
   &lt;/li&gt;
   &lt;li&gt;Give yourself some time. Grid middleware can add a latency of about a minute on top of the amount of time your job spends in the batch system queue (if any time).  While you&#39;re waiting, read on through the rest of the guide for an explanation of the Condor submit file.
   &lt;/li&gt;
   &lt;li&gt;Use =condor_q= to display information about your jobs:
&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% condor_q -globus
&lt;/pre&gt;
You should expect to see the jobs idle for a minute or two, go into the running state for a minute or two, and then end.  You can also follow along in the log file job_test.log which Condor should create in your working directory.

Here is sample output from =condor_q -globus=:
&lt;br/&gt;
%TWISTY{
mode=&quot;div&quot;
showlink=&quot;Show condor output...&quot;
hidelink=&quot;Hide condor output&quot;
showimgleft=&quot;%ICONURLPATH{toggleopen-small}%&quot;
hideimgleft=&quot;%ICONURLPATH{toggleclose-small}%&quot;
}%
&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% condor_q -globus


-- Submitter: hcc-grid.unl.edu : &lt;129.93.229.131:50400&gt; : hcc-grid.unl.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
7709.62  %UCL_USER%      ACTIVE  fork     condor.crc.nd.edu  /opt/osg/osg-120/o
7709.802 %UCL_USER%      ACTIVE  fork     condor.crc.nd.edu  /opt/osg/osg-120/o
7709.816 %UCL_USER%      ACTIVE  fork     tuscany.med.harvar  /opt/osg/osg-120/o
7990.0   %UCL_USER%      PENDING condor   condor.crc.nd.edu   /opt/osg/osg-120/o
8078.0   %UCL_USER%      PENDING condor   gridgk02.racf.bnl.  /opt/osg/osg-120/o
8079.0   %UCL_USER%      PENDING pbs      condor.crc.nd.edu  /opt/osg/osg-120/o
8064.0   %UCL_USER%      PENDING condor   condor.oscer.ou.ed  /opt/osg/osg-120/o
8073.0   osgmm         PENDING sge      antaeus.hpcc.ttu.e  /opt/osg/osg-120/o
&lt;/pre&gt;
%ENDTWISTY%
    &lt;/li&gt;
&lt;/ol&gt;


---++ Condor-G submit files
In this section, we will cover different essential lines in the simple Condor-G submit file used above.
&lt;pre class=&quot;file&quot;&gt;
universe=grid
grid_resource = gt2 red.unl.edu:/jobmanager-condor
&lt;/pre&gt;
Setting the Condor universe to grid enables the Condor-G mode for this job.  In the grid universe, we always must specify what resource to submit to.  The two important parts of this line are &quot;gt2&quot; (referring to the GRAM protocol; you will always use this for OSG sites. &quot;gt2&quot; is actually the default protocol, so you can omit that from the line) plus the GRAM endpoint of the remote host, including the jobmanager name.  Note that we are manually steering this job to a specific cluster - no matchmaking will be done.

Next, we specify what executable to run, where to place the stdout/stderr when the job is done, and where to write the condor log file:
&lt;pre class=&quot;file&quot;&gt;
Executable      = mytest.sh
Output          = job_test.output
Error           = job_test.error
Log             = job_test.log
&lt;/pre&gt;
The output and error are only copied over by Condor-G when the job ends - the output is not streamed back to your submit host as it runs.  The log file will be updated by Condor whenever the job state changes.  The =mytest.sh= script is also copied over to the remote worker node by Condor.

%STOPINCLUDE%

---+ Conclusions

Once this job has completed, congratulations!  You have run your first Condor-G job.  This tutorial covered running a single job, consisting of a single script, no input, and only the stdout/stderr returned.  The job demonstrates using Condor-G, but does &quot;nothing interesting&quot;.

We will continue on with more complex examples which include:
   * Submitting many unique jobs from a single submit file
   * Transferring input and output files with Condor-G
   * Passing arguments to the script
   * Running workflows where the jobs have dependencies on each other.
   * Understanding and using the OSG worker node user environment.

---+ Relerences

   * Tutorial about [[CondorSubmittingMultipleJobs][submitting multiple jobs]].
   * Tutorial about [[CondorSubmittingMultipleComplexJobs][submitting multiple complex jobs]] (including IO transfer).
   * [[http://www.cs.wisc.edu/condor/condorg/][Condor-G documentation]].

---+!! *Comments*
%COMMENT{type=&quot;tableappend&quot;}%

&lt;!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = BrianBockelman

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = User

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = Scientist

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Training
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = AshuGuru
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = MarcoMambelli
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %YES%
############################################################################################################
--&gt;

%LINKCSS%

---+!! Tutorial: An Introduction to the Grid
%TOC%

These exercises explore some simple Grid operations to start developing a few very basic techniques for distributed computing.

We assume you&#39;re using the default &quot;bash&quot; shell set up on your logins on the laptops and lab servers. We recommend you don&#39;t change your shell.

---++ Connecting to the Linux training hosts 


You will be doing almost all the lab exercises this week on a set of Linux computers (&quot;hosts&quot;) named =workshop1=, =workshop2=, =workshop3= and =workshop4= (its &quot;fully qualified host name&quot; looks like &quot;%LOGINHOST%&quot;).  From these machines, we will run Grid jobs and explore various Grid sites.

To access this machine from your computer, we will use &quot;secure shell&quot;.

On a Windows machine, we will use the &lt;nop&gt;PuTTY program. Open &lt;nop&gt;PuTTY and enter the hostname of the computer that you will use.

On a Mac, we will use the Terminal and ssh command-line tool. Open Terminal and type =ssh= followed by your training user name, an &quot;@&quot; symbol and the name of the computer you wish to connect to. For example:

&lt;pre class=&quot;screen&quot;&gt;
mac$ &lt;userinput&gt;ssh %LOGINNAME%@%LOGINHOST%&lt;/userinput&gt;
The authenticity of host &#39;%LOGINHOST% (%LOGINIP%)&#39; can&#39;t be established.
RSA key fingerprint is 36:74:78:a8:ed:6b:38:96:63:20:01:df:46:9b:59:3b.
Are you sure you want to continue connecting (yes/no)? &lt;userinput&gt;yes&lt;/userinput&gt;
Warning: Permanently added &#39;%LOGINHOST%,%LOGINIP%&#39; (RSA) to the list of known hosts.
%LOGINNAME%@%LOGINHOST%&#39;s password: &lt;userinput&gt;PASSWORD&lt;/userinput&gt;  &lt;em&gt;# not echoed !!!&lt;/em&gt;
%LOGINHOSTSHORT%$    &lt;em&gt;# Now you&#39;re talking to a shell on the workshop1 server&lt;/em&gt;
&lt;/pre&gt;

After the first time you do this, you won&#39;t get the &quot;Are you sure.&quot; prompt. Some of you will never see this, as your computers were used for testing this material, and the &quot;yes&quot; reply was already supplied by a tester.  So it will look like:

&lt;pre class=&quot;screen&quot;&gt;
mac$ &lt;userinput&gt;ssh %LOGINNAME%@%LOGINHOST%&lt;/userinput&gt;
Password: &lt;userinput&gt;PASSWORD&lt;/userinput&gt; 
%LOGINHOSTSHORT%$
&lt;/pre&gt;

You should be able to reach all four lab hosts - %LOGINHOSTSHORT% ... workshop4 - in this manner.

For the rest of this exercise, we will use simpler prompts: =$laptop= for commands typed directly to the shell of your laptop Terminal window, and =%LOGINHOSTSHORT$= for the %LOGINHOSTSHORT% lab server.


---++ Conventions

Note the conventions for command-line dialogs that will be used throughout these exercises:

What you type &lt;tt&gt;&lt;userinput&gt;looks like this&lt;/userinput&gt;&lt;/tt&gt;. The system&#39;s responses =look like this=.

Our comments to you (which you should *not* enter) look like:  =&lt;em&gt;# some comment here&lt;/em&gt;=

&lt;pre class=&quot;screen&quot;&gt;
laptop$ &lt;userinput&gt;pwd&lt;/userinput&gt;    &lt;em&gt;# You type &quot;pwd&quot; but NOT this comment!&lt;/em&gt;
/home/%LOGINNAME%   &lt;em&gt;# the system&#39;s response&lt;/em&gt;
laptop$          &lt;em&gt;# the system&#39;s command prompt&lt;/em&gt;
&lt;/pre&gt;

In these exercises, we&#39;ll use the prompt =laptop$= for responses from the shell on the lab laptops, and =workshop$= (or =%OTHERHOSTSHORT%$=, =workshop3$=, etc.) for responses from remote shells. 

%NOTE% The actual shell prompts you&#39;ll see may be different from those we show here.

To start, practice cutting text from your this page in your web browser to run a command or set of commands: cut the =pwd= command from the box above and paste it into your terminal window to execute it. This is a good way to avoid making typos while entering commands from the examples.


&lt;!-- ***  Comments plugin to create comments table for section   ***    --&gt;

&lt;span style=&quot;text-align:center; font-weight:bold; font-size:1.2em;&quot;&gt;ADD A COMMENT&lt;/span&gt;
%STARTMore%

%TABLE{ }%
|  *COMMENT*  |  *NAME*  |  *DATE*  |
%COMMENT{ type=&quot;tableappend&quot; }%

%ENDMore%
&lt;!-- ***  End Comment                                            ***    --&gt;


---++ &quot;Logging in&quot; to the Grid

%INCLUDE{ &quot;LoggingInToTheGrid&quot; }%

---++ Running jobs with Globus Commands

%INCLUDE{ &quot;RunningJobsWithGlobusCommands&quot;}% 

---++ Running under a remote shell

%INCLUDE{ &quot;RunningUnderARemoteShell&quot; }% 

---++ Immediate (fork) and Batch (scheduled) Job managers


Notice that we follow the system name that you want to run the command on with the qualifier =/jobmanager-fork=. GRAM, the Globus protocol for running remote jobs, supports the concept of a &quot;job manager&quot; as an adapter to local job management environments.  Each &quot;Grid site&quot; - or collection of resources - can support one or more such job managers. The &quot;fork&quot; job manager runs an immediate job through the UNIX fork() interface. Another job manager we have installed on the gridlab hosts, =jobmanager-condor= acts as an interface to the Condor batch scheduling system.

Which do you think will be faster?  Lets find out: use the command =time= to test which jobmanager is faster.

To time a command, just enter =time &lt;em&gt;commandname&lt;/em&gt;=:

&lt;pre class=&quot;screen&quot;&gt;
%LOGINHOSTSHORT%$ &lt;userinput&gt;time sleep 3&lt;/userinput&gt;
&lt;/pre&gt;

Use this to time a few trivial Grid jobs to compare =jobmanager-fork= and =jobmanager-condor=.

The &quot;fork&quot; job manager is very fast - it has rather low &quot;scheduling latency&quot;.  It runs trivial commands very quickly.  But it also has no compute power - its usually just a single CPU on a cluster-controlling computer called the _gatekeeper_ or _headnode_.  A batch job manager, on the other hand, has a higher scheduling overhead.  But it gives you access to all computers in a cluster, and the opportunity to do real parallel computing.

It turns out that each of our workshop machines is just an ordinary single-processing machine. We&#39;ve just &quot;tricked&quot; Condor onto thinking that it had a real &quot;cluster&quot; there with a set of CPUs. (We configured each of the hosts with 16 such virtual CPUS).

While our four gridlab hosts use the Condor scheduler, and thus =jobmanager-condor=, other systems use other schedulers. For example, systems using _PBS_ (Portable Batch System) require that you use =jobmanager-pbs=.  You&#39;ll get a chance to try this later in this lab.

Now try running a few jobs through condor:

&lt;pre class=&quot;screen&quot;&gt;
%LOGINHOSTSHORT%$ &lt;userinput&gt;globus-job-run %OTHERHOST%:/jobmanager-condor /bin/hostname&lt;/userinput&gt;
&lt;/pre&gt;

Next try starting five such jobs on workshop3 at once: put an &quot;&amp;amp;&quot; at the end of the line, and either use cut-and-past, or shell command history, or a simple shell script to run five of these commands at once.  


&lt;!-- ***  Comments plugin to create comments table for section   ***    --&gt;

&lt;span style=&quot;text-align:center; font-weight:bold; font-size:1.2em;&quot;&gt;ADD A COMMENT&lt;/span&gt;
%STARTMore%

%TABLE{ }%
|  *COMMENT*  |  *NAME*  |  *DATE*  |
%COMMENT{ type=&quot;tableappend&quot; }%

%ENDMore%
&lt;!-- ***  End Comment                                            ***    --&gt;




---++ Running a simple application remotely, through &quot;staging&quot;

%INCLUDE{ &quot;RunningSimpleApplicationsRemotelyThroughStaging&quot; }%

%STARTSECTION{&quot;TutorialVars&quot;}%
&lt;!--                                                                            
      * Set LOGINHOST = workshop1.lac.uic.edu
      * Set LOGINHOSTSHORT = workshop1
      * Set LOGINIP = 131.193.181.56
      * Set GRIDHOST = tg-login.sdsc.teragrid.org
      * Set OTHERHOST = workshop2.lac.uic.edu
      * Set OTHERHOSTSHORT = workshop2
      * Set CERTSUBJECT = /O=Grid/OU=OSG/CN=Training User 99
      * Set LOGINNAME = train99
      * Set HOMEDIR = /home/%LOGINNAME%
--&gt; 
%ENDSECTION{&quot;TutorialVars&quot;}%
%BOTTOMMATTER%

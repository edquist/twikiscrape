---+ 2013 OSG User School Infrastructure

This page is primarily for instructors and sysadmins to document the computing infrastructure for the School.

Mostly, the School infrastructure is located at UW–Madison and is set up and maintained by the CHTC Infrastructure team, primarily Nate Yehle (the glideinWMS submit node is the exception, see below). The Madison systems are virtual machines.

In 2012, the School infrastructure was done on virtual machines, all at Madison. Dan Rao set up the virtual machine instances using Cobbler (we think). Mat might know a bit since he interacted with Dan on the VDT test computers that were also set up with Cobbler. Ken may also know.

We have four systems, each installed with parts of the OSG software stack, 3.1 release. The UW hostnames all contain =ss= in the name for Summer School, but they probably should have used =us= for User School instead.

---++ Submit Nodes

---+++ UW–Madison Local Submit Node

*Purpose:* Allow submissions to the local cluster and, using HTCondor-G, to the local and remote CEs.

*Use:* Students login directly to this machine to do most of their work, especially on Monday.  Will also be used on Wed. for storage exercises, hence the httpd server.

| *Hostname* | osg-ss-submit.chtc.wisc.edu |
| *OS* | Scientific Linux 5.6 (Boron) |
| *Packages* | osg-client, httpd |
| *Logins* | Students, Instructors |
| *Contacts* | Neil Van Lysel &amp;lt;van-lyse@cs.wisc.edu&amp;gt;&lt;br&gt;Aaron Moate &amp;lt;moate@cs.wisc.edu&amp;gt;&lt;br&gt;Nate Yehle &amp;lt;nyehle@cs.wisc.edu&amp;gt; |

---+++ !GlideinWMS Submit Node

Igor wants a separate machine for running the glidein submit node. Students submitted glidein jobs from there. I really really think it should have been a single computer (i.e., osg-ss-submit) because our principle is that you should submit locally, then grow to submit to the grid in the same environment. I lost that argument with Igor, but I believe Miron agrees with me. This setup was expedient for Igor.

| *Hostname* | osg.ctbp.ucsd.edu |
| *OS* | !CentOS 5 |
| *Packages* | osg-client |
| *Logins* | Students, Instructors |
| *Contacts* | Igor Sfiligoi &amp;lt;sfiligoi@fnal.gov&amp;gt;&lt;br&gt;Terrence Martin &amp;lt;tmartin@physics.ucsd.edu&amp;gt; |

---++ Compute Element (CE)

*Purpose:* A standard OSG CE for demonstrating the use of Condor-G.

*Use:* This machine is part of the grid discussion on Tuesday. Students do not use it directly, but Igor demonstrates jobs running via Condor-G with it.

| *Hostname* | osg-ss-ce.chtc.wisc.edu |
| *OS* |  |
| *Packages* | osg-ce, globus-gram-job-manager-condor |
| *Logins* | — |
| *Contacts* | Neil Van Lysel &amp;lt;van-lyse@cs.wisc.edu&amp;gt;&lt;br&gt;Aaron Moate &amp;lt;moate@cs.wisc.edu&amp;gt;&lt;br&gt;Nate Yehle &amp;lt;nyehle@cs.wisc.edu&amp;gt; |

---++ Storage Elements

---+++ UW–Madison NFS Server

*Purpose:* An NFS server with a volume that is mounted on the UW–Madison reserved worker nodes (see below).

| *Hostname* | osg-ss-se.chtc.wisc.edu |
| *OS* | |
| *Packages* | |
| *Logins* | — |
| *Contacts* | Neil Van Lysel &amp;lt;van-lyse@cs.wisc.edu&amp;gt;&lt;br&gt;Aaron Moate &amp;lt;moate@cs.wisc.edu&amp;gt;&lt;br&gt;Nate Yehle &amp;lt;nyehle@cs.wisc.edu&amp;gt; |

---+++ HTTP Proxy Host

| *Hostname* | proxy.chtc.wisc.edu |
| *OS* | |
| *Packages* | Squid |
| *Logins* | — |
| *Contacts* | Neil Van Lysel &amp;lt;van-lyse@cs.wisc.edu&amp;gt;&lt;br&gt;Aaron Moate &amp;lt;moate@cs.wisc.edu&amp;gt;&lt;br&gt;Nate Yehle &amp;lt;nyehle@cs.wisc.edu&amp;gt; |

---+++ UNL Storage Element for !GridFTP / SRM Exercises

*Purpose:* An OSG SE that provides SRM and !GridFTP interfaces for the exercises.

| *Hostname* | red-srm1.unl.edu |
| *OS* | Scientific Linux 5.9 |
| *Packages* | hadoop-2.0.0+545-1.cdh4.1.1.p0.14.osg.el5, bestman2-server-2.3.0-9.osg.el5 |
| *Logins* | — |
| *Contacts* | Derek Weitzel &amp;lt;dweitzel@cse.unl.edu&amp;gt;, Carl Lundstedt &amp;lt;clundstedt@unl.edu&amp;gt;, Garhan Attebury &lt;gattebury2@unl.edu&gt; |

---++ Local Worker Nodes

Shortly before the school started, we got a couple of nodes in CHTC to be our worker nodes. They were part of the regular CHTC pool, but they only ran our jobs by policy. I think the submit node set an extra !ClassAd attribute (using SUBMIT_EXPRS?), and the START expression on those nodes only ran jobs with that expression set. Simple but effective enough for our purposes. Instructors were able to use the setup in advance of the school. The day the school started, we got a few more nodes. Each node have several CPUs, so we had 40 or 60 batch nodes, I forget exactly. These should have been set up earlier than the day of the school.

Those worker nodes were harder to setup than just the policy, because we had them mount the NFS volume from the SE. This was used for OSG_APP and OSG_DATA. We put the BLAST databases on all the worker nodes this way. This is the standard thing to do in an OSG site, but OSG hopes to one day move away from OSG_APP and OSG_DATA, so it may not be necessary one day for the school. I do not know if it will be needed next year or not, but I would bet we would benefit from it.

---++ General Information From 2012

The machines were managed with Puppet. Dan gave me a script to create accounts for the students and instructors. I *think* it made the users on one machine (perhaps the CE) and Puppet copied it everywhere in an hour. But I am not 100% certain how it worked.

The site is registered with the GOC, so if you change or update the site in interesting ways (new CE hostname or whatever) you need to tell them about it:

http://myosg.grid.iu.edu/search?q=WISC-OSG-EDU-2&amp;type=resource

What should we do next year? We could just lightly update the existing VMs and move on. But when we bought the new VM server machine for the VDT, the thought was that it could support both the OSG-EDU site and an ITB site. I think that&#39;s no problem. So an ideal world with lots of available effort, we&#39;d move to it and decommission the old VMs.

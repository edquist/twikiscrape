&lt;style type=&quot;text/css&quot;&gt;
pre em { font-style: normal; background-color: yellow; }
pre strong { font-style: normal; font-weight: bold; color: #008; }
&lt;/style&gt;

---+ Friday Exercise 1.2: Plan Joe&#39;s Workflow

This exercise outlines our goal (creating a production workflow) and the steps needed to achieve it.  Before starting this section, make sure to first read [[UserSchool16Fri11LearnJoesWork][Exercise 1.1]], which has important background information about Joe&#39;s intended work and how he has submitted jobs so far.

---++ Your mission

Your goal is to plan out Joe’s workflow based upon small-scale test jobs and, later, to write a full-scale DAG to actually run his workflow in production. In particular, you will need to do the following: 

   * *Optimize the submit files for the _permutation_ jobs of each trait,* such that each trait takes advantage of high-throughput parallelization with some number of job _processes_. Each process should calculate a portion of the new total of 100,000 permutations (versus the 10,000 permutations per trait that Joe has been working with previously).

   * *Optimize submit file values for &quot;request_memory&quot; and &quot;request_cpus&quot;* for each _permutation_ and _QTL mapping_ step, for each trait of the three traits.

   * *Create a single DAG file, with _permutation_ and _QTL mapping_ jobs for each of the three traits,* including PRE and/or POST scripts for the =tar= scripts that need to be run.

We&#39;ll be working on the first two optimization steps during this session, and the second optimization step and DAG creation after the break, in [[UserSchool16Fri13ExecFlow][Exercise 1.3]].

---+++*NOTE: In what follows, the only files you will need to modify are the submit files (and as specifically instructed). You will not need to modify any of the input files, output files, or scripts/programs. It is advisable that you split up some of the work within your pair or group in order to be time-efficient.

---++ Draw a diagram

Based upon what you [[https://www.opensciencegrid.org/bin/view/Education/UserSchool15Fri11LearnJoe&#39;sWork][learned from Joe]], *draw the _general workflow_ that you would make* for Joe (on paper), keeping in mind that there are 3 traits for which the _permutation_ and _QTL mapping_ steps need to be completed. (If you started drawing a diagram while reading the previous page, just extend it here).  The tar steps will probably need to be PRE or POST scripts (you decide which is best). Think about what Joe&#39;s intended workflow means for the shape of the DAG, including PARENT-CHILD dependencies for JOBs in the DAG and the fact that the _permutation_ step could be broken up into multiple processes of fewer total permutations, each.  We&#39;ll come back to this diagram in the [[UserSchool16Fri13ExecFlow][next exercise]] after the break, when it&#39;s time to construct the full DAG.  

---++ Optimize job components

Before we assemble the full DAG, we want to optimize each component of the workflow.  This will include 3 optimization steps: 

---+++ 1. Test the HTC optimization of the _permutation_ step.  

You will need to determine the number of permutations that could be batched in a single job process, if each process needs to run in ~30 minutes for good HTC scaling. To do this, you will need to run some test jobs, to see how long it takes a single job process to run, say, 10, 100, or 1000 permutations.  

To run these test jobs: 
   1. Modify one of the permutation submit files to “queue” 10 processes (so that you can average time between the 10 test processes)
   1. Add &quot;request_cpus = 1&quot; according to Joe&#39;s indication
   1. Add reasonable first guesses for &quot;request_memory&quot; and &quot;request_disk&quot; (say, 1 GB?). 
   1. Make a few copies of this submit file so that you can change the last argument (the number of permutations) from &quot;10000&quot; to &quot;10&quot;, &quot;100&quot;, or &quot;1000&quot;. For time&#39;s sake, a member of your group should test all three of these variations at the same time! 

Getting ready for the next step: 
   1. After each set of _permutation_ tests finishes, you’ll need to use =tarit.sh= (with the correct argument!) before running the test jobs for the QTL step.

---+++ 2. Test each of the _QTL mapping_ jobs 

Submit the three submit files after simply adding lines for &quot;request_memory&quot;, &quot;request_disk&quot;, and &quot;request_cpus&quot;. The resource needs (RAM and disk space) and execution time of each _QTL mapping_ job will likely increase with the total number of permutations from the previous _permutation_ step, though the execution time will likely still be short (according to Joe). You&#39;ll test optimized _permutation_ and _QTL mapping_ steps later on.

---+++ 3. Optimization planning

In order to optimize Joe&#39;s overall workflow, we can optimize the following values, based on our test jobs from steps 1 and 2: 
 
*Permutation throughput:* Calculate the number of permutations that should be run _per job process_, such that the runtimes per process will be about 30 minutes (not exact, but closer to 30 than to 5 or 60). You can then calculate the number of processes that should be queued such that 100,000 permutations are calculated for each trait.  Essentially, you want _job processes_ X _permutations_ to equal 100,000 total permutations for each of the three phenotype traits.
*Hint: You can use the &quot;condor_history&quot; feature (similar to condor_q, but for completed jobs) to easily view and compare the &quot;RUNTIME&quot; for jobs in a &quot;cluster&quot; (using the cluster value as an argument to =condor_history=.

*Memory and disk for both steps:* Make sure to examine the log files of your _permutation_ and _QTL_ test jobs, so that you can extrapolate how much memory and disk should be requested in the submit files for the full-scale DAG.

---+++*When you&#39;re done with #3, move on to [[UserSchool16Fri13ExecFlow][Exercise 2.2]]*

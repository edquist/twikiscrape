%LINKCSS%

---+!! Tutorial: An Introduction to the Grid
%TOC%

These exercises explore some simple Grid operations to start developing a few very basic techniques for distributed computing.

We assume you&#39;re using the default &quot;bash&quot; shell set up on your logins on the laptops and lab servers. We recommend you don&#39;t change your shell.

---++ Connecting to the Linux training hosts 


You will be doing almost all the lab exercises this week on a set of Linux computers (&quot;hosts&quot;) named =workshop3= and =workshop4= (its &quot;fully qualified host name&quot; looks like &quot;%LOGINHOST%&quot;).  From these machines, we will run Grid jobs and explore various Grid sites.

To access this machine from your computer, we will use &quot;secure shell&quot;.

On a Windows machine, we will use the &lt;nop&gt;PuTTY program. Open &lt;nop&gt;PuTTY and enter the hostname of the computer that you will use.

On a Mac, we will use the Terminal and ssh command-line tool. Open Terminal and type =ssh= followed by your training user name, an &quot;@&quot; symbol and the name of the computer you wish to connect to. For example:

&lt;pre class=&quot;screen&quot;&gt;
mac$ &lt;userinput&gt;ssh %LOGINNAME%@%LOGINHOST%&lt;/userinput&gt;
The authenticity of host &#39;%LOGINHOST% (%LOGINIP%)&#39; can&#39;t be established.
RSA key fingerprint is 36:74:78:a8:ed:6b:38:96:63:20:01:df:46:9b:59:3b.
Are you sure you want to continue connecting (yes/no)? &lt;userinput&gt;yes&lt;/userinput&gt;
Warning: Permanently added &#39;%LOGINHOST%,%LOGINIP%&#39; (RSA) to the list of known hosts.
%LOGINNAME%@%LOGINHOST%&#39;s password: &lt;userinput&gt;PASSWORD&lt;/userinput&gt;  &lt;em&gt;# not echoed !!!&lt;/em&gt;
%LOGINHOSTSHORT%$    &lt;em&gt;# Now you&#39;re talking to a shell on the workshop4 server&lt;/em&gt;
&lt;/pre&gt;

After the first time you do this, you won&#39;t get the &quot;Are you sure.&quot; prompt. Some of you will never see this, as your computers were used for testing this material, and the &quot;yes&quot; reply was already supplied by a tester.  So it will look like:

&lt;pre class=&quot;screen&quot;&gt;
mac$ &lt;userinput&gt;ssh %LOGINNAME%@%LOGINHOST%&lt;/userinput&gt;
Password: &lt;userinput&gt;PASSWORD&lt;/userinput&gt; 
%LOGINHOSTSHORT%$
&lt;/pre&gt;

You should be able to reach all four lab hosts - %LOGINHOSTSHORT% ... workshop4 - in this manner.

For the rest of this exercise, we will use simpler prompts: =$laptop= for commands typed directly to the shell of your laptop Terminal window, and =%LOGINHOSTSHORT$= for the %LOGINHOSTSHORT% lab server.


---++ Conventions

Note the conventions for command-line dialogs that will be used throughout these exercises:

What you type &lt;tt&gt;&lt;userinput&gt;looks like this&lt;/userinput&gt;&lt;/tt&gt;. The system&#39;s responses =look like this=.

Our comments to you (which you should *not* enter) look like:  =&lt;em&gt;# some comment here&lt;/em&gt;=

&lt;pre class=&quot;screen&quot;&gt;
laptop$ &lt;userinput&gt;pwd&lt;/userinput&gt;    &lt;em&gt;# You type &quot;pwd&quot; but NOT this comment!&lt;/em&gt;
/home/%LOGINNAME%   &lt;em&gt;# the system&#39;s response&lt;/em&gt;
laptop$          &lt;em&gt;# the system&#39;s command prompt&lt;/em&gt;
&lt;/pre&gt;

In these exercises, we&#39;ll use the prompt =laptop$= for responses from the shell on the lab laptops, and =workshop$= (or =%OTHERHOSTSHORT%$=, =workshop3$=, etc.) for responses from remote shells. 

%NOTE% The actual shell prompts you&#39;ll see may be different from those we show here.

To start, practice cutting text from your this page in your web browser to run a command or set of commands: cut the =pwd= command from the box above and paste it into your terminal window to execute it. This is a good way to avoid making typos while entering commands from the examples.


&lt;!-- ***  Comments plugin to create comments table for section   ***    --&gt;

&lt;span style=&quot;text-align:center; font-weight:bold; font-size:1.2em;&quot;&gt;ADD A COMMENT&lt;/span&gt;
%STARTMore%

%TABLE{ }%
|  *COMMENT*  |  *NAME*  |  *DATE*  |
%COMMENT{ type=&quot;tableappend&quot; }%

%ENDMore%
&lt;!-- ***  End Comment                                            ***    --&gt;


---++ &quot;Logging in&quot; to the Grid

%INCLUDE{ &quot;LoggingInToTheGrid&quot; }%

---++ Running jobs with Globus Commands

%INCLUDE{ &quot;RunningJobsWithGlobusCommandsWS&quot;}% 

---++ Running under a remote shell

%INCLUDE{ &quot;RunningUnderARemoteShellWS&quot; }% 

---++ Immediate (fork) and Batch (scheduled) Job managers

GRAM, the Globus protocol for running remote jobs, supports the concept of a &quot;job manager&quot; as an adapter to local job management environments.  Each &quot;Grid site&quot; - or collection of resources - can support one or more such job managers. The &quot;fork&quot; job manager runs an immediate job through the UNIX fork() interface. Another job manager we have installed on the lab hosts acts as an interface to the Condor batch scheduling system.

Which do you think will be faster?  Lets find out: use the command =time= to test which jobmanager is faster.

To time a command, just enter =time &lt;em&gt;commandname&lt;/em&gt;=:

&lt;pre class=&quot;screen&quot;&gt;
%LOGINHOSTSHORT%$ &lt;userinput&gt;time sleep 3&lt;/userinput&gt;
&lt;/pre&gt;

Use this to time a few trivial Grid jobs to compare =Fork= and =Condor=:

&lt;pre class=&quot;screen&quot;&gt;
%LOGINHOSTSHORT%$ globusrun-ws -submit -factory-type Condor -s -job-command /bin/hostname
&lt;/pre&gt;

The &quot;fork&quot; job manager is very fast - it has rather low &quot;scheduling latency&quot;.  It runs trivial commands very quickly.  But it also has no compute power - its usually just a single CPU on a cluster-controlling computer called the _head node_.  A batch job manager, on the other hand, has a higher scheduling overhead.  But it gives you access to all computers in a cluster, and the opportunity to do real parallel computing.

It turns out that each of our workshop machines is just an ordinary single-processing machine. We&#39;ve just &quot;tricked&quot; Condor onto thinking that it had a real &quot;cluster&quot; there with a set of CPUs. (We configured each of the hosts with 16 such virtual CPUS).

While our lab hosts use the Condor scheduler, other systems use other schedulers. For example, systems using _PBS_ (Portable Batch System) require that you specify -factory-type PBS.  You&#39;ll get a chance to try this later in this lab.

Now try running a few jobs through condor on a different machine:


&lt;pre class=&quot;screen&quot;&gt;
%LOGINHOSTSHORT%$ globusrun-ws -submit -F %OTHERHOST% -factory-type Condor -s -job-command /bin/hostname
&lt;/pre&gt;

Next try starting five such jobs on %OTHERHOSTSHORT% at once: put an &quot;&amp;amp;&quot; at the end of the line, and either use cut-and-past, or shell command history, or a simple shell script to run five of these commands at once.  


&lt;!-- ***  Comments plugin to create comments table for section   ***    --&gt;

&lt;span style=&quot;text-align:center; font-weight:bold; font-size:1.2em;&quot;&gt;ADD A COMMENT&lt;/span&gt;
%STARTMore%

%TABLE{ }%
|  *COMMENT*  |  *NAME*  |  *DATE*  |
%COMMENT{ type=&quot;tableappend&quot; }%

%ENDMore%
&lt;!-- ***  End Comment                                            ***    --&gt;




---++ Running a simple application remotely, through &quot;staging&quot;

%INCLUDE{ &quot;RunningSimpleApplicationsRemotelyThroughStagingWS&quot; }%

%STARTSECTION{&quot;TutorialVars&quot;}%
&lt;!--                                                                            
      * Set LOGINHOST = workshop4.ci.uchicago.edu
      * Set LOGINHOSTSHORT = workshop4
      * Set LOGINIP = 131.193.181.56
      * Set GRIDHOST = tg-login.ncsa.teragrid.org
      * Set OTHERHOST = workshop3.ci.uchicago.edu
      * Set OTHERHOSTSHORT = workshop3
      * Set CERTSUBJECT = /O=Grid/OU=OSG/CN=Training User 99
      * Set LOGINNAME = train99
      * Set HOMEDIR = /home/%LOGINNAME%
--&gt; 
%ENDSECTION{&quot;TutorialVars&quot;}%
%BOTTOMMATTER%

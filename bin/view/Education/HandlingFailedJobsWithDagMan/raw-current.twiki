
---+!! %SPACEOUT{ &quot;%TOPIC%&quot; }%

%TOC%

%STARTINCLUDE%
%EDITTHIS%

DAGMan can handle a situation where some of the nodes in a DAG fails. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed.


---+++ Create a script
Let&#39;s create a script that will fail so we can see this:

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;cat &amp;gt; myscript2.sh
#! /bin/sh

echo &quot;I&#39;m process id $$ on&quot; `hostname`
echo &quot;This is sent to standard error&quot; 1&amp;gt;&amp;2
date
echo &quot;Running as binary $0&quot; &quot;$@&quot;
echo &quot;My name (argument 1) is $1&quot;
echo &quot;My sleep duration (argument 2) is $2&quot;
sleep $2
echo &quot;Sleep of $2 seconds finished.  Exiting&quot;
echo &quot;RESULT: 1 FAILURE&quot;
exit 1

&lt;em&gt;[Ctrl+D]&lt;/em&gt;&lt;/userinput&gt;

$ &lt;userinput&gt;cat myscript2.sh&lt;/userinput&gt;
#! /bin/sh

echo &quot;I&#39;m process id $$ on&quot; `hostname`
echo &quot;This is sent to standard error&quot; 1&amp;gt;&amp;2
date
echo &quot;Running as binary $0&quot; &quot;$@&quot;
echo &quot;My name (argument 1) is $1&quot;
echo &quot;My sleep duration (argument 2) is $2&quot;
sleep $2
echo &quot;Sleep of $2 seconds finished.  Exiting&quot;
echo &quot;RESULT: 1 FAILURE&quot;
exit 1
$ &lt;userinput&gt;chmod a+x myscript2.sh&lt;/userinput&gt;
&lt;/pre&gt;


---+++ Modify the submit
Modify job.work2.submit to run myscript2.sh instead of myscript.sh:

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;rm job.work2.submit&lt;/userinput&gt;
$ &lt;userinput&gt;cat &amp;gt; job.work2.submit
executable=myscript2.sh
output=results.work2.output
error=results.work2.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 %LOGINHOST%/jobmanager-fork
arguments=WorkerNode2 60
queue

&lt;em&gt;&lt;strong&gt;[Ctrl+D]&lt;/strong&gt;&lt;/em&gt;
&lt;/userinput&gt;
$ &lt;userinput&gt;cat job.work2.submit&lt;/userinput&gt;
executable=myscript2.sh
output=results.work2.output
error=results.work2.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 %LOGINHOST%/jobmanager-fork
arguments=WorkerNode2 60
queue
&lt;/pre&gt;

---+++ Resubmit the DAG
Submit the dag again.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;condor_submit_dag mydag.dag&lt;/userinput&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.condor.sub
Log of DAGMan debugging messages         : mydag.dag.dagman.out
Log of Condor library debug messages     : mydag.dag.lib.out
Log of the life of condor_dagman itself  : mydag.dag.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 15.
-----------------------------------------------------------------------
&lt;/pre&gt;


---+++ Monitor progress
Use =watch_condor_q= to watch the jobs until they finish.

In separate windows run =tail -f --lines=500 results.log= and =tail -f --lines=500 mydag.dag.dagman.out= to monitor the job&#39;s progress.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;./watch_condor_q&lt;/userinput&gt;


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  15.0   %LOGINNAME%         7/10 11:11   0+00:00:04 R  0   2.6  condor_dagman -f -
  16.0   %LOGINNAME%         7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh       
  17.0   %LOGINNAME%         7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  16.0   %LOGINNAME%       UNSUBMITTED fork     %LOGINHOST%   %HOMEDIR%/condo
  17.0   %LOGINNAME%       UNSUBMITTED fork     %LOGINHOST%   %HOMEDIR%/condo


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  15.0   %LOGINNAME%         7/10 11:11   0+00:00:04 R  0   2.6  condor_dagman -f -
  16.0    |-HelloWorld   7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh       
  17.0    |-Setup        7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


&lt;em&gt;Output of watch_condor_q truncated&lt;/em&gt;

-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held
&lt;userinput&gt;&lt;em&gt;[Ctrl+C]&lt;/em&gt;&lt;/userinput&gt;
&lt;/pre&gt;

&lt;!-- ***  Comments plugin to create comments table for section   ***    --&gt;
&lt;span class=&quot;educationWebAddComment&quot;&gt;ADD A COMMENT&lt;/span&gt;
%STARTMore%

%TABLE{ }%
|  *COMMENT*  |  *NAME*  |  *DATE*  |
%COMMENT{ type=&quot;tableappend&quot; }%

%ENDMore%
&lt;!-- ***  End Comment                                    ***********    --&gt;




---+++ Check your results

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;ls&lt;/userinput&gt;
job.finalize.submit   mydag.dag.condor.sub  myscript.sh           results.output      results.work2.output
job.setup.submit      mydag.dag.dagman.log  myscript2.sh        results.setup.error   results.workfinal.error
job.work1.submit      mydag.dag.dagman.out  results.error        results.setup.output  results.workfinal.output
job.work2.submit      mydag.dag.lib.out     results.finalize.error   results.work1.error   watch_condor_q
job.workfinal.submit  mydag.dag.lock       results.finalize.output  results.work1.output
mydag.dag         myjob.submit       results.log           results.work2.error
$ &lt;userinput&gt;cat results.work2.output&lt;/userinput&gt;
I&#39;m process id 29921 on %LOGINHOST%
Thu Jul 10 11:12:42 CDT 2003
Running as binary %HOMEDIR%/.globus/.gass_cache/local/md5/87/459c159766cefb36f0d75023de0e35/md5/70/5d82b930ec61460d9c9ca65cbe5a8a/data WorkerNode2 60
My name (argument 1) is WorkerNode2
My sleep duration (argument 2) is 60
Sleep of 60 seconds finished.  Exiting
RESULT: 1 FAILURE
$ &lt;userinput&gt;cat mydag.dag.dagman.out&lt;/userinput&gt;
7/10 11:11:55 ******************************************************
7/10 11:11:55 ** condor_scheduniv_exec.15.0 (CONDOR_DAGMAN) STARTING UP
7/10 11:11:55 ** $CondorVersion: 6.8.4 Apr 22 2003 $
7/10 11:11:55 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 11:11:55 ** PID = 27126
7/10 11:11:55 ******************************************************
7/10 11:11:55 DaemonCore: Command Socket at &amp;lt;%LOGINHOST%:34769&amp;gt;
&lt;em&gt;%STARTMore%&lt;/em&gt;
7/10 11:11:55 argv[0] == &quot;condor_scheduniv_exec.15.0&quot;
7/10 11:11:55 argv[1] == &quot;-Debug&quot;
7/10 11:11:55 argv[2] == &quot;3&quot;
7/10 11:11:55 argv[3] == &quot;-Lockfile&quot;
7/10 11:11:55 argv[4] == &quot;mydag.dag.lock&quot;
7/10 11:11:55 argv[5] == &quot;-Condorlog&quot;
7/10 11:11:55 argv[6] == &quot;results.log&quot;
7/10 11:11:55 argv[7] == &quot;-Dag&quot;
7/10 11:11:55 argv[8] == &quot;mydag.dag&quot;
7/10 11:11:55 argv[9] == &quot;-Rescue&quot;
7/10 11:11:55 argv[10] == &quot;mydag.dag.rescue&quot;
7/10 11:11:55 Condor log will be written to results.log
7/10 11:11:55 DAG Lockfile will be written to mydag.dag.lock
7/10 11:11:55 DAG Input file is mydag.dag
7/10 11:11:55 Rescue DAG will be written to mydag.dag.rescue
7/10 11:11:55 Parsing mydag.dag ...
7/10 11:11:55 Dag contains 6 total jobs
7/10 11:11:55 Bootstrapping...
7/10 11:11:55 Number of pre-completed jobs: 0
7/10 11:11:55 Submitting Job HelloWorld ...
7/10 11:11:55    assigned Condor ID (16.0.0)
7/10 11:11:55 Submitting Job Setup ...
7/10 11:11:55    assigned Condor ID (17.0.0)
7/10 11:11:56 Event: ULOG_SUBMIT for Job HelloWorld (16.0.0)
7/10 11:11:56 Event: ULOG_SUBMIT for Job Setup (17.0.0)
7/10 11:11:56 0/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:12:16 Event: ULOG_GLOBUS_SUBMIT for Job HelloWorld (16.0.0)
7/10 11:12:16 Event: ULOG_EXECUTE for Job HelloWorld (16.0.0)
7/10 11:12:16 Event: ULOG_GLOBUS_SUBMIT for Job Setup (17.0.0)
7/10 11:12:16 Event: ULOG_EXECUTE for Job Setup (17.0.0)
7/10 11:12:21 Event: ULOG_JOB_TERMINATED for Job HelloWorld (16.0.0)
7/10 11:12:21 Job HelloWorld completed successfully.
7/10 11:12:21 1/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:12:31 Event: ULOG_JOB_TERMINATED for Job Setup (17.0.0)
7/10 11:12:31 Job Setup completed successfully.
7/10 11:12:31 Submitting Job WorkerNode_1 ...
7/10 11:12:32    assigned Condor ID (18.0.0)
7/10 11:12:32 Submitting Job WorkerNode_Two ...
7/10 11:12:32    assigned Condor ID (19.0.0)
7/10 11:12:32 Event: ULOG_SUBMIT for Job WorkerNode_1 (18.0.0)
7/10 11:12:32 Event: ULOG_SUBMIT for Job WorkerNode_Two (19.0.0)
7/10 11:12:32 2/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:12:47 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_Two (19.0.0)
7/10 11:12:47 Event: ULOG_EXECUTE for Job WorkerNode_Two (19.0.0)
7/10 11:12:47 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_1 (18.0.0)
7/10 11:12:47 Event: ULOG_EXECUTE for Job WorkerNode_1 (18.0.0)
7/10 11:13:07 Event: ULOG_JOB_TERMINATED for Job WorkerNode_1 (18.0.0)
7/10 11:13:07 Job WorkerNode_1 completed successfully.
7/10 11:13:07 3/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:13:57 Event: ULOG_JOB_TERMINATED for Job WorkerNode_Two (19.0.0)
7/10 11:13:57 Job WorkerNode_Two completed successfully.
7/10 11:13:57 Submitting Job CollectResults ...
7/10 11:13:57    assigned Condor ID (20.0.0)
7/10 11:13:57 Event: ULOG_SUBMIT for Job CollectResults (20.0.0)
7/10 11:13:57 4/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:14:12 Event: ULOG_GLOBUS_SUBMIT for Job CollectResults (20.0.0)
7/10 11:14:12 Event: ULOG_EXECUTE for Job CollectResults (20.0.0)
7/10 11:14:32 Event: ULOG_JOB_TERMINATED for Job CollectResults (20.0.0)
7/10 11:14:32 Job CollectResults completed successfully.
7/10 11:14:32 Submitting Job LastNode ...
7/10 11:14:32    assigned Condor ID (21.0.0)
7/10 11:14:32 Event: ULOG_SUBMIT for Job LastNode (21.0.0)
7/10 11:14:32 5/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:14:47 Event: ULOG_GLOBUS_SUBMIT for Job LastNode (21.0.0)
7/10 11:14:47 Event: ULOG_EXECUTE for Job LastNode (21.0.0)
7/10 11:15:02 Event: ULOG_JOB_TERMINATED for Job LastNode (21.0.0)
7/10 11:15:02 Job LastNode completed successfully.
7/10 11:15:02 6/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 11:15:02 All jobs Completed!
7/10 11:15:02 **** condor_scheduniv_exec.15.0 (condor_DAGMAN) EXITING WITH STATUS 0
%ENDMore%
&lt;/pre&gt;

Uh oh, DAGMan ran that remaining nodes based on bad data from node work2. Normally DAGMan checks the return code and considers non-zero a failure.  We did modify myscript2.sh to return non-zero.  That would normally work, but we&#39;re using Condor-G, not normal Condor.  Condor-G relies on Globus and Globus doesn&#39;t return error codes.


---+++ Adding a POST script
If you&#39;re interested in having DAGMan notice a failed job and stopping the DAG at that point, you&#39;ll need to use a POST script to detect the problem.  One solution is to wrap your executable in a script that will output the executable&#39;s return code to stdout and have the POST script scan the stdout for the status.  Of perhaps your executable&#39;s normal output contains enough information to make the decision.

In this case, our executable is emitting a well known message.  Let&#39;s add a POST script.


---++++ Cleaning up
First, clean up your results.  

   $ *%RED% WARNING %ENDCOLOR%*: Be careful about deleting the =mydag.dag.*= files. Do not delete the =mydag.dag= file. Note the ending =.*=!

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;rm mydag.dag.* results.*&lt;/userinput&gt;
&lt;/pre&gt;


---+++ Create the script to check output
Now create a script to check the output.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;cat &amp;gt; postscript_checker
#! /bin/sh
grep &#39;RESULT: 0 SUCCESS&#39; $1 &amp;gt; /dev/null 2&amp;gt;/dev/null

&lt;strong&gt;&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;&lt;/userinput&gt;

$ &lt;userinput&gt;cat postscript_checker&lt;/userinput&gt;
#! /bin/sh
grep &#39;RESULT: 0 SUCCESS&#39; $1 &amp;gt; /dev/null 2&amp;gt;/dev/null
$ &lt;userinput&gt;chmod a+x postscript_checker &lt;/userinput&gt;
&lt;/pre&gt;



---++++ Modify dag
Modify your mydag.dag to use the new script for the nodes. 

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;cat &amp;gt;&amp;gt;mydag.dag
Script POST Setup postscript_checker results.setup.output
Script POST WorkerNode_1 postscript_checker results.work1.output
Script POST WorkerNode_Two postscript_checker results.work2.output
Script POST CollectResults postscript_checker results.workfinal.output
Script POST LastNode postscript_checker results.finalize.output

&lt;strong&gt;&lt;em&gt;[Ctrl+D]&lt;/em&gt;&lt;/strong&gt;&lt;/userinput&gt;

$ &lt;userinput&gt;cat mydag.dag&lt;/userinput&gt;
Job HelloWorld myjob.submit
Job Setup job.setup.submit
Job WorkerNode_1 job.work1.submit
Job WorkerNode_Two job.work2.submit
Job CollectResults job.workfinal.submit
Job LastNode job.finalize.submit
PARENT Setup CHILD WorkerNode_1 WorkerNode_Two
PARENT WorkerNode_1 WorkerNode_Two CHILD CollectResults
PARENT CollectResults CHILD LastNode
Script POST Setup postscript_checker results.setup.output
Script POST WorkerNode_1 postscript_checker results.work1.output
Script POST WorkerNode_Two postscript_checker results.work2.output
Script POST CollectResults postscript_checker results.workfinal.output
Script POST LastNode postscript_checker results.finalize.output
$ &lt;userinput&gt;ls&lt;/userinput&gt;
job.finalize.submit  job.work1.submit  job.workfinal.submit  myjob.submit  myscript2.sh        watch_condor_q
job.setup.submit     job.work2.submit  mydag.dag        myscript.sh   postscript_checker
&lt;/pre&gt;


---++++ Resubmit the DAG
Submit the DAG again with the new POST scripts in place.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;condor_submit_dag mydag.dag&lt;/userinput&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.condor.sub
Log of DAGMan debugging messages         : mydag.dag.dagman.out
Log of Condor library debug messages     : mydag.dag.lib.out
Log of the life of condor_dagman itself  : mydag.dag.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 22.
-----------------------------------------------------------------------
&lt;/pre&gt;

---++++ Monitor the job
Watch the job with =watch_condor_q=. 

In separate windows run &lt;tt class=&quot;in&quot;&gt;tail -f --lines=500 results.log&lt;/tt&gt; and &lt;tt&gt;&lt;userinput&gt;tail -f --lines=500 mydag.dag.dagman.out&lt;/userinput&gt;&lt;/tt&gt; to monitor the job&#39;s progress.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;./watch_condor_q &lt;/userinput&gt;


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  22.0   %LOGINNAME%         7/10 11:25   0+00:00:03 R  0   2.6  condor_dagman -f -
  23.0   %LOGINNAME%         7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh       
  24.0   %LOGINNAME%         7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  23.0   %LOGINNAME%       UNSUBMITTED fork     %LOGINHOST%   %HOMEDIR%/condo
  24.0   %LOGINNAME%       UNSUBMITTED fork     %LOGINHOST%   %HOMEDIR%/condo


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  22.0   %LOGINNAME%         7/10 11:25   0+00:00:03 R  0   2.6  condor_dagman -f -
  23.0    |-HelloWorld   7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh       
  24.0    |-Setup        7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


&lt;em&gt;[Output of watch_condor_q truncated]&lt;/em&gt;

-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held
&lt;userinput&gt;&lt;em&gt;[Ctrl+C]&lt;/em&gt;&lt;/userinput&gt;
&lt;/pre&gt;




---++++ Check your results

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;ls&lt;/userinput&gt;
job.finalize.submit   mydag.dag          mydag.dag.rescue   results.error         results.work1.error
job.setup.submit      mydag.dag.condor.sub  myjob.submit   results.log         results.work1.output
job.work1.submit      mydag.dag.dagman.log  myscript.sh      results.output         results.work2.error
job.work2.submit      mydag.dag.dagman.out  myscript2.sh   results.setup.error   results.work2.output
job.workfinal.submit  mydag.dag.lib.out     postscript_checker   results.setup.output  watch_condor_q
$ &lt;userinput&gt;cat mydag.dag.dagman.out&lt;/userinput&gt;
7/10 11:25:35 ******************************************************
7/10 11:25:35 ** condor_scheduniv_exec.22.0 (CONDOR_DAGMAN) STARTING UP
7/10 11:25:35 ** $CondorVersion: 6.8.4 Apr 22 2003 $
7/10 11:25:35 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 11:25:35 ** PID = 27251
7/10 11:25:35 ******************************************************
%STARTMore%
7/10 11:25:35 DaemonCore: Command Socket at &amp;lt;128.105.185.14:34913&amp;gt;
7/10 11:25:35 argv[0] == &quot;condor_scheduniv_exec.22.0&quot;
7/10 11:25:35 argv[1] == &quot;-Debug&quot;
7/10 11:25:35 argv[2] == &quot;3&quot;
7/10 11:25:35 argv[3] == &quot;-Lockfile&quot;
7/10 11:25:35 argv[4] == &quot;mydag.dag.lock&quot;
7/10 11:25:35 argv[5] == &quot;-Condorlog&quot;
7/10 11:25:35 argv[6] == &quot;results.log&quot;
7/10 11:25:35 argv[7] == &quot;-Dag&quot;
7/10 11:25:35 argv[8] == &quot;mydag.dag&quot;
7/10 11:25:35 argv[9] == &quot;-Rescue&quot;
7/10 11:25:35 argv[10] == &quot;mydag.dag.rescue&quot;
7/10 11:25:35 Condor log will be written to results.log
%STARTMore%
7/10 11:25:35 DAG Lockfile will be written to mydag.dag.lock
7/10 11:25:35 DAG Input file is mydag.dag
7/10 11:25:35 Rescue DAG will be written to mydag.dag.rescue
7/10 11:25:35 Parsing mydag.dag ...
7/10 11:25:35 jobName: Setup
7/10 11:25:35 jobName: WorkerNode_1
7/10 11:25:35 jobName: WorkerNode_Two
7/10 11:25:35 jobName: CollectResults
7/10 11:25:35 jobName: LastNode
7/10 11:25:35 Dag contains 6 total jobs
7/10 11:25:35 Bootstrapping...
7/10 11:25:35 Number of pre-completed jobs: 0
7/10 11:25:35 Submitting Job HelloWorld ...
7/10 11:25:35    assigned Condor ID (23.0.0)
7/10 11:25:35 Submitting Job Setup ...
7/10 11:25:35    assigned Condor ID (24.0.0)
7/10 11:25:36 Event: ULOG_SUBMIT for Job HelloWorld (23.0.0)
7/10 11:25:36 Event: ULOG_SUBMIT for Job Setup (24.0.0)
7/10 11:25:36 0/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:25:56 Event: ULOG_GLOBUS_SUBMIT for Job HelloWorld (23.0.0)
7/10 11:25:56 Event: ULOG_EXECUTE for Job HelloWorld (23.0.0)
7/10 11:25:56 Event: ULOG_GLOBUS_SUBMIT for Job Setup (24.0.0)
7/10 11:25:56 Event: ULOG_EXECUTE for Job Setup (24.0.0)
7/10 11:26:01 Event: ULOG_JOB_TERMINATED for Job HelloWorld (23.0.0)
7/10 11:26:01 Job HelloWorld completed successfully.
7/10 11:26:01 1/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:26:11 Event: ULOG_JOB_TERMINATED for Job Setup (24.0.0)
7/10 11:26:11 Job Setup completed successfully.
7/10 11:26:11 Running POST script of Job Setup...
7/10 11:26:11 1/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:26:16 Event: ULOG_POST_SCRIPT_TERMINATED for Job Setup (24.0.0)
7/10 11:26:16 POST Script of Job Setup completed successfully.
7/10 11:26:16 Submitting Job WorkerNode_1 ...
7/10 11:26:16    assigned Condor ID (25.0.0)
7/10 11:26:16 Submitting Job WorkerNode_Two ...
7/10 11:26:17    assigned Condor ID (26.0.0)
7/10 11:26:17 Event: ULOG_SUBMIT for Job WorkerNode_1 (25.0.0)
7/10 11:26:17 Event: ULOG_SUBMIT for Job WorkerNode_Two (26.0.0)
7/10 11:26:17 2/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:26:32 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_1 (25.0.0)
7/10 11:26:32 Event: ULOG_EXECUTE for Job WorkerNode_1 (25.0.0)
7/10 11:26:32 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_Two (26.0.0)
7/10 11:26:32 Event: ULOG_EXECUTE for Job WorkerNode_Two (26.0.0)
7/10 11:26:52 Event: ULOG_JOB_TERMINATED for Job WorkerNode_1 (25.0.0)
7/10 11:26:52 Job WorkerNode_1 completed successfully.
7/10 11:26:52 Running POST script of Job WorkerNode_1...
7/10 11:26:52 2/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 1 post
7/10 11:26:57 Event: ULOG_POST_SCRIPT_TERMINATED for Job WorkerNode_1 (25.0.0)
7/10 11:26:57 POST Script of Job WorkerNode_1 completed successfully.
7/10 11:26:57 3/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:27:42 Event: ULOG_JOB_TERMINATED for Job WorkerNode_Two (26.0.0)
7/10 11:27:42 Job WorkerNode_Two completed successfully.
7/10 11:27:42 Running POST script of Job WorkerNode_Two...
7/10 11:27:42 3/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:27:47 Event: ULOG_POST_SCRIPT_TERMINATED for Job WorkerNode_Two (26.0.0)
7/10 11:27:47 POST Script of Job WorkerNode_Two failed with status 1
7/10 11:27:47 3/6 done, 1 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 11:27:47 ERROR: the following job(s) failed:
7/10 11:27:47 ---------------------- Job ----------------------
7/10 11:27:47       Node Name: WorkerNode_Two
7/10 11:27:47          NodeID: 3
7/10 11:27:47     Node Status: STATUS_ERROR    
7/10 11:27:47           Error: POST Script failed with status 1
7/10 11:27:47 Job Submit File: job.work2.submit
7/10 11:27:47     POST Script: postscript_checker results.work2.output
7/10 11:27:47   Condor Job ID: (26.0.0)
7/10 11:27:47       Q_PARENTS: 1, &amp;lt;END&amp;gt;
7/10 11:27:47       Q_WAITING: &amp;lt;END&amp;gt;

7/10 11:27:47      Q_CHILDREN: 4, &amp;lt;END&amp;gt;
7/10 11:27:47 ---------------------------------------   &amp;lt;END&amp;gt;
7/10 11:27:47 Writing Rescue DAG file...
7/10 11:27:47 **** condor_scheduniv_exec.22.0 (condor_DAGMAN) EXITING WITH STATUS 1
%ENDMore%
&lt;/pre&gt;

DAGMan notices that one of the jobs failed.  DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved.

&lt;!-- ***  Comments plugin to create comments table for section   ***    --&gt;
&lt;span class=&quot;educationWebAddComment&quot;&gt;ADD A COMMENT&lt;/span&gt;
%STARTMore%

%TABLE{ }%
|  *COMMENT*  |  *NAME*  |  *DATE*  |
%COMMENT{ type=&quot;tableappend&quot; }%

%ENDMore%
&lt;!-- ***  End Comment                                    ***********    --&gt;

---+++ Examine mydag.dag.rescue
Look at the rescue DAG. It&#39;s the same structurally as your original DAG, but notes that finished are marked DONE.  (DAGMan also reorganized the file.) When you submit the rescue DAG, DONE nodes will be skipped.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;cat mydag.dag.rescue &lt;/userinput&gt;
# Rescue DAG file, created after running
#   the mydag.dag DAG file
#
# Total number of Nodes: 6
# Nodes premarked DONE: 3
# Nodes that failed: 1
#   WorkerNode_Two,&amp;lt;ENDLIST&amp;gt;

JOB HelloWorld myjob.submit DONE

JOB Setup job.setup.submit DONE
SCRIPT POST Setup postscript_checker results.setup.output

JOB WorkerNode_1 job.work1.submit DONE
SCRIPT POST WorkerNode_1 postscript_checker results.work1.output

JOB WorkerNode_Two job.work2.submit 
SCRIPT POST WorkerNode_Two postscript_checker results.work2.output

JOB CollectResults job.workfinal.submit 
SCRIPT POST CollectResults postscript_checker results.workfinal.output

JOB LastNode job.finalize.submit 
SCRIPT POST LastNode postscript_checker results.finalize.output


PARENT Setup CHILD WorkerNode_1 WorkerNode_Two
PARENT WorkerNode_1 CHILD CollectResults
PARENT WorkerNode_Two CHILD CollectResults
PARENT CollectResults CHILD LastNode
&lt;/pre&gt;

We know there is a problem with the work2 step.  Let&#39;s &quot;fix&quot; it.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;rm myscript2.sh&lt;/userinput&gt;
$ &lt;userinput&gt;cp myscript.sh myscript2.sh&lt;/userinput&gt;
&lt;/pre&gt;

---++++ Resubmitting rescue DAG
Now we can submit our rescue DAG. 

   $ *%RED% NOTE %ENDCOLOR%*: If you didn&#39;t fix the problem, DAGMan would have generated another rescue DAG (=mydag.dag.rescue.rescue=).

In separate windows run &lt;tt class=&quot;in&quot;&gt;tail -f --lines=500 results.log&lt;/tt&gt; and &lt;tt class=&quot;in&quot;&gt;tail -f --lines=500 mydag.dag.dagman.out&lt;/tt&gt; to monitor the job&#39;s progress.


&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;condor_submit_dag mydag.dag.rescue &lt;/userinput&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.rescue.condor.sub
Log of DAGMan debugging messages         : mydag.dag.rescue.dagman.out
Log of Condor library debug messages     : mydag.dag.rescue.lib.out
Log of the life of condor_dagman itself  : mydag.dag.rescue.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 27.
-----------------------------------------------------------------------
$ &lt;userinput&gt;./watch_condor_q &lt;/userinput&gt;

-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  27.0   %LOGINNAME%         7/10 11:34   0+00:00:01 R  0   2.6  condor_dagman -f -
  28.0   %LOGINNAME%         7/10 11:34   0+00:00:00 I  0   0.0  myscript2.sh Worke

2 jobs; 1 idle, 1 running, 0 held

%STARTMore%
-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  28.0   %LOGINNAME%       UNSUBMITTED fork     %LOGINHOST%   %HOMEDIR%/condo


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  27.0   %LOGINNAME%         7/10 11:34   0+00:00:01 R  0   2.6  condor_dagman -f -
  28.0    |-WorkerNode_  7/10 11:34   0+00:00:00 I  0   0.0  myscript2.sh Worke

2 jobs; 1 idle, 1 running, 0 held

&lt;strong&gt;&lt;em&gt;[Output of watch_condor_q truncated]&lt;/em&gt;&lt;/strong&gt;

-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


-- Submitter: %LOGINHOST% : &amp;lt;%LOGINIP%:33785&amp;gt; : %LOGINHOST%
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held

&lt;userinput&gt;&lt;em&gt;[Ctrl+C]&lt;/em&gt;&lt;/userinput&gt;
%ENDMore%
&lt;/pre&gt;



---+++ Check your results.

&lt;pre class=&quot;screen&quot;&gt;
$ &lt;userinput&gt;ls&lt;/userinput&gt;
job.finalize.submit   mydag.dag.lib.out         myscript2.sh          results.work1.error
job.setup.submit      mydag.dag.rescue         postscript_checker       results.work1.output
job.work1.submit      mydag.dag.rescue.condor.sub  results.error       results.work2.error
job.work2.submit      mydag.dag.rescue.dagman.log  results.finalize.error   results.work2.output
job.workfinal.submit  mydag.dag.rescue.dagman.out  results.finalize.output  results.workfinal.error
mydag.dag         mydag.dag.rescue.lib.out      results.log          results.workfinal.output
mydag.dag.condor.sub  mydag.dag.rescue.lock      results.output       watch_condor_q
mydag.dag.dagman.log  myjob.submit         results.setup.error
mydag.dag.dagman.out  myscript.sh         results.setup.output
$ &lt;userinput&gt;cat mydag.dag.rescue.dagman.out&lt;/userinput&gt;
7/10 11:34:33 ******************************************************
7/10 11:34:33 ** condor_scheduniv_exec.27.0 (CONDOR_DAGMAN) STARTING UP
7/10 11:34:33 ** $CondorVersion: 6.8.4 Apr 22 2003 $
7/10 11:34:33 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 11:34:33 ** PID = 27317
7/10 11:34:33 ******************************************************
%STARTMore%
7/10 11:34:33 DaemonCore: Command Socket at &amp;lt;%LOGINIP%:35032&amp;gt;
7/10 11:34:33 argv[0] == &quot;condor_scheduniv_exec.27.0&quot;
7/10 11:34:33 argv[1] == &quot;-Debug&quot;
7/10 11:34:33 argv[2] == &quot;3&quot;
7/10 11:34:33 argv[3] == &quot;-Lockfile&quot;
7/10 11:34:33 argv[4] == &quot;mydag.dag.rescue.lock&quot;
7/10 11:34:33 argv[5] == &quot;-Condorlog&quot;
7/10 11:34:33 argv[6] == &quot;results.log&quot;
7/10 11:34:33 argv[7] == &quot;-Dag&quot;
7/10 11:34:33 argv[8] == &quot;mydag.dag.rescue&quot;
7/10 11:34:33 argv[9] == &quot;-Rescue&quot;
7/10 11:34:33 argv[10] == &quot;mydag.dag.rescue.rescue&quot;
7/10 11:34:33 Condor log will be written to results.log
7/10 11:34:33 DAG Lockfile will be written to mydag.dag.rescue.lock
7/10 11:34:33 DAG Input file is mydag.dag.rescue
7/10 11:34:33 Rescue DAG will be written to mydag.dag.rescue.rescue
7/10 11:34:33 Parsing mydag.dag.rescue ...
7/10 11:34:33 jobName: Setup
7/10 11:34:33 jobName: WorkerNode_1
7/10 11:34:33 jobName: WorkerNode_Two
7/10 11:34:33 jobName: CollectResults
7/10 11:34:33 jobName: LastNode
7/10 11:34:33 Dag contains 6 total jobs
7/10 11:34:33 Deleting older version of results.log
7/10 11:34:33 Bootstrapping...
7/10 11:34:33 Number of pre-completed jobs: 3
7/10 11:34:33 Submitting Job WorkerNode_Two ...
7/10 11:34:33    assigned Condor ID (28.0.0)
7/10 11:34:34 Event: ULOG_SUBMIT for Job WorkerNode_Two (28.0.0)
7/10 11:34:34 3/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:34:54 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_Two (28.0.0)
7/10 11:34:54 Event: ULOG_EXECUTE for Job WorkerNode_Two (28.0.0)
7/10 11:35:59 Event: ULOG_JOB_TERMINATED for Job WorkerNode_Two (28.0.0)
7/10 11:35:59 Job WorkerNode_Two completed successfully.
7/10 11:35:59 Running POST script of Job WorkerNode_Two...
7/10 11:35:59 3/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:36:04 Event: ULOG_POST_SCRIPT_TERMINATED for Job WorkerNode_Two (28.0.0)
7/10 11:36:04 POST Script of Job WorkerNode_Two completed successfully.
7/10 11:36:04 Submitting Job CollectResults ...
7/10 11:36:04    assigned Condor ID (29.0.0)
7/10 11:36:04 Event: ULOG_SUBMIT for Job CollectResults (29.0.0)
7/10 11:36:04 4/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:36:19 Event: ULOG_GLOBUS_SUBMIT for Job CollectResults (29.0.0)
7/10 11:36:19 Event: ULOG_EXECUTE for Job CollectResults (29.0.0)
7/10 11:36:34 Event: ULOG_JOB_TERMINATED for Job CollectResults (29.0.0)
7/10 11:36:34 Job CollectResults completed successfully.
7/10 11:36:34 Running POST script of Job CollectResults...
7/10 11:36:34 4/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:36:39 Event: ULOG_POST_SCRIPT_TERMINATED for Job CollectResults (29.0.0)
7/10 11:36:39 POST Script of Job CollectResults completed successfully.
7/10 11:36:39 Submitting Job LastNode ...
7/10 11:36:39    assigned Condor ID (30.0.0)
7/10 11:36:39 Event: ULOG_SUBMIT for Job LastNode (30.0.0)
7/10 11:36:39 5/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:36:54 Event: ULOG_GLOBUS_SUBMIT for Job LastNode (30.0.0)
7/10 11:36:54 Event: ULOG_EXECUTE for Job LastNode (30.0.0)
7/10 11:37:09 Event: ULOG_JOB_TERMINATED for Job LastNode (30.0.0)
7/10 11:37:09 Job LastNode completed successfully.
7/10 11:37:09 Running POST script of Job LastNode...
7/10 11:37:09 5/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:37:14 Event: ULOG_POST_SCRIPT_TERMINATED for Job LastNode (30.0.0)
7/10 11:37:14 POST Script of Job LastNode completed successfully.
7/10 11:37:14 6/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 11:37:14 All jobs Completed!
7/10 11:37:14 **** condor_scheduniv_exec.27.0 (condor_DAGMAN) EXITING WITH STATUS 0
$ &lt;userinput&gt;cat results.work2.output&lt;/userinput&gt;
I&#39;m process id 30478 on %LOGINHOST%
Thu Jul 10 11:34:46 CDT 2003
Running as binary %HOMEDIR%/.globus/.gass_cache/local/md5/23/61b50cd9b278330cac68107dd390d6/md5/5e/004f7216b8b846d548357da00985f4/data WorkerNode2 60
My name (argument 1) is WorkerNode2
My sleep duration (argument 2) is 60
Sleep of 60 seconds finished.  Exiting
RESULT: 0 SUCCESS
$ &lt;userinput&gt;exit&lt;/userinput&gt;
%ENDMore%
&lt;/pre&gt;

&lt;!-- ***  Comments plugin to create comments table for section   ***    --&gt;
&lt;span class=&quot;educationWebAddComment&quot;&gt;ADD A COMMENT&lt;/span&gt;
%STARTMore%

%TABLE{ }%
|  *COMMENT*  |  *NAME*  |  *DATE*  |
%COMMENT{ type=&quot;tableappend&quot; }%

%ENDMore%
&lt;!-- ***  End Comment                                    ***********    --&gt;





%STOPINCLUDE%

&lt;!--                                                                            
      * Set LOGINHOST = workshop1.lac.uic.edu
      * Set LOGINIP = 131.193.181.56
      * Set GRIDHOST = tg-login.sdsc.teragrid.org
      * Set OTHERHOST = workshop2.lac.uic.edu
      * Set CERTSUBJECT = /O=Grid/OU=OSG/CN=Training User 99
      * Set LOGINNAME = train99
      * Set HOMEDIR = /home/%LOGINNAME%

--&gt;
%BOTTOMMATTER%
-- Main.ForrestChristian - 25 Jan 2007 (edited original)  %BR%
-- Main.ForrestChristian - 24 Mar 2007 - Added VARIABLES  %BR%

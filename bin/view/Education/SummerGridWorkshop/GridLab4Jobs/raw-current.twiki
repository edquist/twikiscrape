&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Exercises for Lecture 5: Grid Resources and Job Management&lt;/title&gt;
&lt;style type=&quot;text/css&quot;&gt;&lt;!--
body {
   background-color: white;
   color: black;
}
pre {
   margin-left: 1em;
   padding: 0.5em;
   background-color: #f0f0f0;
   border: 1px solid black;
}
h1 {
   text-align: center;
}
h2 {
   padding-top: 0.5em;
   margin-top: 4em;
   border-top: 2px solid black;
}
h3 {
   padding-top: 0.5em;
   margin-top: 4em;
   border-top: 1px solid #AAAAAA;
}
--&gt;&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;


&lt;h1&gt;Exercises for Lecture 5: Grid Resources and Job Management&lt;/h1&gt;

&lt;p&gt;Earlier today we learned about Condor-G and DAGMan.  Now we will
submit some simple jobs using these components.&lt;/p&gt;

&lt;p&gt;In the exercises below, students should select a gridlab host 2, 3, or 4, at random, to use
throughout the exercise, rather than all choosing gridlab2. Use the host you select -
gridlab2, gridlab3 or gridlab4  - whenever the notes say &quot;gridlab2&quot;.&lt;/p&gt;

&lt;h2&gt;Table of Contents&lt;/h2&gt;

&lt;ul&gt;

&lt;li&gt;&lt;a href=&quot;#setup&quot;&gt;Part I: Getting setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#simple_job&quot;&gt;Part II: Submitting a Simple Grid Job with Condor-G&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#held_job&quot;&gt;Part III: Held Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#simple_dag&quot;&gt;Part IV: A Simple DAG with Condor-G and
DAGMan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#complex_dag&quot;&gt;Part V: A More Complex DAG&lt;/a&gt;&lt;/li&gt;

   &lt;ul&gt;
   &lt;li&gt;&lt;a href=&quot;#optional_multi_resource&quot;&gt;Optional: Multiple Globus Resources&lt;/a&gt;

   &lt;/ul&gt;

&lt;li&gt;&lt;a href=&quot;#failed_dag&quot;&gt;Part VI: Handling Jobs That Fail with
DAGMan&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

&lt;a name=&quot;setup&quot;&gt;&lt;/a&gt;
&lt;h2&gt;Part I: Getting setup&lt;/h2&gt;

&lt;p&gt;First, log into gridlab1. This will be our client machine for these exercises.

&lt;pre&gt;$ &lt;b&gt;ssh gridlab1.phys.utb.edu&lt;/b&gt;&lt;/pre&gt;

&lt;p&gt;Condor should already be set up and running on gridlab1. You can check this
by running
&lt;a href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_q.html&quot;&gt;
condor_q&lt;/a&gt;:

&lt;pre&gt;$ &lt;b&gt;condor_q&lt;/b&gt;

-- Submitter: gk1.phys.utb.edu : &lt;206.76.233.104:36236&gt; : gk1.phys.utb.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held&lt;/pre&gt;

&lt;p&gt;Everyone will be using the same Condor installation for these exercises.
You will see other students&#39; jobs in the queue alongside your own.

&lt;p&gt;Next we are going to create some directories for you to work
in. We&#39;ll make them in your home directory.

&lt;pre&gt;$ &lt;b&gt;cd ~&lt;/b&gt;

$ &lt;b&gt;mkdir condor-g-dagman-tutorial&lt;/b&gt;
$ &lt;b&gt;cd condor-g-dagman-tutorial&lt;/b&gt;
$ &lt;b&gt;mkdir submit&lt;/b&gt;&lt;/pre&gt;

&lt;p&gt;Create a short lived proxy for this tutorial.  (The default proxy
length is 12 hours.  For a long lived task, you might create proxies
with lifespans of 24 hours, several days, or even several months.)
(The &quot;-verify&quot; option is not required, but can is useful for
debugging.  -verify will warn you if an expected Certificate Authority
Certificate is missing.)

&lt;pre&gt;
$ &lt;strong&gt;grid-proxy-info -all&lt;/strong&gt;


ERROR: Couldn&#39;t find a valid proxy.
Use -debug for further information.

$ &lt;strong&gt;grid-proxy-init -hours 4 -verify&lt;/strong&gt;
Your identity: /C=US/O=Globus/O=University of Wisconsin/OU=Computer Sciences Department/CN=Alan De Smet
Enter GRID pass phrase for this identity: &lt;strong&gt;&lt;em&gt;Your pass phrase&lt;/em&gt;&lt;/strong&gt;
Creating proxy ........................................... Done
Proxy Verify OK
Your proxy is valid until Thu Jul 10 16:06:13 2003
$ &lt;strong&gt;grid-proxy-info -all&lt;/strong&gt;
subject  : /C=US/O=Globus/O=University of Wisconsin/OU=Computer Sciences Department/CN=Alan De Smet/CN=proxy
issuer   : /C=US/O=Globus/O=University of Wisconsin/OU=Computer Sciences Department/CN=Alan De Smet
type     : full
strength : 512 bits
timeleft : 3:59:57
&lt;/pre&gt;


&lt;p&gt;Do a quick test with globus-job-run to ensure that everything is
working:

&lt;pre&gt;
$ &lt;strong&gt;globus-job-run gridlab2 /bin/date&lt;/strong&gt;

Wed Jun 2 17:57:49 CDT 2004
&lt;/pre&gt;


&lt;p&gt;Our basic Globus setup appears to be functioning.


&lt;a name=&quot;simple_job&quot;&gt;&lt;/a&gt;
&lt;h2&gt;Part II: Submitting a Simple Grid Job with Condor-G&lt;/h2&gt;

&lt;p&gt; Now we are ready to submit our first job with Condor-G.  The basic
procedure is to create a Condor job submit description file.  This
file can tell Condor what executable to run, what resources to
use, how to handle failures, where to store the job&#39;s output,
and many other characteristics of the job
submission.  Then this file is given to condor_submit.

&lt;p&gt;First, move to our scratch submission location:

&lt;pre&gt;
$ &lt;strong&gt;cd ~/condor-g-dagman-tutorial/submit&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Create a Condor submit file.  
As you can see from the
&lt;a href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_submit.html&quot;&gt;condor_submit manual page&lt;/a&gt;,
there are many options that can be specified in a Condor-G submit
description file.  We will start out with just a few.
We&#39;ll be sending the job to the computer &quot;gridlab2&quot; and
running under the &quot;jobmanager-fork&quot; job manager.  We&#39;re setting
notification to never to avoid getting email messages about the
completion of our job, and redirecting the stdout/err of the job
back to the submission computer.

&lt;p&gt;(Feel free to use your favorite editor, but we will demonstrate with &#39;cat&#39;
in the example below.  When using cat to create files, press Ctrl-D to close
the file -- don&#39;t actually type &quot;Ctrl-D&quot; into the file.  Whenever you create a file using cat, we suggest you use cat to display the file and confirm that it contains the expected text.)

&lt;p&gt;Create the submit file, then verify that it was entered correctly:

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt; myjob.submit
executable=myscript.sh
arguments=TestJob 10
output=results.output
error=results.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 gridlab2/jobmanager-fork
queue
&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat myjob.submit&lt;/strong&gt;

executable=myscript.sh
arguments=TestJob 10
output=results.output
error=results.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 gridlab2/jobmanager-fork
queue
&lt;/pre&gt;

&lt;p&gt;Create a little program to run on the grid.

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt; myscript.sh
#! /bin/sh

echo &quot;I&#39;m process id $$ on&quot; `hostname`
echo &quot;This is sent to standard error&quot; 1&amp;gt;&amp;2
date
echo &quot;Running as binary $0&quot; &quot;$@&quot;
echo &quot;My name (argument 1) is $1&quot;
echo &quot;My sleep duration (argument 2) is $2&quot;
sleep $2
echo &quot;Sleep of $2 seconds finished.  Exiting&quot;
echo &quot;RESULT: 0 SUCCESS&quot;
&lt;em&gt;Ctrl-D&lt;/em&gt; &lt;/strong&gt;
$ &lt;strong&gt;cat myscript.sh&lt;/strong&gt;
#! /bin/sh

echo &quot;I&#39;m process id $$ on&quot; `hostname`
echo &quot;This is sent to standard error&quot; 1&amp;gt;&amp;2
date
echo &quot;Running as binary $0&quot; &quot;$@&quot;
echo &quot;My name (argument 1) is $1&quot;
echo &quot;My sleep duration (argument 2) is $2&quot;
sleep $2
echo &quot;Sleep of $2 seconds finished.  Exiting&quot;
echo &quot;RESULT: 0 SUCCESS&quot;

&lt;/pre&gt;

&lt;p&gt;Make the program executable and test it.

&lt;pre&gt;
$ &lt;strong&gt;chmod a+x myscript.sh&lt;/strong&gt;
$ &lt;strong&gt;./myscript.sh TEST 1&lt;/strong&gt;
I&#39;m process id 3428 on puffin.cs.wisc.edu
This is sent to standard error
Thu Jul 10 12:21:11 CDT 2003
Running as binary ./myscript.sh TEST 1
My name (argument 1) is TEST
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting
RESULT: 0 SUCCESS
&lt;/pre&gt;

&lt;p&gt;Submit your test job to Condor-G.

&lt;pre&gt;
$ &lt;strong&gt;condor_submit myjob.submit&lt;/strong&gt;

Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 1.
&lt;/pre&gt;

&lt;p&gt;Occasionally run
 &lt;a href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_q.html&quot;&gt;condor_q&lt;/a&gt;
 to 
watch the progress of your job.  You may also want to occasionally run
&quot;condor_q&amp;nbsp;-globus&quot; which presents Globus specific status
information.
(&lt;a href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_q.html&quot;&gt;Additional documentation on condor_q&lt;/a&gt;)

&lt;pre&gt;
$ &lt;strong&gt;condor_q&lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   1.0   adesmet         7/10 17:28   0+00:00:00 I  0   0.0  myscript.sh TestJo

1 jobs; 1 idle, 0 running, 0 held
$ &lt;strong&gt;condor_q -globus&lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   1.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
$ &lt;strong&gt;condor_q&lt;/strong&gt;

-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   1.0   adesmet         7/10 17:28   0+00:00:27 R  0   0.0  myscript.sh TestJo

1 jobs; 0 idle, 1 running, 0 held
$ &lt;strong&gt;condor_q -globus&lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   1.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
$ &lt;strong&gt;condor_q&lt;/strong&gt;

-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   1.0   adesmet         7/10 17:28   0+00:00:40 C  0   0.0  myscript.sh       

0 jobs; 0 idle, 0 running, 0 held
$ &lt;strong&gt;condor_q -globus&lt;/strong&gt;

-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   1.0   adesmet       DONE fork     gk2   /afs/cs.wisc.edu/u

$ &lt;strong&gt;condor_q&lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


&lt;/pre&gt;

&lt;p&gt;In another window you can run &quot;&lt;tt&gt;tail -f&lt;/tt&gt;&quot; to watch the log file for your job
to monitor its progress.
  For the remainder of this tutorial, we suggest you re-run this command when you submit one or more jobs.  This will allow you to see monitor how typical Condor-G jobs progress.  Use &quot;&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt;&quot; to stop watching the file.

&lt;p&gt;In a second window: 

&lt;pre&gt;
$ &lt;strong&gt;cd ~/condor-g-dagman-tutorial/submit&lt;/strong&gt;
$ &lt;strong&gt;tail -f --lines=500 results.log&lt;/strong&gt;
000 (001.000.000) 07/10 17:28:48 Job submitted from host: &lt;128.105.185.14:35688&gt;
...
017 (001.000.000) 07/10 17:29:01 Job submitted to Globus
    RM-Contact: gridlab2:/jobmanager-fork
    JM-Contact: https://gk2:2321/696/1057876132/
    Can-Restart-JM: 1
...
001 (001.000.000) 07/10 17:29:01 Job executing on host: gk2
...
005 (001.000.000) 07/10 17:30:08 Job terminated.
        (1) Normal termination (return value 0)
                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
        0  -  Run Bytes Sent By Job
        0  -  Run Bytes Received By Job
        0  -  Total Bytes Sent By Job
        0  -  Total Bytes Received By Job
...

&lt;/pre&gt;


&lt;p&gt;When the job is no longer listed in condor_q or when the log file reports
&quot;Job terminated,&quot; you can see the results in 
&lt;a href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_history.html&quot;&gt;condor_history&lt;/a&gt;.

&lt;pre&gt;
$ &lt;strong&gt;condor_history&lt;/strong&gt;
 ID      OWNER            SUBMITTED     RUN_TIME ST   COMPLETED CMD
   1.0   adesmet         7/10 10:28   0+00:00:00 C   ???        /afs/cs.wisc.ed
&lt;/pre&gt;


&lt;p&gt;When the job completes, verify that the output is as expected.  (The
binary name is different from what you created because of how Globus and
Condor-G cooperate to stage your file to execute computer.)

&lt;pre&gt;

$ &lt;strong&gt;ls&lt;/strong&gt;
myjob.submit  myscript.sh*  results.error  results.log   results.output
$ &lt;strong&gt;cat results.error&lt;/strong&gt;
This is sent to standard error
$ &lt;strong&gt;cat results.output &lt;/strong&gt;
$I&#39;m process id 733 on gk2
Thu Jul 10 17:28:57 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/28/fcae5001dbcd99cc476984b4151284/md5/af/355c4959dc83a74b18b7c03eb27201/data TestJob 10
My name (argument 1) is TestJob
My sleep duration (argument 2) is 10
Sleep of 10 seconds finished.  Exiting
RESULT: 0 SUCCESS
&lt;/pre&gt;

&lt;p&gt;If you didn&#39;t watch the results.log file with tail -f above, you will want to examine the information logged now:

&lt;pre&gt;$ &lt;strong&gt;cat results.log &lt;/strong&gt;&lt;/pre&gt;

&lt;p&gt;&lt;tt&gt;gridlab2&lt;/tt&gt; is also running a little Condor pool. Try submitting a job to
it through Condor-G and Globus. First, create a new submit file:

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt; myjob2.submit
executable=myscript.sh
arguments=TestJob 10
output=results2.output
error=results2.error
log=results2.log
notification=never
universe=grid
grid_resource=gt2 gridlab2/jobmanager-condor
queue
&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat myjob2.submit&lt;/strong&gt;
executable=myscript.sh
arguments=TestJob 10
output=results2.output
error=results2.error
log=results2.log
notification=never
universe=grid
grid_resource=gt2 gridlab2/jobmanager-condor
queue
&lt;/pre&gt;

&lt;p&gt;Notice the changed &lt;tt&gt;grid_resource&lt;/tt&gt; line. It now refers to 

&lt;tt&gt;condor&lt;/tt&gt; instead of &lt;tt&gt;fork&lt;/tt&gt;. This means Globus will submit the
job to Condor on &lt;tt&gt;gridlab2&lt;/tt&gt; instead of running the job directly.

&lt;p&gt;Now submit the job to Condor-G:

&lt;pre&gt;$ &lt;b&gt;condor_submit myjob2.submit&lt;/b&gt;
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 2.&lt;/pre&gt;

&lt;p&gt;You can monitor the job&#39;s progress just like the first job.

&lt;p&gt;If you log into
gridlab2 in another window, you can see your job in the Condor queue there (be
quick, or the job will finish before you look):


&lt;pre&gt;$ &lt;b&gt;ssh gridlab2&lt;/b&gt;
adesmet@gridlab2&#39;s password: 
$ &lt;b&gt;condor_status&lt;/b&gt;

Name          OpSys       Arch   State      Activity   LoadAv Mem   ActvtyTime

vm100@clu1.ph LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:35
vm10@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:33
vm11@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:34
vm12@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:35
...
vm99@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:34
vm9@clu1.phys LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:32

                     Machines Owner Claimed Unclaimed Matched Preempting

         INTEL/LINUX      100     0       0       100       0          0

               Total      100     0       0       100       0          0
$ &lt;b&gt;condor_q&lt;/b&gt;

-- Submitter: gk2.phys.utb.edu : &lt;206.76.233.105:36311&gt; : gk2.phys.utb.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  11.0   adesmet         7/10 23:04   0+00:00:00 I  0   0.0  data              

1 jobs; 1 idle, 0 running, 0 held&lt;/pre&gt;


&lt;p&gt;Clean up the results after the second job is complete:


&lt;pre&gt;
$ &lt;strong&gt;rm results.* results2.*&lt;/strong&gt;
&lt;/pre&gt;

&lt;a name=&quot;held_job&quot;&gt;&lt;/a&gt;
&lt;h2&gt;Part III: Held Jobs&lt;/h2&gt;

&lt;p&gt;When an problem occurs in the middleware, Condor-G will place your job
on &quot;Hold&quot;.  Held jobs remain in the queue, but wait for user intervention.
When you resolve the problem, you can use 
&lt;a
href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_release.html&quot;&gt;condor_release&lt;/a&gt;
to free job to continue.

&lt;p&gt;You can places jobs on hold yourself, perhaps if you want to delay your
run using &lt;a href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_hold.html&quot;&gt;condor_hold&lt;/a&gt;

&lt;p&gt;For this example, we&#39;ll make the output file non-writable.  The job will
be unable to copy the results back and will be placed on hold.

&lt;p&gt;Submit the job again, but this time immediately after submitting it,
mark the output file as read-only:

&lt;pre&gt;
$ &lt;strong&gt;condor_submit myjob.submit ; chmod a-w results.output&lt;/strong&gt;
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 3.
&lt;/pre&gt;

&lt;p&gt;Watch the job with tail.  When the job goes on hold, use 
&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt; to exit tail.  Note that condor_q reports
that the job is in the &quot;H&quot; or Held state.

&lt;pre&gt;
$ &lt;strong&gt;tail -f --lines=500 results.log&lt;/strong&gt;

000 (003.000.000) 07/12 22:35:44 Job submitted from host: &lt;192.167.1.23:32864&gt;
...
017 (003.000.000) 07/12 22:35:57 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:33178/12497/1058042148/
    Can-Restart-JM: 1
...
001 (003.000.000) 07/12 22:35:57 Job executing on host: gk2
...
012 (003.000.000) 07/12 22:36:52 Job was held.
        Globus error 129: the standard output/error size is different
...
&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;condor_q&lt;/strong&gt;

 
-- Submitter: pc-23.gs.unina.it : &lt;192.167.1.23:32864&gt; : pc-23.gs.unina.it
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD
   2.0   adesmet         7/12 22:35   0+00:00:55 H  0   0.0  myscript.sh TestJo
 
1 jobs; 0 idle, 0 running, 1 held
&lt;/pre&gt;


&lt;p&gt;Fix the problem (make the file writable again), then release the job.
You can specifiy the job&#39;s ID, or just use &quot;-all&quot; to release all held jobs.

&lt;pre&gt;
$ &lt;strong&gt;chmod u+w results.output&lt;/strong&gt;

$ &lt;strong&gt;condor_release -all&lt;/strong&gt;
All jobs released.
&lt;/pre&gt;

&lt;p&gt;Again, watch the log until the job finishes:

&lt;pre&gt;
$ &lt;strong&gt;tail -f --lines=500 results.log&lt;/strong&gt;
000 (003.000.000) 07/12 22:35:44 Job submitted from host: &lt;192.167.1.23:32864&gt;
...
017 (003.000.000) 07/12 22:35:57 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:33178/12497/1058042148/
    Can-Restart-JM: 1
...
001 (003.000.000) 07/12 22:35:57 Job executing on host: gk2
...
012 (003.000.000) 07/12 22:36:52 Job was held.
        Globus error 129: the standard output/error size is different
...
013 (003.000.000) 07/12 22:44:33 Job was released.
        via condor_release (by user Todd)
...
001 (003.000.000) 07/12 22:44:46 Job executing on host: gk2
...
005 (003.000.000) 07/12 22:44:51 Job terminated.
        (1) Normal termination (return value 0)
                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
        0  -  Run Bytes Sent By Job
        0  -  Run Bytes Received By Job
        0  -  Total Bytes Sent By Job
        0  -  Total Bytes Received By Job
...
&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Your job finished, the results have been retreived successfully:

&lt;pre&gt;
$ &lt;strong&gt;cat results.output&lt;/strong&gt;
I&#39;m process id 12528 on gk2
Sat Jul 12 22:35:53 CEST 2003
Running as binary /home/home45/Aland/.globus/.gass_cache/local/md5/6d/217f3f7926c06a529143f6129bf269/md5/a7/2af94ba728c69c588e523a99baaefd/data TestJob 10
My name (argument 1) is TestJob
My sleep duration (argument 2) is 10
Sleep of 10 seconds finished.  Exiting
RESULT: 0 SUCCESS
&lt;/pre&gt;

&lt;p&gt;Before continuing, clean up the results:

&lt;pre&gt;
$ &lt;strong&gt;rm results.*&lt;/strong&gt;
&lt;/pre&gt;


&lt;a name=&quot;simple_dag&quot;&gt;&lt;/a&gt;

&lt;h2&gt;Part IV: A Simple DAG&lt;/h2&gt;

&lt;p&gt;Since it will be handy for the rest of the tutorial, create a
little shell script to monitor the Condor-G queue:

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt; watch_condor_q
#! /bin/sh
while true; do
     condor_q
     condor_q -globus
     sleep 10
done
&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat watch_condor_q&lt;/strong&gt;
#! /bin/sh
while true; do
     condor_q
     condor_q -globus
     sleep 10
done
$ &lt;strong&gt;chmod a+x watch_condor_q &lt;/strong&gt;

&lt;/pre&gt;

&lt;p&gt;Create a minimal DAG for DAGMan.  This DAG will have a single node.

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt; mydag.dag
Job HelloWorld myjob.submit
&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat mydag.dag&lt;/strong&gt;
Job HelloWorld myjob.submit
&lt;/pre&gt;

&lt;p&gt;Submit it with 

&lt;a
href=&quot;http://www.cs.wisc.edu/condor/manual/v6.7/condor_submit_dag.html&quot;&gt;condor_submit_dag&lt;/a&gt;, then watch the run.  Notice that
condor_dagman is running as a job and that condor_dagman submits your real
job without your direct intervention.  You might happen to catch the &quot;C&quot; (completed) state as your job finishes, but that often goes by too quickly to notice.

&lt;p&gt;Again, in another window you may want to run
&quot;&lt;tt&gt;tail -f --lines=500 results.log&lt;/tt&gt;&quot; in a second window
to watch the job log file as your job runs.  You might also want to watch
DAGMan&#39;s log file with
&quot;&lt;tt&gt;tail -f --lines=500 mydag.dag.dagman.out&lt;/tt&gt;&quot;
 in a
third window.
(mydag.dag.dagman.out) in the same way in a third window.
  For the remainder of this tutorial, we suggest you re-run this command
when you submit a DAG.  This will allow you to see how typical DAGs progress.  Use &quot;&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt;&quot; to stop watching the file.

&lt;p&gt;Third window: 
&lt;pre&gt;
$ &lt;strong&gt;cd ~/condor-g-dagman-tutorial/submit&lt;/strong&gt;
$ &lt;strong&gt;tail -f --lines=500 mydag.dag.dagman.out&lt;/strong&gt;

7/10 10:36:43 ******************************************************
7/10 10:36:43 ** condor_scheduniv_exec.6.0 (CONDOR_DAGMAN) STARTING UP
7/10 10:36:43 ** $CondorVersion: 6.5.1 Apr 22 2003 $
7/10 10:36:43 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 10:36:43 ** PID = 26844
7/10 10:36:43 ******************************************************
7/10 10:36:44 DaemonCore: Command Socket at &amp;lt;128.105.185.14:34571&amp;gt;
7/10 10:36:44 argv[0] == &quot;condor_scheduniv_exec.6.0&quot;
7/10 10:36:44 argv[1] == &quot;-Debug&quot;
7/10 10:36:44 argv[2] == &quot;3&quot;
7/10 10:36:44 argv[3] == &quot;-Lockfile&quot;
7/10 10:36:44 argv[4] == &quot;mydag.dag.lock&quot;
7/10 10:36:44 argv[5] == &quot;-Condorlog&quot;
7/10 10:36:44 argv[6] == &quot;results.log&quot;
7/10 10:36:44 argv[7] == &quot;-Dag&quot;
7/10 10:36:44 argv[8] == &quot;mydag.dag&quot;
7/10 10:36:44 argv[9] == &quot;-Rescue&quot;
7/10 10:36:44 argv[10] == &quot;mydag.dag.rescue&quot;
7/10 10:36:44 Condor log will be written to results.log
7/10 10:36:44 DAG Lockfile will be written to mydag.dag.lock
7/10 10:36:44 DAG Input file is mydag.dag
7/10 10:36:44 Rescue DAG will be written to mydag.dag.rescue
7/10 10:36:44 Parsing mydag.dag ...
7/10 10:36:44 Dag contains 1 total jobs
7/10 10:36:44 Bootstrapping...
7/10 10:36:44 Number of pre-completed jobs: 0
7/10 10:36:44 Submitting Job HelloWorld ...
7/10 10:36:44    assigned Condor ID (7.0.0)
7/10 10:36:45 Event: ULOG_SUBMIT for Job HelloWorld (7.0.0)
7/10 10:36:45 0/1 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 10:37:05 Event: ULOG_GLOBUS_SUBMIT for Job HelloWorld (7.0.0)
7/10 10:37:05 Event: ULOG_EXECUTE for Job HelloWorld (7.0.0)
7/10 10:38:10 Event: ULOG_JOB_TERMINATED for Job HelloWorld (7.0.0)
7/10 10:38:10 Job HelloWorld completed successfully.
7/10 10:38:10 1/1 done, 0 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 10:38:10 All jobs Completed!
7/10 10:38:10 **** condor_scheduniv_exec.6.0 (condor_DAGMAN) EXITING WITH STATUS 0
&lt;/pre&gt;

&lt;p&gt;First window:
&lt;pre&gt;
$ &lt;strong&gt;condor_submit_dag mydag.dag&lt;/strong&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.condor.sub
Log of DAGMan debugging messages         : mydag.dag.dagman.out
Log of Condor library debug messages     : mydag.dag.lib.out
Log of the life of condor_dagman itself  : mydag.dag.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 2.
-----------------------------------------------------------------------
$ &lt;strong&gt;./watch_condor_q &lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   2.0   adesmet         7/10 17:33   0+00:00:03 R  0   2.6  condor_dagman -f -
   3.0   adesmet         7/10 17:33   0+00:00:00 I  0   0.0  myscript.sh TestJo

2 jobs; 1 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   3.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   2.0   adesmet         7/10 17:33   0+00:00:33 R  0   2.6  condor_dagman -f -
   3.0   adesmet         7/10 17:33   0+00:00:15 R  0   0.0  myscript.sh TestJo

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   3.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   2.0   adesmet         7/10 17:33   0+00:01:03 R  0   2.6  condor_dagman -f -
   3.0   adesmet         7/10 17:33   0+00:00:45 R  0   0.0  myscript.sh TestJo

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   3.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


&lt;em&gt;&lt;strong&gt;Ctrl-C&lt;/strong&gt;&lt;/em&gt;
&lt;/pre&gt;

&lt;p&gt;Verify your results:

&lt;pre&gt;
$ &lt;strong&gt;ls -l&lt;/strong&gt;
total 12
-rw-r--r--    1 adesmet  adesmet        28 Jul 10 10:35 mydag.dag
-rw-r--r--    1 adesmet  adesmet       523 Jul 10 10:36 mydag.dag.condor.sub
-rw-r--r--    1 adesmet  adesmet       608 Jul 10 10:38 mydag.dag.dagman.log
-rw-r--r--    1 adesmet  adesmet      1860 Jul 10 10:38 mydag.dag.dagman.out
-rw-r--r--    1 adesmet  adesmet        29 Jul 10 10:38 mydag.dag.lib.out
-rw-------    1 adesmet  adesmet         0 Jul 10 10:36 mydag.dag.lock
-rw-r--r--    1 adesmet  adesmet       175 Jul  9 18:13 myjob.submit
-rwxr-xr-x    1 adesmet  adesmet       194 Jul 10 10:36 myscript.sh
-rw-r--r--    1 adesmet  adesmet        31 Jul 10 10:37 results.error
-rw-------    1 adesmet  adesmet       833 Jul 10 10:38 results.log
-rw-r--r--    1 adesmet  adesmet       261 Jul 10 10:37 results.output
-rwxr-xr-x    1 adesmet  adesmet        81 Jul 10 10:35 watch_condor_q
$ &lt;strong&gt;cat results.error &lt;/strong&gt;
This is sent to standard error
$ &lt;strong&gt;cat results.output &lt;/strong&gt;
I&#39;m process id 29149 on gk2
This is sent to standard error
Thu Jul 10 10:38:44 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/aa/ceb9e04077256aaa2acf4dff670897/md5/27/2f50da149fc049d07b1c27f30b67df/data TEST 1
My name (argument 1) is TEST
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting
RESULT: 0 SUCCESS
&lt;/pre&gt;

&lt;p&gt;Looking at DAGMan&#39;s various files, we see that DAGMan itself ran as a
Condor job (specifically, a scheduler universe job):

&lt;pre&gt;
$ &lt;strong&gt;ls&lt;/strong&gt;
mydag.dag         mydag.dag.dagman.log  mydag.dag.lib.out  myjob.submit  results.error  results.output
mydag.dag.condor.sub  mydag.dag.dagman.out  mydag.dag.lock     myscript.sh   results.log    watch_condor_q
$ &lt;strong&gt;cat mydag.dag.condor.sub&lt;/strong&gt;
# Filename: mydag.dag.condor.sub
# Generated by condor_submit_dag mydag.dag
universe   = scheduler
executable   = /afs/cs.wisc.edu/u/a/d/adesmet/miron-condor-g-dagman-talk/vdt/condor/bin/condor_dagman
getenv      = True
output      = mydag.dag.lib.out
error      = mydag.dag.lib.out
log      = mydag.dag.dagman.log
remove_kill_sig   = SIGUSR1
arguments   = -f -l . -Debug 3 -Lockfile mydag.dag.lock -Condorlog results.log -Dag mydag.dag -Rescue mydag.dag.rescue
environment   = _CONDOR_DAGMAN_LOG=mydag.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0
queue
$ &lt;strong&gt;cat mydag.dag.dagman.log&lt;/strong&gt;
000 (006.000.000) 07/10 10:36:43 Job submitted from host: &amp;lt;128.105.185.14:33785&amp;gt;
...
001 (006.000.000) 07/10 10:36:44 Job executing on host: &amp;lt;128.105.185.14:33785&amp;gt;

...
005 (006.000.000) 07/10 10:38:10 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
&lt;/pre&gt;

&lt;p&gt;If you weren&#39;t watching the DAGMan output file with tail -f, you can examine the file with the following command:

&lt;pre&gt;$ &lt;strong&gt;cat mydag.dag.dagman.out&lt;/strong&gt;&lt;/pre&gt;

&lt;p&gt;Clean up your results.  Be careful when deleting the mydag.dag.*
files, you do not want to delete the mydag.dag file, just mydag.dag.*
.

&lt;pre&gt;
$ &lt;strong&gt;rm mydag.dag.* results.*&lt;/strong&gt;
&lt;/pre&gt;

&lt;a name=&quot;complex_dag&quot;&gt;&lt;/a&gt;
&lt;h2&gt;Part V: A More Complex DAG&lt;/h2&gt;

&lt;p&gt;Typically each node in a DAG will have its own Condor submit file.
Create some more submit files by copying our existing file.  For simplicity
during this tutorial, we&#39;ll keep the submit files very similar, notably
using the same executable, but your submit files and executables can differ
in real-world use.

&lt;pre&gt;
$ &lt;strong&gt;cp myjob.submit job.setup.submit&lt;/strong&gt;
$ &lt;strong&gt;cp myjob.submit job.work1.submit&lt;/strong&gt;
$ &lt;strong&gt;cp myjob.submit job.work2.submit&lt;/strong&gt;
$ &lt;strong&gt;cp myjob.submit job.workfinal.submit&lt;/strong&gt;
$ &lt;strong&gt;cp myjob.submit job.finalize.submit&lt;/strong&gt;

&lt;/pre&gt;

&lt;p&gt;Edit the various submit files.  Change the output and error entries
to point to results.NODE.output and results.NODE.error files where
NODE is actually the middle word in the submit file (job.NODE.submit).
So job.finalize.error would include:

&lt;blockquote&gt;
&lt;pre&gt;
output=results.finalize.output
error=results.finalize.error
&lt;/pre&gt;
&lt;/blockquote&gt;


&lt;p&gt;Here is one possible set of settings for the output entries:

&lt;pre&gt;
$ &lt;strong&gt;grep &#39;^output=&#39; job.*.submit&lt;/strong&gt;
job.finalize.submit:output=results.finalize.output
job.setup.submit:output=results.setup.output
job.work1.submit:output=results.work1.output
job.work2.submit:output=results.work2.output
job.workfinal.submit:output=results.workfinal.output
&lt;/pre&gt;


&lt;p&gt;This is important so that the various nodes don&#39;t overwrite each others
output.

&lt;p&gt;Leave the log entries alone.  DAGMan requires that all nodes output
their logs in the same location.  Condor will ensure that the
different jobs will not overwrite each other&#39;s entries in the log.
(Never versions of DAGMan lift this requirement, and allow each job to
use its own log file -- but you may want to use one common log file
anyway because it&#39;s convenient to have all of your job status
information in a single place.)

&lt;blockquote&gt;
&lt;pre&gt;
log=results.log
&lt;/pre&gt;
&lt;/blockquote&gt;

&lt;p&gt;Also change the arguments entries so that the first argument is
something unique to each node (perhaps the NODE name).

&lt;p&gt;For node work2, change the second argument to 120 so that it looks
something like:

&lt;blockquote&gt;
&lt;pre&gt;
arguments=MyWorkerNode2 120
&lt;/pre&gt;
&lt;/blockquote&gt;

&lt;p&gt;Add the new nodes to your DAG:

&lt;pre&gt;
$ &lt;strong&gt;cat mydag.dag &lt;/strong&gt;
Job HelloWorld myjob.submit
$ &lt;strong&gt;cat &amp;gt;&amp;gt; mydag.dag
Job Setup job.setup.submit
Job WorkerNode_1 job.work1.submit
Job WorkerNode_Two job.work2.submit
Job CollectResults job.workfinal.submit
Job LastNode job.finalize.submit
PARENT Setup CHILD WorkerNode_1 WorkerNode_Two
PARENT WorkerNode_1 WorkerNode_Two CHILD CollectResults
PARENT CollectResults CHILD LastNode
&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat mydag.dag &lt;/strong&gt;
Job HelloWorld myjob.submit
Job Setup job.setup.submit
Job WorkerNode_1 job.work1.submit
Job WorkerNode_Two job.work2.submit
Job CollectResults job.workfinal.submit
Job LastNode job.finalize.submit
PARENT Setup CHILD WorkerNode_1 WorkerNode_Two
PARENT WorkerNode_1 WorkerNode_Two CHILD CollectResults
PARENT CollectResults CHILD LastNode
&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;

&lt;/pre&gt;

&lt;p&gt;condor_q -dag will organize jobs into their associated DAGs.  Change
watch_condor_q to use this:

&lt;pre&gt;
$ &lt;strong&gt;rm watch_condor_q&lt;/strong&gt;
$ &lt;strong&gt;cat &amp;gt; watch_condor_q
#! /bin/sh
while true; do
    echo ....
    echo .... Output from condor_q
    echo ....
     condor_q
    echo ....
    echo .... Output from condor_q -globus
    echo ....
     condor_q -globus
    echo ....
    echo .... Output from condor_q -dag
    echo ....
     condor_q -dag
     sleep 10
done
&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat watch_condor_q&lt;/strong&gt;
#! /bin/sh
while true; do
    echo ....
    echo .... Output from condor_q
    echo ....
     condor_q
    echo ....
    echo .... Output from condor_q -globus
    echo ....
     condor_q -globus
    echo ....
    echo .... Output from condor_q -dag
    echo ....
     condor_q -dag
     sleep 10
done
$ &lt;strong&gt;chmod a+x watch_condor_q &lt;/strong&gt;

&lt;/pre&gt;

&lt;p&gt;Submit your new DAG and monitor it.

&lt;p&gt;Again, in separate windows you may want to run 
&quot;&lt;tt&gt;tail -f --lines=500 results.log&lt;/tt&gt;&quot; and &quot;&lt;tt&gt;tail -f --lines=500
mydag.dag.dagman.out&lt;/tt&gt;&quot; to monitor the job&#39;s progress.

&lt;pre&gt;
$ &lt;strong&gt;condor_submit_dag mydag.dag&lt;/strong&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.condor.sub
Log of DAGMan debugging messages         : mydag.dag.dagman.out
Log of Condor library debug messages     : mydag.dag.lib.out
Log of the life of condor_dagman itself  : mydag.dag.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 8.
-----------------------------------------------------------------------
$ &lt;strong&gt;./watch_condor_q&lt;/strong&gt;

-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:00:08 R  0   2.6  condor_dagman -f -
   5.0   adesmet         7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh TestJo
   6.0   adesmet         7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   5.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
   6.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:00:08 R  0   2.6  condor_dagman -f -
   5.0    |-HelloWorld   7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh TestJo
   6.0    |-Setup        7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:00:12 R  0   2.6  condor_dagman -f -
   5.0   adesmet         7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh TestJo
   6.0   adesmet         7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   5.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
   6.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:00:12 R  0   2.6  condor_dagman -f -
   5.0    |-HelloWorld   7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh TestJo
   6.0    |-Setup        7/10 17:45   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:00:42 R  0   2.6  condor_dagman -f -
   5.0   adesmet         7/10 17:45   0+00:00:24 R  0   0.0  myscript.sh TestJo
   6.0   adesmet         7/10 17:45   0+00:00:24 R  0   0.0  myscript.sh Setup 

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   5.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
   6.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:00:42 R  0   2.6  condor_dagman -f -
   5.0    |-HelloWorld   7/10 17:45   0+00:00:24 R  0   0.0  myscript.sh TestJo
   6.0    |-Setup        7/10 17:45   0+00:00:24 R  0   0.0  myscript.sh Setup 

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:01:12 R  0   2.6  condor_dagman -f -
   5.0   adesmet         7/10 17:45   0+00:00:54 R  0   0.0  myscript.sh TestJo
   6.0   adesmet         7/10 17:45   0+00:00:54 R  0   0.0  myscript.sh Setup 

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   5.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
   6.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:01:12 R  0   2.6  condor_dagman -f -
   5.0    |-HelloWorld   7/10 17:45   0+00:00:54 R  0   0.0  myscript.sh TestJo
   6.0    |-Setup        7/10 17:45   0+00:00:54 R  0   0.0  myscript.sh Setup 

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:01:42 R  0   2.6  condor_dagman -f -
   7.0   adesmet         7/10 17:46   0+00:00:00 I  0   0.0  myscript.sh work1 
   8.0   adesmet         7/10 17:46   0+00:00:00 I  0   0.0  myscript.sh Worker

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   7.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
   8.0   adesmet       UNSUBMITTED fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:01:42 R  0   2.6  condor_dagman -f -
   7.0    |-WorkerNode_  7/10 17:46   0+00:00:00 I  0   0.0  myscript.sh work1 
   8.0    |-WorkerNode_  7/10 17:46   0+00:00:00 I  0   0.0  myscript.sh Worker

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:02:12 R  0   2.6  condor_dagman -f -
   7.0   adesmet         7/10 17:46   0+00:00:27 R  0   0.0  myscript.sh work1 
   8.0   adesmet         7/10 17:46   0+00:00:27 R  0   0.0  myscript.sh Worker

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   7.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
   8.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:02:12 R  0   2.6  condor_dagman -f -
   7.0    |-WorkerNode_  7/10 17:46   0+00:00:27 R  0   0.0  myscript.sh work1 
   8.0    |-WorkerNode_  7/10 17:46   0+00:00:27 R  0   0.0  myscript.sh Worker

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:02:42 R  0   2.6  condor_dagman -f -
   7.0   adesmet         7/10 17:46   0+00:00:57 R  0   0.0  myscript.sh work1 
   8.0   adesmet         7/10 17:46   0+00:00:57 R  0   0.0  myscript.sh Worker

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   7.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond
   8.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:02:43 R  0   2.6  condor_dagman -f -
   7.0    |-WorkerNode_  7/10 17:46   0+00:00:58 R  0   0.0  myscript.sh work1 
   8.0    |-WorkerNode_  7/10 17:46   0+00:00:58 R  0   0.0  myscript.sh Worker

3 jobs; 0 idle, 3 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:03:13 R  0   2.6  condor_dagman -f -
   8.0   adesmet         7/10 17:46   0+00:01:28 R  0   0.0  myscript.sh Worker

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   8.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:03:13 R  0   2.6  condor_dagman -f -
   8.0    |-WorkerNode_  7/10 17:46   0+00:01:28 R  0   0.0  myscript.sh Worker

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:03:43 R  0   2.6  condor_dagman -f -
   8.0   adesmet         7/10 17:46   0+00:01:58 R  0   0.0  myscript.sh Worker

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   8.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:03:43 R  0   2.6  condor_dagman -f -
   8.0    |-WorkerNode_  7/10 17:46   0+00:01:58 R  0   0.0  myscript.sh Worker

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:04:13 R  0   2.6  condor_dagman -f -
   9.0   adesmet         7/10 17:49   0+00:00:02 R  0   0.0  myscript.sh workfi

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   9.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:04:13 R  0   2.6  condor_dagman -f -
   9.0    |-CollectResu  7/10 17:49   0+00:00:02 R  0   0.0  myscript.sh workfi

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:04:43 R  0   2.6  condor_dagman -f -
   9.0   adesmet         7/10 17:49   0+00:00:32 R  0   0.0  myscript.sh workfi

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   9.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:04:43 R  0   2.6  condor_dagman -f -
   9.0    |-CollectResu  7/10 17:49   0+00:00:32 R  0   0.0  myscript.sh workfi

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:05:13 R  0   2.6  condor_dagman -f -
   9.0   adesmet         7/10 17:49   0+00:01:02 R  0   0.0  myscript.sh workfi

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   9.0   adesmet       DONE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:05:13 R  0   2.6  condor_dagman -f -
   9.0    |-CollectResu  7/10 17:49   0+00:01:02 C  0   0.0  myscript.sh workfi

1 jobs; 0 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:05:43 R  0   2.6  condor_dagman -f -
  10.0   adesmet         7/10 17:50   0+00:00:13 R  0   0.0  myscript.sh Final 

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  10.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:05:44 R  0   2.6  condor_dagman -f -
  10.0    |-LastNode     7/10 17:50   0+00:00:13 R  0   0.0  myscript.sh Final 

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:06:14 R  0   2.6  condor_dagman -f -
  10.0   adesmet         7/10 17:50   0+00:00:43 R  0   0.0  myscript.sh Final 

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  10.0   adesmet       ACTIVE fork     gk2   /tmp/&lt;em&gt;username&lt;/em&gt;-cond


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   4.0   adesmet         7/10 17:45   0+00:06:14 R  0   2.6  condor_dagman -f -
  10.0    |-LastNode     7/10 17:50   0+00:00:43 R  0   0.0  myscript.sh Final 

2 jobs; 0 idle, 2 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:35688&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held

&lt;em&gt;&lt;strong&gt;Ctrl-C&lt;/strong&gt;&lt;/em&gt;
&lt;/pre&gt;

&lt;p&gt;Watching the logs or the condor_q output, you&#39;ll note that the
CollectResults node (&quot;workfinal&quot;) wasn&#39;t run until both of the WorkerNode
nodes (&quot;work1&quot; and &quot;work2&quot;) finished.

&lt;p&gt;Examine your results.

&lt;pre&gt;
$ &lt;strong&gt;ls&lt;/strong&gt;
job.finalize.submit   mydag.dag.condor.sub  myscript.sh           results.setup.error   results.workfinal.error
job.setup.submit      mydag.dag.dagman.log  results.error        results.setup.output  results.workfinal.output
job.work1.submit      mydag.dag.dagman.out  results.finalize.error   results.work1.error   watch_condor_q
job.work2.submit      mydag.dag.lib.out     results.finalize.output  results.work1.output
job.workfinal.submit  mydag.dag.lock       results.log           results.work2.error
mydag.dag         myjob.submit       results.output        results.work2.output
$ &lt;strong&gt;tail --lines=500 results.*.error&lt;/strong&gt;
==&amp;gt; results.finalize.error &amp;lt;==
This is sent to standard error

==&amp;gt; results.setup.error &amp;lt;==
This is sent to standard error

==&amp;gt; results.work1.error &amp;lt;==
This is sent to standard error

==&amp;gt; results.work2.error &amp;lt;==
This is sent to standard error

==&amp;gt; results.workfinal.error &amp;lt;==
This is sent to standard error
$ &lt;strong&gt;tail --lines=500 results.*.output&lt;/strong&gt;

==&amp;gt; results.finalize.output &amp;lt;==
I&#39;m process id 29614 on gk2
Thu Jul 10 10:53:58 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/0d/7c60aa10b34817d3ffe467dd116816/md5/de/03c3eb8a20852948a2af53438bbce1/data Finalize 1
My name (argument 1) is Finalize
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting

==&amp;gt; results.setup.output &amp;lt;==
I&#39;m process id 29337 on gk2
Thu Jul 10 10:50:31 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/a5/fab7b658db65dbfec3ecf0a5414e1c/md5/f4/e9a04ae03bff43f00a10c78ebd60fd/data Setup 1
My name (argument 1) is Setup
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting

==&amp;gt; results.work1.output &amp;lt;==
I&#39;m process id 29444 on gk2
Thu Jul 10 10:51:04 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/2e/17db42df4e113f813cea7add42e03e/md5/f6/f1bd82a2fec9a3a372a44c009a63ca/data WorkerNode1 1
My name (argument 1) is WorkerNode1
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting

==&amp;gt; results.work2.output &amp;lt;==
I&#39;m process id 29432 on gk2
Thu Jul 10 10:51:03 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/ea/9a3c8d16346b2fea808cda4b5969fa/md5/f6/f1bd82a2fec9a3a372a44c009a63ca/data WorkerNode2 120
My name (argument 1) is WorkerNode2
My sleep duration (argument 2) is 120
Sleep of 120 seconds finished.  Exiting

==&amp;gt; results.workfinal.output &amp;lt;==
I&#39;m process id 29554 on gk2
Thu Jul 10 10:53:27 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/c9/7ba5d43acad3d9ebdfa633839e75c3/md5/11/cd84efa75305d54100f0f451b46b35/data WorkFinal 1
My name (argument 1) is WorkFinal
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting
&lt;/pre&gt;

&lt;p&gt;Examine your log

&lt;pre&gt;
$ &lt;strong&gt;cat results.log&lt;/strong&gt;
000 (005.000.000) 07/10 17:45:24 Job submitted from host: &lt;128.105.185.14:35688&gt;
    DAG Node: HelloWorld
...
000 (006.000.000) 07/10 17:45:24 Job submitted from host: &lt;128.105.185.14:35688&gt;
    DAG Node: Setup
...
017 (006.000.000) 07/10 17:45:42 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:2349/914/1057877133/
    Can-Restart-JM: 1
...
001 (006.000.000) 07/10 17:45:42 Job executing on host: gk2
...
017 (005.000.000) 07/10 17:45:42 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:2348/915/1057877133/
    Can-Restart-JM: 1
...
001 (005.000.000) 07/10 17:45:42 Job executing on host: gk2
...
005 (005.000.000) 07/10 17:46:50 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
005 (006.000.000) 07/10 17:46:50 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
000 (007.000.000) 07/10 17:46:55 Job submitted from host: &lt;128.105.185.14:35688&gt;
    DAG Node: WorkerNode_1
...
000 (008.000.000) 07/10 17:46:56 Job submitted from host: &lt;128.105.185.14:35688&gt;
    DAG Node: WorkerNode_Two
...
017 (008.000.000) 07/10 17:47:09 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:2364/1037/1057877219/
    Can-Restart-JM: 1
...
001 (008.000.000) 07/10 17:47:09 Job executing on host: gk2
...
017 (007.000.000) 07/10 17:47:09 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:2367/1040/1057877220/
    Can-Restart-JM: 1
...
001 (007.000.000) 07/10 17:47:09 Job executing on host: gk2
...
005 (007.000.000) 07/10 17:48:17 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
005 (008.000.000) 07/10 17:49:18 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
000 (009.000.000) 07/10 17:49:22 Job submitted from host: &lt;128.105.185.14:35688&gt;
    DAG Node: CollectResults
...
017 (009.000.000) 07/10 17:49:35 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:2383/1185/1057877366/
    Can-Restart-JM: 1
...
001 (009.000.000) 07/10 17:49:35 Job executing on host: gk2
...
005 (009.000.000) 07/10 17:50:42 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
000 (010.000.000) 07/10 17:50:42 Job submitted from host: &lt;128.105.185.14:35688&gt;
    DAG Node: LastNode
...
017 (010.000.000) 07/10 17:50:55 Job submitted to Globus
    RM-Contact: gk2:/jobmanager-fork
    JM-Contact: https://gk2:2392/1247/1057877446/
    Can-Restart-JM: 1
...
001 (010.000.000) 07/10 17:50:55 Job executing on host: gk2
...
005 (010.000.000) 07/10 17:52:02 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
&lt;/pre&gt;

&lt;p&gt;Examine the DAGMan log

&lt;pre&gt;

$ &lt;strong&gt;cat mydag.dag.dagman.out&lt;/strong&gt;
7/10 17:45:24 ******************************************************
7/10 17:45:24 ** condor_scheduniv_exec.4.0 (CONDOR_DAGMAN) STARTING UP
7/10 17:45:24 ** $CondorVersion: 6.5.1 Apr 22 2003 $
7/10 17:45:24 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 17:45:24 ** PID = 18826
7/10 17:45:24 ******************************************************
7/10 17:45:24 DaemonCore: Command Socket at &lt;128.105.185.14:35774&gt;
7/10 17:45:24 argv[0] == &quot;condor_scheduniv_exec.4.0&quot;
7/10 17:45:24 argv[1] == &quot;-Debug&quot;
7/10 17:45:24 argv[2] == &quot;3&quot;
7/10 17:45:24 argv[3] == &quot;-Lockfile&quot;
7/10 17:45:24 argv[4] == &quot;mydag.dag.lock&quot;
7/10 17:45:24 argv[5] == &quot;-Condorlog&quot;
7/10 17:45:24 argv[6] == &quot;results.log&quot;
7/10 17:45:24 argv[7] == &quot;-Dag&quot;
7/10 17:45:24 argv[8] == &quot;mydag.dag&quot;
7/10 17:45:24 argv[9] == &quot;-Rescue&quot;
7/10 17:45:24 argv[10] == &quot;mydag.dag.rescue&quot;
7/10 17:45:24 Condor log will be written to results.log
7/10 17:45:24 DAG Lockfile will be written to mydag.dag.lock
7/10 17:45:24 DAG Input file is mydag.dag
7/10 17:45:24 Rescue DAG will be written to mydag.dag.rescue
7/10 17:45:24 Parsing mydag.dag ...
7/10 17:45:24 Dag contains 6 total jobs
7/10 17:45:24 Bootstrapping...
7/10 17:45:24 Number of pre-completed jobs: 0
7/10 17:45:24 Submitting Job HelloWorld ...
7/10 17:45:24    assigned Condor ID (5.0.0)
7/10 17:45:24 Submitting Job Setup ...
7/10 17:45:24    assigned Condor ID (6.0.0)
7/10 17:45:25 Event: ULOG_SUBMIT for Job HelloWorld (5.0.0)
7/10 17:45:25 Event: ULOG_SUBMIT for Job Setup (6.0.0)
7/10 17:45:25 0/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 17:45:45 Event: ULOG_GLOBUS_SUBMIT for Job Setup (6.0.0)
7/10 17:45:45 Event: ULOG_EXECUTE for Job Setup (6.0.0)
7/10 17:45:45 Event: ULOG_GLOBUS_SUBMIT for Job HelloWorld (5.0.0)
7/10 17:45:45 Event: ULOG_EXECUTE for Job HelloWorld (5.0.0)
7/10 17:46:55 Event: ULOG_JOB_TERMINATED for Job HelloWorld (5.0.0)
7/10 17:46:55 Job HelloWorld completed successfully.
7/10 17:46:55 Event: ULOG_JOB_TERMINATED for Job Setup (6.0.0)
7/10 17:46:55 Job Setup completed successfully.
7/10 17:46:55 Submitting Job WorkerNode_1 ...
7/10 17:46:55    assigned Condor ID (7.0.0)
7/10 17:46:55 Submitting Job WorkerNode_Two ...
7/10 17:46:56    assigned Condor ID (8.0.0)
7/10 17:46:56 Event: ULOG_SUBMIT for Job WorkerNode_1 (7.0.0)
7/10 17:46:56 Event: ULOG_SUBMIT for Job WorkerNode_Two (8.0.0)
7/10 17:46:56 2/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 17:47:11 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_Two (8.0.0)
7/10 17:47:11 Event: ULOG_EXECUTE for Job WorkerNode_Two (8.0.0)
7/10 17:47:11 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_1 (7.0.0)
7/10 17:47:11 Event: ULOG_EXECUTE for Job WorkerNode_1 (7.0.0)
7/10 17:48:21 Event: ULOG_JOB_TERMINATED for Job WorkerNode_1 (7.0.0)
7/10 17:48:21 Job WorkerNode_1 completed successfully.
7/10 17:48:21 3/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 17:49:21 Event: ULOG_JOB_TERMINATED for Job WorkerNode_Two (8.0.0)
7/10 17:49:21 Job WorkerNode_Two completed successfully.
7/10 17:49:21 Submitting Job CollectResults ...
7/10 17:49:22    assigned Condor ID (9.0.0)
7/10 17:49:22 Event: ULOG_SUBMIT for Job CollectResults (9.0.0)
7/10 17:49:22 4/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 17:49:37 Event: ULOG_GLOBUS_SUBMIT for Job CollectResults (9.0.0)
7/10 17:49:37 Event: ULOG_EXECUTE for Job CollectResults (9.0.0)
7/10 17:50:42 Event: ULOG_JOB_TERMINATED for Job CollectResults (9.0.0)
7/10 17:50:42 Job CollectResults completed successfully.
7/10 17:50:42 Submitting Job LastNode ...
7/10 17:50:42    assigned Condor ID (10.0.0)
7/10 17:50:42 Event: ULOG_SUBMIT for Job LastNode (10.0.0)
7/10 17:50:42 5/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 17:50:57 Event: ULOG_GLOBUS_SUBMIT for Job LastNode (10.0.0)
7/10 17:50:57 Event: ULOG_EXECUTE for Job LastNode (10.0.0)
7/10 17:52:02 Event: ULOG_JOB_TERMINATED for Job LastNode (10.0.0)
7/10 17:52:02 Job LastNode completed successfully.
7/10 17:52:02 6/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 17:52:02 All jobs Completed!
7/10 17:52:02 **** condor_scheduniv_exec.4.0 (condor_DAGMAN) EXITING WITH STATUS 0
&lt;/pre&gt;

&lt;p&gt;Clean up your results.  Be careful about deleting the mydag.dag.* files,
you do not want to delete the mydag.dag file, just mydag.dag.* .

&lt;pre&gt;
$ &lt;strong&gt;rm mydag.dag.* results.*&lt;/strong&gt;
&lt;/pre&gt;


&lt;a name=&quot;optional_multi_resource&quot;&gt;&lt;/a&gt;
&lt;h3&gt;Optional: Multiple Globus Schedulers&lt;/h3&gt;

&lt;p&gt;If you&#39;re ahead of schedule, you can try redoing this section, but with
other Grid sites.  Modify some of the grid_resource entries in your
submit files to point to &lt;tt&gt;evitable.uchicago.edu&lt;/tt&gt;,
&lt;tt&gt;terminable.uchicago.edu or gridlab3/jobmanager-condor&lt;/tt&gt;.  A single
DAG can send jobs to a variety of sites.  Condor-G is capable of managing
jobs being distributed to many different sites simultaneously.

&lt;a name=&quot;failed_dag&quot;&gt;&lt;/a&gt;
&lt;h2&gt;Part VI: Handling Jobs That Fail with DAGMan&lt;/h2&gt;

&lt;p&gt;DAGMan can handle a situation where some of the nodes in a DAG fails.
DAGMan will run as many nodes as possible, then create a rescue DAG making
it easy to continue when the problem is fixed.

&lt;p&gt;Let&#39;s create a script that will fail so we can see this:

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt; myscript2.sh
#! /bin/sh

echo &quot;I&#39;m process id $$ on&quot; `hostname`
echo &quot;This is sent to standard error&quot; 1&amp;gt;&amp;2
date
echo &quot;Running as binary $0&quot; &quot;$@&quot;
echo &quot;My name (argument 1) is $1&quot;
echo &quot;My sleep duration (argument 2) is $2&quot;
sleep $2
echo &quot;Sleep of $2 seconds finished.  Exiting&quot;
echo &quot;RESULT: 1 FAILURE&quot;
exit 1

&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat myscript2.sh&lt;/strong&gt;
#! /bin/sh

echo &quot;I&#39;m process id $$ on&quot; `hostname`
echo &quot;This is sent to standard error&quot; 1&amp;gt;&amp;2
date
echo &quot;Running as binary $0&quot; &quot;$@&quot;
echo &quot;My name (argument 1) is $1&quot;
echo &quot;My sleep duration (argument 2) is $2&quot;
sleep $2
echo &quot;Sleep of $2 seconds finished.  Exiting&quot;
echo &quot;RESULT: 1 FAILURE&quot;
exit 1
$ &lt;strong&gt;chmod a+x myscript2.sh&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Modify job.work2.submit to run myscript2.sh instead of myscript.sh:

&lt;pre&gt;
$ &lt;strong&gt;rm job.work2.submit&lt;/strong&gt;
$ &lt;strong&gt;cat &amp;gt; job.work2.submit
executable=myscript2.sh
output=results.work2.output
error=results.work2.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 gridlab2/jobmanager-fork
arguments=WorkerNode2 60
queue

&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat job.work2.submit&lt;/strong&gt;
executable=myscript2.sh
output=results.work2.output
error=results.work2.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 gridlab2/jobmanager-fork
arguments=WorkerNode2 60
queue
&lt;/pre&gt;

&lt;p&gt;Submit the dag again.

&lt;pre&gt;
$ &lt;strong&gt;condor_submit_dag mydag.dag&lt;/strong&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.condor.sub
Log of DAGMan debugging messages         : mydag.dag.dagman.out
Log of Condor library debug messages     : mydag.dag.lib.out
Log of the life of condor_dagman itself  : mydag.dag.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 15.
-----------------------------------------------------------------------
&lt;/pre&gt;

&lt;p&gt;Use watch_condor_q to watch the jobs until they finish.

&lt;p&gt;In separate windows run &quot;&lt;tt&gt;tail -f --lines=500 results.log&lt;/tt&gt;&quot; and &quot;&lt;tt&gt;tail -f --lines=500 mydag.dag.dagman.out&lt;/tt&gt;&quot; to monitor the job&#39;s progress.

&lt;pre&gt;
$ &lt;strong&gt;./watch_condor_q&lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  15.0   adesmet         7/10 11:11   0+00:00:04 R  0   2.6  condor_dagman -f -
  16.0   adesmet         7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh       
  17.0   adesmet         7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  16.0   adesmet       UNSUBMITTED fork     gk1   /afs/cs.wisc.edu/u
  17.0   adesmet       UNSUBMITTED fork     gk1   /afs/cs.wisc.edu/u


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  15.0   adesmet         7/10 11:11   0+00:00:04 R  0   2.6  condor_dagman -f -
  16.0    |-HelloWorld   7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh       
  17.0    |-Setup        7/10 11:11   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


&lt;em&gt;Output of watch_condor_q truncated&lt;/em&gt;

-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held
&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Check your results:

&lt;pre&gt;
$ &lt;strong&gt;ls&lt;/strong&gt;
job.finalize.submit   mydag.dag.condor.sub  myscript.sh           results.output      results.work2.output
job.setup.submit      mydag.dag.dagman.log  myscript2.sh        results.setup.error   results.workfinal.error
job.work1.submit      mydag.dag.dagman.out  results.error        results.setup.output  results.workfinal.output
job.work2.submit      mydag.dag.lib.out     results.finalize.error   results.work1.error   watch_condor_q
job.workfinal.submit  mydag.dag.lock       results.finalize.output  results.work1.output
mydag.dag         myjob.submit       results.log           results.work2.error
$ &lt;strong&gt;cat results.work2.output&lt;/strong&gt;
I&#39;m process id 29921 on pc-26
Thu Jul 10 11:12:42 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/87/459c159766cefb36f0d75023de0e35/md5/70/5d82b930ec61460d9c9ca65cbe5a8a/data WorkerNode2 60
My name (argument 1) is WorkerNode2
My sleep duration (argument 2) is 60
Sleep of 60 seconds finished.  Exiting
RESULT: 1 FAILURE
$ &lt;strong&gt;cat mydag.dag.dagman.out&lt;/strong&gt;
7/10 11:11:55 ******************************************************
7/10 11:11:55 ** condor_scheduniv_exec.15.0 (CONDOR_DAGMAN) STARTING UP
7/10 11:11:55 ** $CondorVersion: 6.5.1 Apr 22 2003 $
7/10 11:11:55 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 11:11:55 ** PID = 27126
7/10 11:11:55 ******************************************************
7/10 11:11:55 DaemonCore: Command Socket at &amp;lt;128.105.185.14:34769&amp;gt;
7/10 11:11:55 argv[0] == &quot;condor_scheduniv_exec.15.0&quot;
7/10 11:11:55 argv[1] == &quot;-Debug&quot;
7/10 11:11:55 argv[2] == &quot;3&quot;
7/10 11:11:55 argv[3] == &quot;-Lockfile&quot;
7/10 11:11:55 argv[4] == &quot;mydag.dag.lock&quot;
7/10 11:11:55 argv[5] == &quot;-Condorlog&quot;
7/10 11:11:55 argv[6] == &quot;results.log&quot;
7/10 11:11:55 argv[7] == &quot;-Dag&quot;
7/10 11:11:55 argv[8] == &quot;mydag.dag&quot;
7/10 11:11:55 argv[9] == &quot;-Rescue&quot;
7/10 11:11:55 argv[10] == &quot;mydag.dag.rescue&quot;
7/10 11:11:55 Condor log will be written to results.log
7/10 11:11:55 DAG Lockfile will be written to mydag.dag.lock
7/10 11:11:55 DAG Input file is mydag.dag
7/10 11:11:55 Rescue DAG will be written to mydag.dag.rescue
7/10 11:11:55 Parsing mydag.dag ...
7/10 11:11:55 Dag contains 6 total jobs
7/10 11:11:55 Bootstrapping...
7/10 11:11:55 Number of pre-completed jobs: 0
7/10 11:11:55 Submitting Job HelloWorld ...
7/10 11:11:55    assigned Condor ID (16.0.0)
7/10 11:11:55 Submitting Job Setup ...
7/10 11:11:55    assigned Condor ID (17.0.0)
7/10 11:11:56 Event: ULOG_SUBMIT for Job HelloWorld (16.0.0)
7/10 11:11:56 Event: ULOG_SUBMIT for Job Setup (17.0.0)
7/10 11:11:56 0/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:12:16 Event: ULOG_GLOBUS_SUBMIT for Job HelloWorld (16.0.0)
7/10 11:12:16 Event: ULOG_EXECUTE for Job HelloWorld (16.0.0)
7/10 11:12:16 Event: ULOG_GLOBUS_SUBMIT for Job Setup (17.0.0)
7/10 11:12:16 Event: ULOG_EXECUTE for Job Setup (17.0.0)
7/10 11:12:21 Event: ULOG_JOB_TERMINATED for Job HelloWorld (16.0.0)
7/10 11:12:21 Job HelloWorld completed successfully.
7/10 11:12:21 1/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:12:31 Event: ULOG_JOB_TERMINATED for Job Setup (17.0.0)
7/10 11:12:31 Job Setup completed successfully.
7/10 11:12:31 Submitting Job WorkerNode_1 ...
7/10 11:12:32    assigned Condor ID (18.0.0)
7/10 11:12:32 Submitting Job WorkerNode_Two ...
7/10 11:12:32    assigned Condor ID (19.0.0)
7/10 11:12:32 Event: ULOG_SUBMIT for Job WorkerNode_1 (18.0.0)
7/10 11:12:32 Event: ULOG_SUBMIT for Job WorkerNode_Two (19.0.0)
7/10 11:12:32 2/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:12:47 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_Two (19.0.0)
7/10 11:12:47 Event: ULOG_EXECUTE for Job WorkerNode_Two (19.0.0)
7/10 11:12:47 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_1 (18.0.0)
7/10 11:12:47 Event: ULOG_EXECUTE for Job WorkerNode_1 (18.0.0)
7/10 11:13:07 Event: ULOG_JOB_TERMINATED for Job WorkerNode_1 (18.0.0)
7/10 11:13:07 Job WorkerNode_1 completed successfully.
7/10 11:13:07 3/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:13:57 Event: ULOG_JOB_TERMINATED for Job WorkerNode_Two (19.0.0)
7/10 11:13:57 Job WorkerNode_Two completed successfully.
7/10 11:13:57 Submitting Job CollectResults ...
7/10 11:13:57    assigned Condor ID (20.0.0)
7/10 11:13:57 Event: ULOG_SUBMIT for Job CollectResults (20.0.0)
7/10 11:13:57 4/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:14:12 Event: ULOG_GLOBUS_SUBMIT for Job CollectResults (20.0.0)
7/10 11:14:12 Event: ULOG_EXECUTE for Job CollectResults (20.0.0)
7/10 11:14:32 Event: ULOG_JOB_TERMINATED for Job CollectResults (20.0.0)
7/10 11:14:32 Job CollectResults completed successfully.
7/10 11:14:32 Submitting Job LastNode ...
7/10 11:14:32    assigned Condor ID (21.0.0)
7/10 11:14:32 Event: ULOG_SUBMIT for Job LastNode (21.0.0)
7/10 11:14:32 5/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:14:47 Event: ULOG_GLOBUS_SUBMIT for Job LastNode (21.0.0)
7/10 11:14:47 Event: ULOG_EXECUTE for Job LastNode (21.0.0)
7/10 11:15:02 Event: ULOG_JOB_TERMINATED for Job LastNode (21.0.0)
7/10 11:15:02 Job LastNode completed successfully.
7/10 11:15:02 6/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 11:15:02 All jobs Completed!
7/10 11:15:02 **** condor_scheduniv_exec.15.0 (condor_DAGMAN) EXITING WITH STATUS 0
&lt;/pre&gt;

&lt;p&gt; Uh oh, DAGMan ran that remaining nodes based on bad data from node
work2. Normally DAGMan checks the return code and considers non-zero a
failure.  We did modify myscript2.sh to return non-zero.  That would
normally work, but we&#39;re using Condor-G, not normal Condor.  Condor-G
relies on Globus and Globus doesn&#39;t return error codes.

&lt;p&gt;If you&#39;re interested in having DAGMan notice a failed job and stopping
the DAG at that point, you&#39;ll need to use a POST script to detect the
problem.  One solution is to wrap your executable in a script that will
output the executable&#39;s return code to stdout and have the POST script scan
the stdout for the status.  Of perhaps your executable&#39;s normal output
contains enough information to make the decision.

&lt;p&gt;In this case, our executable is emitting a well known message.  Let&#39;s
add a POST script.

&lt;p&gt;First, clean up your results.  Be careful about deleting the mydag.dag.* files,
you do not want to delete the mydag.dag file, just mydag.dag.* .

&lt;pre&gt;
$ &lt;strong&gt;rm mydag.dag.* results.*&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Now create a script to check the output.

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt; postscript_checker
#! /bin/sh
grep &#39;RESULT: 0 SUCCESS&#39; $1 &amp;gt; /dev/null 2&amp;gt;/dev/null

&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat postscript_checker&lt;/strong&gt;
#! /bin/sh
grep &#39;RESULT: 0 SUCCESS&#39; $1 &amp;gt; /dev/null 2&amp;gt;/dev/null
$ &lt;strong&gt;chmod a+x postscript_checker &lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;modify your mydag.dag to use the new script for the nodes. 

&lt;pre&gt;
$ &lt;strong&gt;cat &amp;gt;&amp;gt;mydag.dag
Script POST Setup postscript_checker results.setup.output
Script POST WorkerNode_1 postscript_checker results.work1.output
Script POST WorkerNode_Two postscript_checker results.work2.output
Script POST CollectResults postscript_checker results.workfinal.output
Script POST LastNode postscript_checker results.finalize.output

&lt;em&gt;Ctrl-D&lt;/em&gt;&lt;/strong&gt;
$ &lt;strong&gt;cat mydag.dag&lt;/strong&gt;
Job HelloWorld myjob.submit
Job Setup job.setup.submit
Job WorkerNode_1 job.work1.submit
Job WorkerNode_Two job.work2.submit
Job CollectResults job.workfinal.submit
Job LastNode job.finalize.submit
PARENT Setup CHILD WorkerNode_1 WorkerNode_Two
PARENT WorkerNode_1 WorkerNode_Two CHILD CollectResults
PARENT CollectResults CHILD LastNode
Script POST Setup postscript_checker results.setup.output
Script POST WorkerNode_1 postscript_checker results.work1.output
Script POST WorkerNode_Two postscript_checker results.work2.output
Script POST CollectResults postscript_checker results.workfinal.output
Script POST LastNode postscript_checker results.finalize.output
$ &lt;strong&gt;ls&lt;/strong&gt;
job.finalize.submit  job.work1.submit  job.workfinal.submit  myjob.submit  myscript2.sh        watch_condor_q
job.setup.submit     job.work2.submit  mydag.dag        myscript.sh   postscript_checker
&lt;/pre&gt;

&lt;p&gt;Submit the dag again with the new POST scripts in place.

&lt;pre&gt;
$ &lt;strong&gt;condor_submit_dag mydag.dag&lt;/strong&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.condor.sub
Log of DAGMan debugging messages         : mydag.dag.dagman.out
Log of Condor library debug messages     : mydag.dag.lib.out
Log of the life of condor_dagman itself  : mydag.dag.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 22.
-----------------------------------------------------------------------
&lt;/pre&gt;

&lt;p&gt;Again, watch the job with watch_condor_q.
In separate windows run &quot;&lt;tt&gt;tail -f --lines=500 results.log&lt;/tt&gt;&quot; and &quot;&lt;tt&gt;tail -f --lines=500 mydag.dag.dagman.out&lt;/tt&gt;&quot; to monitor the job&#39;s progress.

&lt;pre&gt;
$ &lt;strong&gt;./watch_condor_q &lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  22.0   adesmet         7/10 11:25   0+00:00:03 R  0   2.6  condor_dagman -f -
  23.0   adesmet         7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh       
  24.0   adesmet         7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  23.0   adesmet       UNSUBMITTED fork     gk1   /afs/cs.wisc.edu/u
  24.0   adesmet       UNSUBMITTED fork     gk1   /afs/cs.wisc.edu/u


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  22.0   adesmet         7/10 11:25   0+00:00:03 R  0   2.6  condor_dagman -f -
  23.0    |-HelloWorld   7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh       
  24.0    |-Setup        7/10 11:25   0+00:00:00 I  0   0.0  myscript.sh Setup 

3 jobs; 2 idle, 1 running, 0 held


&lt;em&gt;Output of watch_condor_q truncated&lt;/em&gt;

-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held
&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Check your results:


&lt;pre&gt;
$ &lt;strong&gt;ls&lt;/strong&gt;
job.finalize.submit   mydag.dag          mydag.dag.rescue   results.error         results.work1.error
job.setup.submit      mydag.dag.condor.sub  myjob.submit   results.log         results.work1.output
job.work1.submit      mydag.dag.dagman.log  myscript.sh      results.output         results.work2.error
job.work2.submit      mydag.dag.dagman.out  myscript2.sh   results.setup.error   results.work2.output
job.workfinal.submit  mydag.dag.lib.out     postscript_checker   results.setup.output  watch_condor_q
$ &lt;strong&gt;cat mydag.dag.dagman.out&lt;/strong&gt;
7/10 11:25:35 ******************************************************
7/10 11:25:35 ** condor_scheduniv_exec.22.0 (CONDOR_DAGMAN) STARTING UP
7/10 11:25:35 ** $CondorVersion: 6.5.1 Apr 22 2003 $
7/10 11:25:35 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 11:25:35 ** PID = 27251
7/10 11:25:35 ******************************************************
7/10 11:25:35 DaemonCore: Command Socket at &amp;lt;128.105.185.14:34913&amp;gt;
7/10 11:25:35 argv[0] == &quot;condor_scheduniv_exec.22.0&quot;
7/10 11:25:35 argv[1] == &quot;-Debug&quot;
7/10 11:25:35 argv[2] == &quot;3&quot;
7/10 11:25:35 argv[3] == &quot;-Lockfile&quot;
7/10 11:25:35 argv[4] == &quot;mydag.dag.lock&quot;
7/10 11:25:35 argv[5] == &quot;-Condorlog&quot;
7/10 11:25:35 argv[6] == &quot;results.log&quot;
7/10 11:25:35 argv[7] == &quot;-Dag&quot;
7/10 11:25:35 argv[8] == &quot;mydag.dag&quot;
7/10 11:25:35 argv[9] == &quot;-Rescue&quot;
7/10 11:25:35 argv[10] == &quot;mydag.dag.rescue&quot;
7/10 11:25:35 Condor log will be written to results.log
7/10 11:25:35 DAG Lockfile will be written to mydag.dag.lock
7/10 11:25:35 DAG Input file is mydag.dag
7/10 11:25:35 Rescue DAG will be written to mydag.dag.rescue
7/10 11:25:35 Parsing mydag.dag ...
7/10 11:25:35 jobName: Setup
7/10 11:25:35 jobName: WorkerNode_1
7/10 11:25:35 jobName: WorkerNode_Two
7/10 11:25:35 jobName: CollectResults
7/10 11:25:35 jobName: LastNode
7/10 11:25:35 Dag contains 6 total jobs
7/10 11:25:35 Bootstrapping...
7/10 11:25:35 Number of pre-completed jobs: 0
7/10 11:25:35 Submitting Job HelloWorld ...
7/10 11:25:35    assigned Condor ID (23.0.0)
7/10 11:25:35 Submitting Job Setup ...
7/10 11:25:35    assigned Condor ID (24.0.0)
7/10 11:25:36 Event: ULOG_SUBMIT for Job HelloWorld (23.0.0)
7/10 11:25:36 Event: ULOG_SUBMIT for Job Setup (24.0.0)
7/10 11:25:36 0/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:25:56 Event: ULOG_GLOBUS_SUBMIT for Job HelloWorld (23.0.0)
7/10 11:25:56 Event: ULOG_EXECUTE for Job HelloWorld (23.0.0)
7/10 11:25:56 Event: ULOG_GLOBUS_SUBMIT for Job Setup (24.0.0)
7/10 11:25:56 Event: ULOG_EXECUTE for Job Setup (24.0.0)
7/10 11:26:01 Event: ULOG_JOB_TERMINATED for Job HelloWorld (23.0.0)
7/10 11:26:01 Job HelloWorld completed successfully.
7/10 11:26:01 1/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:26:11 Event: ULOG_JOB_TERMINATED for Job Setup (24.0.0)
7/10 11:26:11 Job Setup completed successfully.
7/10 11:26:11 Running POST script of Job Setup...
7/10 11:26:11 1/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:26:16 Event: ULOG_POST_SCRIPT_TERMINATED for Job Setup (24.0.0)
7/10 11:26:16 POST Script of Job Setup completed successfully.
7/10 11:26:16 Submitting Job WorkerNode_1 ...
7/10 11:26:16    assigned Condor ID (25.0.0)
7/10 11:26:16 Submitting Job WorkerNode_Two ...
7/10 11:26:17    assigned Condor ID (26.0.0)
7/10 11:26:17 Event: ULOG_SUBMIT for Job WorkerNode_1 (25.0.0)
7/10 11:26:17 Event: ULOG_SUBMIT for Job WorkerNode_Two (26.0.0)
7/10 11:26:17 2/6 done, 0 failed, 2 submitted, 0 ready, 0 pre, 0 post
7/10 11:26:32 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_1 (25.0.0)
7/10 11:26:32 Event: ULOG_EXECUTE for Job WorkerNode_1 (25.0.0)
7/10 11:26:32 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_Two (26.0.0)
7/10 11:26:32 Event: ULOG_EXECUTE for Job WorkerNode_Two (26.0.0)
7/10 11:26:52 Event: ULOG_JOB_TERMINATED for Job WorkerNode_1 (25.0.0)
7/10 11:26:52 Job WorkerNode_1 completed successfully.
7/10 11:26:52 Running POST script of Job WorkerNode_1...
7/10 11:26:52 2/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 1 post
7/10 11:26:57 Event: ULOG_POST_SCRIPT_TERMINATED for Job WorkerNode_1 (25.0.0)
7/10 11:26:57 POST Script of Job WorkerNode_1 completed successfully.
7/10 11:26:57 3/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:27:42 Event: ULOG_JOB_TERMINATED for Job WorkerNode_Two (26.0.0)
7/10 11:27:42 Job WorkerNode_Two completed successfully.
7/10 11:27:42 Running POST script of Job WorkerNode_Two...
7/10 11:27:42 3/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:27:47 Event: ULOG_POST_SCRIPT_TERMINATED for Job WorkerNode_Two (26.0.0)
7/10 11:27:47 POST Script of Job WorkerNode_Two failed with status 1
7/10 11:27:47 3/6 done, 1 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 11:27:47 ERROR: the following job(s) failed:
7/10 11:27:47 ---------------------- Job ----------------------
7/10 11:27:47       Node Name: WorkerNode_Two
7/10 11:27:47          NodeID: 3
7/10 11:27:47     Node Status: STATUS_ERROR    
7/10 11:27:47           Error: POST Script failed with status 1
7/10 11:27:47 Job Submit File: job.work2.submit
7/10 11:27:47     POST Script: postscript_checker results.work2.output
7/10 11:27:47   Condor Job ID: (26.0.0)
7/10 11:27:47       Q_PARENTS: 1, &amp;lt;END&amp;gt;
7/10 11:27:47       Q_WAITING: &amp;lt;END&amp;gt;

7/10 11:27:47      Q_CHILDREN: 4, &amp;lt;END&amp;gt;
7/10 11:27:47 ---------------------------------------   &amp;lt;END&amp;gt;
7/10 11:27:47 Writing Rescue DAG file...
7/10 11:27:47 **** condor_scheduniv_exec.22.0 (condor_DAGMAN) EXITING WITH STATUS 1
&lt;/pre&gt;

&lt;p&gt;DAGMan notices that one of the jobs failed.  DAGMan ran as much of the
DAG as possible and logged enough information to continue the run when the
situation is resolved.

&lt;p&gt;Look at the rescue DAG. It&#39;s the same structurally as your original DAG,
but notes that finished are marked DONE.  (DAGMan also reorganized the
file.) When you submit the rescue DAG, DONE nodes will be skipped.

&lt;pre&gt;
$ &lt;strong&gt;cat mydag.dag.rescue &lt;/strong&gt;
# Rescue DAG file, created after running
#   the mydag.dag DAG file
#
# Total number of Nodes: 6
# Nodes premarked DONE: 3
# Nodes that failed: 1
#   WorkerNode_Two,&amp;lt;ENDLIST&amp;gt;

JOB HelloWorld myjob.submit DONE

JOB Setup job.setup.submit DONE
SCRIPT POST Setup postscript_checker results.setup.output

JOB WorkerNode_1 job.work1.submit DONE
SCRIPT POST WorkerNode_1 postscript_checker results.work1.output

JOB WorkerNode_Two job.work2.submit 
SCRIPT POST WorkerNode_Two postscript_checker results.work2.output

JOB CollectResults job.workfinal.submit 
SCRIPT POST CollectResults postscript_checker results.workfinal.output

JOB LastNode job.finalize.submit 
SCRIPT POST LastNode postscript_checker results.finalize.output


PARENT Setup CHILD WorkerNode_1 WorkerNode_Two
PARENT WorkerNode_1 CHILD CollectResults
PARENT WorkerNode_Two CHILD CollectResults
PARENT CollectResults CHILD LastNode
&lt;/pre&gt;

&lt;p&gt;So we know there is a problem with the work2 step.  Let&#39;s &quot;fix&quot; it.

&lt;pre&gt;
$ &lt;strong&gt;rm myscript2.sh&lt;/strong&gt;
$ &lt;strong&gt;cp myscript.sh myscript2.sh&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Now we can submit our rescue DAG.  (If you didn&#39;t fix the problem,
DAGMan would have generated another rescue DAG, this time
&quot;mydag.dag.rescue.rescue&quot;.)
In separate windows run &quot;&lt;tt&gt;tail -f --lines=500 results.log&lt;/tt&gt;&quot; and &quot;&lt;tt&gt;tail -f --lines=500 mydag.dag.dagman.out&lt;/tt&gt;&quot; to monitor the job&#39;s progress.


&lt;pre&gt;
$ &lt;strong&gt;condor_submit_dag mydag.dag.rescue &lt;/strong&gt;

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.rescue.condor.sub
Log of DAGMan debugging messages         : mydag.dag.rescue.dagman.out
Log of Condor library debug messages     : mydag.dag.rescue.lib.out
Log of the life of condor_dagman itself  : mydag.dag.rescue.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 27.
-----------------------------------------------------------------------
$ &lt;strong&gt;./watch_condor_q &lt;/strong&gt;


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  27.0   adesmet         7/10 11:34   0+00:00:01 R  0   2.6  condor_dagman -f -
  28.0   adesmet         7/10 11:34   0+00:00:00 I  0   0.0  myscript2.sh Worke

2 jobs; 1 idle, 1 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
  28.0   adesmet       UNSUBMITTED fork     gk1   /afs/cs.wisc.edu/u


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  27.0   adesmet         7/10 11:34   0+00:00:01 R  0   2.6  condor_dagman -f -
  28.0    |-WorkerNode_  7/10 11:34   0+00:00:00 I  0   0.0  myscript2.sh Worke

2 jobs; 1 idle, 1 running, 0 held


&lt;em&gt;Output of watch_condor_q truncated&lt;/em&gt;

-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


-- Submitter: puffin.cs.wisc.edu : &amp;lt;128.105.185.14:33785&amp;gt; : puffin.cs.wisc.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held

&lt;strong&gt;&lt;em&gt;Ctrl-C&lt;/em&gt;&lt;/strong&gt;
&lt;/pre&gt;

&lt;p&gt;Check your results.

&lt;pre&gt;
$ &lt;strong&gt;ls&lt;/strong&gt;
job.finalize.submit   mydag.dag.lib.out         myscript2.sh          results.work1.error
job.setup.submit      mydag.dag.rescue         postscript_checker       results.work1.output
job.work1.submit      mydag.dag.rescue.condor.sub  results.error       results.work2.error
job.work2.submit      mydag.dag.rescue.dagman.log  results.finalize.error   results.work2.output
job.workfinal.submit  mydag.dag.rescue.dagman.out  results.finalize.output  results.workfinal.error
mydag.dag         mydag.dag.rescue.lib.out      results.log          results.workfinal.output
mydag.dag.condor.sub  mydag.dag.rescue.lock      results.output       watch_condor_q
mydag.dag.dagman.log  myjob.submit         results.setup.error
mydag.dag.dagman.out  myscript.sh         results.setup.output
$ &lt;strong&gt;cat mydag.dag.rescue.dagman.out&lt;/strong&gt;
7/10 11:34:33 ******************************************************
7/10 11:34:33 ** condor_scheduniv_exec.27.0 (CONDOR_DAGMAN) STARTING UP
7/10 11:34:33 ** $CondorVersion: 6.5.1 Apr 22 2003 $
7/10 11:34:33 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 11:34:33 ** PID = 27317
7/10 11:34:33 ******************************************************
7/10 11:34:33 DaemonCore: Command Socket at &amp;lt;128.105.185.14:35032&amp;gt;
7/10 11:34:33 argv[0] == &quot;condor_scheduniv_exec.27.0&quot;
7/10 11:34:33 argv[1] == &quot;-Debug&quot;
7/10 11:34:33 argv[2] == &quot;3&quot;
7/10 11:34:33 argv[3] == &quot;-Lockfile&quot;
7/10 11:34:33 argv[4] == &quot;mydag.dag.rescue.lock&quot;
7/10 11:34:33 argv[5] == &quot;-Condorlog&quot;
7/10 11:34:33 argv[6] == &quot;results.log&quot;
7/10 11:34:33 argv[7] == &quot;-Dag&quot;
7/10 11:34:33 argv[8] == &quot;mydag.dag.rescue&quot;
7/10 11:34:33 argv[9] == &quot;-Rescue&quot;
7/10 11:34:33 argv[10] == &quot;mydag.dag.rescue.rescue&quot;
7/10 11:34:33 Condor log will be written to results.log
7/10 11:34:33 DAG Lockfile will be written to mydag.dag.rescue.lock
7/10 11:34:33 DAG Input file is mydag.dag.rescue
7/10 11:34:33 Rescue DAG will be written to mydag.dag.rescue.rescue
7/10 11:34:33 Parsing mydag.dag.rescue ...
7/10 11:34:33 jobName: Setup
7/10 11:34:33 jobName: WorkerNode_1
7/10 11:34:33 jobName: WorkerNode_Two
7/10 11:34:33 jobName: CollectResults
7/10 11:34:33 jobName: LastNode
7/10 11:34:33 Dag contains 6 total jobs
7/10 11:34:33 Deleting older version of results.log
7/10 11:34:33 Bootstrapping...
7/10 11:34:33 Number of pre-completed jobs: 3
7/10 11:34:33 Submitting Job WorkerNode_Two ...
7/10 11:34:33    assigned Condor ID (28.0.0)
7/10 11:34:34 Event: ULOG_SUBMIT for Job WorkerNode_Two (28.0.0)
7/10 11:34:34 3/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:34:54 Event: ULOG_GLOBUS_SUBMIT for Job WorkerNode_Two (28.0.0)
7/10 11:34:54 Event: ULOG_EXECUTE for Job WorkerNode_Two (28.0.0)
7/10 11:35:59 Event: ULOG_JOB_TERMINATED for Job WorkerNode_Two (28.0.0)
7/10 11:35:59 Job WorkerNode_Two completed successfully.
7/10 11:35:59 Running POST script of Job WorkerNode_Two...
7/10 11:35:59 3/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:36:04 Event: ULOG_POST_SCRIPT_TERMINATED for Job WorkerNode_Two (28.0.0)
7/10 11:36:04 POST Script of Job WorkerNode_Two completed successfully.
7/10 11:36:04 Submitting Job CollectResults ...
7/10 11:36:04    assigned Condor ID (29.0.0)
7/10 11:36:04 Event: ULOG_SUBMIT for Job CollectResults (29.0.0)
7/10 11:36:04 4/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:36:19 Event: ULOG_GLOBUS_SUBMIT for Job CollectResults (29.0.0)
7/10 11:36:19 Event: ULOG_EXECUTE for Job CollectResults (29.0.0)
7/10 11:36:34 Event: ULOG_JOB_TERMINATED for Job CollectResults (29.0.0)
7/10 11:36:34 Job CollectResults completed successfully.
7/10 11:36:34 Running POST script of Job CollectResults...
7/10 11:36:34 4/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:36:39 Event: ULOG_POST_SCRIPT_TERMINATED for Job CollectResults (29.0.0)
7/10 11:36:39 POST Script of Job CollectResults completed successfully.
7/10 11:36:39 Submitting Job LastNode ...
7/10 11:36:39    assigned Condor ID (30.0.0)
7/10 11:36:39 Event: ULOG_SUBMIT for Job LastNode (30.0.0)
7/10 11:36:39 5/6 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 11:36:54 Event: ULOG_GLOBUS_SUBMIT for Job LastNode (30.0.0)
7/10 11:36:54 Event: ULOG_EXECUTE for Job LastNode (30.0.0)
7/10 11:37:09 Event: ULOG_JOB_TERMINATED for Job LastNode (30.0.0)
7/10 11:37:09 Job LastNode completed successfully.
7/10 11:37:09 Running POST script of Job LastNode...
7/10 11:37:09 5/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 1 post
7/10 11:37:14 Event: ULOG_POST_SCRIPT_TERMINATED for Job LastNode (30.0.0)
7/10 11:37:14 POST Script of Job LastNode completed successfully.
7/10 11:37:14 6/6 done, 0 failed, 0 submitted, 0 ready, 0 pre, 0 post
7/10 11:37:14 All jobs Completed!
7/10 11:37:14 **** condor_scheduniv_exec.27.0 (condor_DAGMAN) EXITING WITH STATUS 0
$ &lt;strong&gt;cat results.work2.output&lt;/strong&gt;
I&#39;m process id 30478 on pc-26
Thu Jul 10 11:34:46 CDT 2003
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/23/61b50cd9b278330cac68107dd390d6/md5/5e/004f7216b8b846d548357da00985f4/data WorkerNode2 60
My name (argument 1) is WorkerNode2
My sleep duration (argument 2) is 60
Sleep of 60 seconds finished.  Exiting
RESULT: 0 SUCCESS
$ &lt;strong&gt;exit&lt;/strong&gt;

&lt;/pre&gt;


&lt;p&gt; That&#39;s it.  There is a lot more you can do with Condor-G
and DAGMan, but this basic introduction is all you need to
know to get started.  Good luck!  

&lt;/body&gt;&lt;/html&gt;
-- Main.MichaelWilde - 15 Jun 2006


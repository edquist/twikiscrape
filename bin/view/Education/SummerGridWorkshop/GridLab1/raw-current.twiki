---+Exercises for Lecture 1: Introduction to the Grid

This exercise explores some simple Grid operations to start developing a few very basic techniques for distributed computing.

We assume you&#39;re using the default &quot;bash&quot; shell set up on your logins on the laptops and lab servers. We recommend you don&#39;t change your shell.

%TOC{depth=&quot;3&quot;}%

---++Getting to know your Mac Laptop

You will be working for the week in teams of two, which have been pre-assigned and are listed on a handout titled Team List.  Please find your partner, and locate (from the handout) the laptop that you have been assigned to.

You and your partner will share the same userid on the laptop, and use the same userid to log into the Linux lab systems that we will be using.  These IDs are of the form &quot;trainingN&quot; - listed on the Class Team Roster. You&#39;ll need to work on the laptop with the same number - that&#39;s where your login has been established. These numbers are also marked on stickers on the bottom of the laptop, and on each desk in the lab.

You will be using the following tools on your laptop:

   * &quot;Terminal&quot; tool to connect to the Linux lab systems.
   * &quot;FireFox&quot; browser.  Since some things require !FireFox and don&#39;t work well with Safari or Internet Explorer, its best to stay in !FireFox and keep all your bookmarks in this one browser.
   * Occasionally, a simple text editor.

Please take a moment and learn how to start these tools from the !MacOS &quot;Dock&quot; - the tool selector at the bottom of the laptop. To create a new Terminal window from within Terminal, use File -&gt; New Shell.

A handy tip: you can use the Apple-Key (has an &quot;Apple&quot; and &quot;Propeller&quot; symbol - just to the left of the space bar) for fast cut and paste. Apple-C = &quot;copy the text selected by the cursor&quot;, Apple-V = &quot;paste text selected by the cursor&quot;, Apple-X = &quot;delete text selected by the cursor&quot;.


Now, start a &quot;Terminal&quot; window from the desktop Dock at the bottom of your screen (it may have been moved to the sides or even the top, though.). The icon for Terminal is a black screen with the characters &quot;&gt;_&quot; on it.

Note the conventions for command-line dialogs that will be used throughout these exercises:

What you type is in *bold*. The system&#39;s responses are in regular weight text.

Our comments to you (which you should *not* enter) look like:  &lt;pre&gt;#  some comment here&lt;/pre&gt;

&lt;pre&gt;
laptop$ &lt;b&gt;pwd&lt;/b&gt;    # You type &quot;pwd&quot; but NOT this comment!
/home/ccttraining42   # the system&#39;s response
laptop$               # the system&#39;s command prompt
&lt;/pre&gt;

In these exercises, we&#39;ll use the prompt &quot;laptop$ &quot; for responses from the shell on the lab laptops, and &quot;gridlab$ &quot; or &quot;gridlab1$ &quot;, etc., for responses from remote shells. The actual shell prompts you&#39;ll see may be different from those we show here.

To start, practice cutting text from your this page in your web browser to run a command or set of commands: cut the &quot;pwd&quot; command from the box above and paste it into your terminal window to execute it. This is a good way to avoid making typos while entering commands from the examples.
 
---++ Connecting to the Linux gridlab hosts

You will be doing almost all the lab exercises this week on a Linux computer (&quot;host&quot;) named &quot;gridlab1&quot; (its &quot;fully qualified host name&quot; is &quot;gridlab1.phys.utb.edu&quot;).  From this machine, we will run Grid jobs and explore various Grid sites.

To access this machine, from your laptop Terminal window, we will use the &quot;secure shell&quot; utility, ssh, to do a &quot;remote login&quot; from your laptop to the gridlab1 Linux server:

&lt;pre&gt;
laptop$ &lt;b&gt;ssh train42@gridlab1.phys.utb.edu&lt;/b&gt;
The authenticity of host &#39;gridlab1.phys.utb.edu (206.76.233.104)&#39; can&#39;t be established.
RSA key fingerprint is 36:74:78:a8:ed:6b:38:96:63:20:01:df:46:9b:59:3b.
Are you sure you want to continue connecting (yes/no)? &lt;b&gt;yes&lt;/b&gt;
Warning: Permanently added &#39;gridblab1.phys.utb.edu,206.76.233.104&#39; (RSA) to the list of known hosts.
train42@grodlab1.phys.utb.edu&#39;s password: &lt;b&gt;summergrid&lt;/b&gt; # not echoed !!!
gridlab1$     # Now you&#39;re talking to a shell on the gridlab1 server
&lt;/pre&gt;

After the first time you do this, you won&#39;t get the &quot;Are you sure.&quot; prompt. Some of you will never see this, as your computers were used for testing this material, and the &quot;yes&quot; reply was already supplied by a tester.  So it will look like:

&lt;pre&gt;
laptop$ &lt;b&gt;ssh train42@gridlab1.phys.utb.edu&lt;/b&gt;
Password: &lt;b&gt;summergrid&lt;/b&gt; # enter trainingN or NN here, same as your laptop name
gridlab1$
&lt;/pre&gt;

You should be able to reach lab hosts gridlab2, 3, and 4 in the same manner.

For the rest of this exercise, we will use simpler prompts: &lt;b&gt;$laptop&lt;/b&gt; for commands typed directly to the shell of your laptop Terminal window, and &lt;b&gt;gridlab1$&lt;/b&gt; for the gridlab1 lab server.

---++ &quot;Logging in&quot; to the Grid

Now, create a &quot;Grid security proxy&quot; for this tutorial.  A proxy is
like a temporary ticket to use the Grid - in this case, for the next
12 hours.  (Grid proxies will be explained in Lecture 2).

The Grid pass phrase for our workshop is &lt;b&gt;summergrid&lt;/b&gt;

&lt;pre&gt;
gridlab1$ &lt;b&gt;grid-proxy-init&lt;/b&gt;

Your identity: /C=US/O=SDSC/OU=SDSC/CN=Account Train35/UID=train35
Enter GRID pass phrase for this identity:&lt;b&gt;summergrid&lt;/b&gt; &lt;i&gt;# this will not echo! &lt;/i&gt;
Creating proxy ............................................. Done
Your proxy is valid until: Mon Jun 26 04:24:20 2006
gridlab1$

&lt;/pre&gt;
 
---++ Running jobs with Globus Commands

Now, if everything is set correctly, you should be able to run &quot;Grid
jobs&quot; on the hosts in the lab Grid. And of course, one&#39;s first Grid job must always be this:

&lt;pre&gt;
gridlab1$ &lt;b&gt;globus-job-run gridlab2/jobmanager-fork /bin/echo Hello World&lt;/b&gt;
Hello World
gridlab1$
&lt;/pre&gt;

You&#39;ve just submitted a &quot;job&quot; (the Linux command &quot;echo&quot;) to the
GRAM gatekeeper on gridlab2, from the &quot;submit host&quot; gridlab1! Trivial,
perhaps, but a building block to more powerful capabilities.
If this doesn&#39;t work, (or, even if it does, because the following is a handy diagnostic test to know) check that the &quot;gatekeeper&quot; - the Grid component that accepts and executes remote jobs - can correctly authenticate you:

&lt;pre&gt;
gridlab1$ &lt;b&gt;globusrun -a -r gridlab2/jobmanager-fork&lt;/b&gt;
GRAM Authentication test successful
gridlab1$
&lt;/pre&gt;

If you type &lt;pre&gt;&lt;b&gt;grid-proxy-destroy&lt;/b&gt;&lt;/pre&gt; and then do the authentication test, you will see a long chain of errors.  Do another &lt;pre&gt;&lt;b&gt;grid-proxy-init&lt;/b&gt;&lt;/pre&gt;, exactly as before, and you should be able to correctly authenticate again.

Now, back to &quot;globus-job-run&quot;. This utility runs commands on remote sites,
but it expects them to be &quot;fully qualified&quot; path names (i.e., they
must start with a &quot;/&quot;). Lets say we want to run the Linux command
&quot;hostname&quot; on the remote site to verify that we&#39;re talking to the
resource we think we are.

First, run it locally to make sure you are invoking it correctly.
Then use the command &quot;which&quot; to find out what fully-qualified path
name your shell located this command in (i.e., which system-supplied
directory of executable commands was it found in)?

&lt;pre&gt;
gridlab1$ &lt;b&gt;hostname&lt;/b&gt;
gridlab1.phys.utb.edu
gridlab1$ &lt;b&gt;which hostname&lt;/b&gt;
/bin/hostname
gridlab1$
&lt;/pre&gt;

This tells you that to run the &quot;hostname&quot; command via globus-job-run, use /bin/hostname.  Try this for the commands &lt;b&gt;id&lt;/b&gt;, &lt;b&gt;env&lt;/b&gt;, &lt;b&gt;ps&lt;/b&gt; and &lt;b&gt;uptime&lt;/b&gt;.  Now run hostname remotely, on gridlab2, 3, and 4, to verify that you really are reaching a remote system.

Next, see what else can you learn about the remote system with this approach.  Find what user ID your job ran under (&quot;id&quot; command).  Find out what environment variables are set (&quot;env&quot; command),
what the load on the remote Grid server is (&quot;uptime&quot;), and what the default working directory your remote job will run in (&quot;pwd&quot; command). Do an &quot;ls&quot; of this working directory.  Use &quot;df&quot; to find out how much space is there, and how much space exists in the remote &quot;/tmp&quot; directory.  Can you create a file on the remote system?  Can you remove it?

---++ Running under a remote shell

Fully qualified pathnames are necessary when running commands under globus-job-run, because by default it does not start a UNIX &quot;shell&quot; on the remote system and its the shell that implements mechanisms like searching for commands in your $PATH variable, and many other features, like input/output redirection ( e.g. &gt;foo), pipes (eg cmd1|cmd2) and $VAR substitution. But we can tell globus-job-run to run a shell for you on the remote Grid site, and pass the command string to that remote shell:

Try for example:

&lt;pre&gt;
gridlab1$ &lt;b&gt;globus-job-run gridlab2/jobmanager-fork /bin/sh -c &quot;grep train /etc/group | wc -l&quot;&lt;/b&gt;
&lt;/pre&gt;

Experiment for a few minutes with this to try a few shell commands and pipelines on gridlab2, gridlab3 and gridlab4. How similar are these machines?

Common Linux system commands are typically, but not always, found in
the same directory on all Linux systems, and this week we&#39;ll be using
Grid systems with exclusively Linux-based hosts. In real work,
organizations establish conventions for such things, which will be touched on when we discuss production Grids.

---++ Immediate (fork) and Batch (scheduled) Job managers

Notice that we follow the system name that you want to run the command on with the qualifier &quot;&lt;b&gt;/jobmanager-fork&lt;/b&gt;&quot;. GRAM, the Globus protocol for running remote jobs, supports the concept of a &quot;job manager&quot; as an adapter to local job management environments.  Each &quot;Grid site&quot; - or collection of resources - can support one or more such job managers. The &quot;fork&quot; job manager runs an immediate job through the UNIX fork() interface. Another job manager we have installed on the gridlab hosts, &quot;jobmanager-condor&quot; acts as an interface to the Condor batch scheduling system.

Which do you think will be faster?  Lets find out: use the command &quot;time&quot; to test which jobmanager is faster.

To time a command, just say &quot;time commandname&quot; as in:

&lt;pre&gt;
gridlab1$ &lt;b&gt;time sleep 3&lt;/b&gt;
&lt;/pre&gt;

Use this to time a few trvial Grid jobs to compare jobmanager-fork and jobmanager-condor.

Assuming you&#39;ve done this, we&#39;ll now explain the basic rule here: the &quot;fork&quot; job manager is very fast - it has rather low &quot;scheduling latency&quot;.  It runs trivial commands very quickly.  But it also has no compute power - its usually just a single CPU on a cluster-controlling computer called the &quot;gatekeeper&quot; or &quot;headnode&quot;.  A batch job manager, on the other hand, has a higher scheduling overhead.  But it gives you access to all computers in a cluster, and the opportunity to do real parallel computing.

It turns out that each of our gridlab machines is just an ordinary single-processing machine. We&#39;ve just &quot;tricked&quot; Condor onto thinking that it had a real &quot;cluster&quot; there with a set of CPUs. (We configured each of the gridlab hosts with 16 such virtual CPUS).

While our four gridlab hosts use the &quot;Condor&quot; scheduler, and thus &quot;jobmanager-condor&quot;, other systems use other schedulers, such as &quot;PBS&quot; - the Portable Batch System, and for those systems you would use &quot;jobmanager-pbs&quot;.  You&#39;ll get a chance to try this later in this lab.

Now try running a few jobs at a grid site in California, at ISI, which does have a real cluster:

&lt;pre&gt;
gridlab1$ &lt;b&gt;globus-job-run skynet-login.isi.edu:/jobmanager-pbs /bin/hostname&lt;/b&gt;
&lt;/pre&gt;

Next try starting 5 such jobs on the skynet cluster at once: put an &quot;&amp;&quot; at the end of the line, and either use cut-and-past, or shell command history, or a simple shell script to run 5 of these commands at once.  Now can you see multiple hostnames being used?  Are any of these the skynet-login &quot;gatekeeper&quot; hostname?

---++ Running a simple application remotely, through &quot;staging&quot;

Now we&#39;ll create a simple application - in this case a shell script - and send the application itself to remote Grid sites. We&#39;ll call this application &quot;genr&quot;  - a simple script to generate fixed-width pseudo-random numbers.

&lt;verbatim&gt;
#! /bin/sh
NLINES=$1
NDIGITS=$2
awk &quot;BEGIN {
  for (i = 0 ; i &lt; $NLINES; i++ ) {
    print int(rand() * 10^$NDIGITS )
  }
}&quot;
&lt;/verbatim&gt;

Using copy/paste, copy this script above into a new file named &quot;genr&quot; in your home directory on gridlab1, and use:

&lt;pre&gt;
$ &lt;b&gt;chmod +x genr&lt;/b&gt;
&lt;/pre&gt;

to make it executable. Test it locally first.  Its usage is: genr nlines ndigits. Your genr script should behave like this:

&lt;pre&gt;
$ &lt;b&gt;./genr 5 8&lt;/b&gt; # 5 lines of 8 digits
23778751
29106573
84581385
15220829
58553734
$ 
&lt;/pre&gt;

Then test &lt;b&gt;genr&lt;/b&gt; under globus-job-run, to make sure it runs locally through the Globus GRAM interface:

&lt;pre&gt;
gridlab1$ &lt;b&gt;globus-job-run gridlab1/jobmanager-fork /home/train35/genr 5 4 &lt;/b&gt;
2377
2910
8458
1522
5855
gridlab1$
&lt;/pre&gt;

Now, try executing genr on a remote Grid site (here, gridlab3, from gridlab1):

&lt;pre&gt;
gridlab1$ &lt;b&gt;globus-job-run gridlab3/jobmanager-fork /home/train35/genr 5 4&lt;/b&gt;
&lt;/pre&gt;

What happens? Why?

Now, add the &quot;-s&quot; or &quot;stage&quot; option to the request, to automatically copy this mini &quot;application&quot; to the remote Grid site so that it can be executed there:

&lt;pre&gt;
gridlab1$ &lt;b&gt;globus-job-run gridlab1/jobmanager-fork -s /home/train35/genr 5 4 &lt;/b&gt;
&lt;/pre&gt;

Exercise: can you run the same application at ISI in California?  Did it really stage, or did the application already exist out there?  How can you be sure?

---++ Run a &quot;Scientific Application&quot; across several Grid sites

Here are several sites that you can access using your Grid certificate:

&lt;pre&gt;

Site Name              Gatekeeper                Sched     CPUS GridFTP

LAB1                   gridlab1.phys.utb.edu     condor    16   gridlab1.phys.utb.edu
LAB2                   gridlab1.phys.utb.edu     condor    16   gridlab2.phys.utb.edu
LAB3                   gridlab1.phys.utb.edu     condor    16   gridlab3.phys.utb.edu
LAB4                   gridlab1.phys.utb.edu     condor    16   gridlab4.phys.utb.edu
ISI                    skynet-login.isi.edu      pbs       90   skynet-login.isi.edu
ANL-UC                 tg-grid.uc.teragrid.org    pbs      32   tg-grid.uc.teragird.org    
NCSA                   tg-login.ncsa.teragrid.org pbs      512  tg-login.ncsa.teragrid.org
SDSC                   tg-login.sdsc.teragrid.org pbs      512  tg-login.sdsc.teragrid.org

&lt;/pre&gt;

Try a few of these using the simple examples you learned above. Write a simple test script of globus-job-run commands that checks if all these sites are working for you.

Finally: lets pretend that &quot;genr&quot; is a compute intensive simulation program that you want to run on the Grid.  Your last task in this lab is to create 5 lists of 10 random numbers each: 1-digit numbers in the first file, 2-digit numbers in the second file, and so on.  Since this work is &quot;compute intensive&quot;, you want to run these jobs under the &quot;batch&quot; job manager at each site, *not* the fork job manager.  So you want to use either jobmanager-pbs or jobmanager-condor, according to the table above.  And, of course, you want to do it all in parallel, in a script so that you can reproduce your results as needed.


---+ Operations Meeting August 4th, 2008

   * 12:00pm Central 
   * Phone Number 510-665-5437 Meeting ID 786999 

Attending: Kyle, Tim, Soichi, Daniel, Rob, Burt, Mats, Robert, Steve, Fred, Ian, Doug, Karthik, Arvind, Tom, Wayne, Sheryas, Anand

---+ Operations Agenda

---++ Followup items from the [[http://twiki.grid.iu.edu/twiki/bin/view/Operations/MinutesJuly28][previous meeting]]
   * OSG 1.0.0 installations (34 resources at 1.0.0, NERSC-Franklin came up last week on 1.0.0, one last ATLAS site UTA updating this week.) 
   * CEmon resources not reporting after restart - Some users are still reporting to the old DNS. (These have been updated, and the sync has been stopped. Tickets are closed, this can be considered complete.) 
      * Tickets have been sent out the resources that require a restart. 

---++ Hot Topics for the week
   * BDII outage (The GOC is looking at alternative versions to bring up. We hope to have a new version up and ready for testing by the end of the month. We will then do load testing, failover and such from that point.) 

---++ OSG RA Items

---++ Pending Registrations

Burt announced that CMS is hiring for a Tier 3 support person http://tinyurl.com/unljob.

| Name | Support Center | !ResURL | Site_Verify Status | Other Info |
| NERSC-Franklin | NERSC | | | ~9600 dual core processors, from registration: The NERSC Cray XT4 system, named Franklin, is a massively parallel processing (MPP) system with 9,660 compute nodes. Each node has dual processor cores, and the entire system has a total of 19,320 processor cores available for scientific applications. The system is named in honor of Benjamin Franklin. Each of Franklin&#39;s compute nodes consists of a 2.6 GHz dual-core AMD Opteron processor with a theoretical peak performance of 5.2 GFlop/sec. Each compute node has 4 GBytes of memory, and each service node (e.g. login node) has 8 GBytes of memory. The full system consists of 102 cabinets with 39 TBytes of aggregate memory. The theoretical peak performance of Franklin is about 101.5 TFlop/sec. Each compute node is connected to a dedicated SeaStar2 router through Hypertransport with a 3D torus topology which ensures high performance, low-latency communication for MPI and SHMEM jobs. Franklin uses two different operating systems. Full-featured SuSE Linux is run on service nodes (16 login nodes and other I/O, network and system nodes). A light weight OS based on Linux, Compute Node Linux (CNL), is run on each compute node. CNL reduces system overhead, and is critical for the system to scale to very large concurrency. The Parallel file system on Franklin is provided by Lustre with approximately 350 TBytes of user disk space. |
| FNAL_FCDFOSG_1 | Fermilab | | | Only accessible from inside FNAL. |

---++ Resources Removed from OSG
 None.

---++ GOC Tickets

---+++ Relevant URLs
   * [[http://www.opensciencegrid.org/tickets][Open GOC Tickets]] | [[http://www.grid.iu.edu/reports/ticketreports.html][OSG Operations Metrics]] 
---+++ Tickets to discuss on call:

&lt;table width=&quot;455&quot; cellspacing=&quot;1&quot; cellpadding=&quot;0&quot; height=&quot;33&quot; border=&quot;1&quot;&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;GOC ticket&lt;/td&gt;&lt;td&gt;Support Center&lt;/td&gt;&lt;td&gt;Action/Summary&lt;/td&gt;&lt;td&gt;Last Action Date&lt;/td&gt;&lt;td&gt;GOC Assignee&lt;/td&gt;&lt;td&gt;Notes&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;
---+++ Other issues
 1/6 of jobs result in wall failed state in Gratia. Can we check CPU failure as well as wall failure?

---++ WLCG/OSG/EGEE Issues

---++ Open Technical Discussion of the Operations Group Developments &amp; Plans

-- Main.ElizabethChism - 31 Jul 2008


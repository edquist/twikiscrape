---+++ Root Cause Analysis of BDII failure on 9/Nov/2012

---++++ Timeline
At approximately 12:50 (UTC) one instance (is2, hosted in Indianapolis) of BDII became unavailable.
The GOC became aware of the problem via automated notification systems at 13:03. A request was 
sent to the Indiana University DNS administrators to remove that instance from DNS round robin.
The change was made at 14:00 and the problem became invisible to users. At 14:35 a manual reset
was performed and the machine became reachable via KVM. It was brought up in single user mode
to examine logs and determine the failure mechanism. By 15:13 system was operating normally and
the RSV status returned to OK. At 16:08 a request to return the instance to round robin was sent and
at 17:00 the service was operating normally.

---++++ Root cause
Examination of the remote logs revealed that the RAID controller on the host machine experienced
a spurious reset at 12:39:41. At this time disk I/O became impossible and processes began to fail.
The BDII service itself survived for several minutes after the reset event. At 12:46 the RSV status
for the service changed to critical and at 13:00 a text message was generated. The text arrived
3 minutes later and repairs began.


---++++ Remediation
The RAID reset is a firmware level event and no further information as to the underlying cause is available.
The GOC is not aware of any actions that can be taken to prevent future events of this type other than
replacement of the controller should it happen again.

-- Main.ScottTeige - 12 Nov 2012


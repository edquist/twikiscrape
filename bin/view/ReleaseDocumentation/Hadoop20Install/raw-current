<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> Hadoop20Install &lt; ReleaseDocumentation &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/ReleaseDocumentation/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/ReleaseDocumentation/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/ReleaseDocumentation/Hadoop20Install?_T=16 Feb 2017" type="application/x-wiki" title="edit Hadoop20Install" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/bin/view/ReleaseDocumentation/WebRss" />    
<base href="https://twiki.opensciencegrid.org/bin/view/ReleaseDocumentation/Hadoop20Install"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiPref.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twiki_edit.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern_edit.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">
#patternTopBar,
#patternTopBarContentsOuter {
	height:95px; /* top bar height; make room for header columns */
}
#twikinetTopToolBar,
#twikinetTopToolBar table {
	top:95px; /* top bar height */
	height:48px;
}
#patternClearHeaderCenter,
#patternClearHeaderLeft,
#patternClearHeaderRight {
	height:144px; /* 95 + border + 48 */
}
#twikinetLogo {
	position:absolute;
	width:100%;
	height:95px;
}
#twikinetLogo a {
	position:absolute;
	display:block;
	width:179px;
	height:65px;
	background:none;
	top:10px;
	left:340px;
}
#twikinetLogo a:link, 
#twikinetLogo a:visited {
	border:none;
}
#twikinetLogo a:hover {
	border-style:none none solid none;
	border-width:1px;
	border-color:#c4cbd6;
}
#twikinetLogo a span {
	display:none;
}
#patternOuter {
	margin-left:220px;
}
#patternLeftBar {
	width:220px;
	margin-left:-220px;
}
</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#9999FF;
	}
	.patternBookView {
		border-color:#9999FF;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">

@import url('https://twiki.opensciencegrid.org/twiki/pub/ReleaseDocumentation/Hadoop20Install/centerpageborder-rd.css');

</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternRawViewPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternWrapper"><div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain"><div id="patternClearHeaderCenter"></div>
<div id="patternMainContents"><div class="patternTop"><span class="patternHomePath"><span class="patternHomePathTitle">You are here: </span> <a href="/bin/view/Main/WebHome" class="twikiLink">TWiki    </a><span class='twikiSeparator'>&gt;</span><img src="/twiki/pub/TWiki/TWikiDocGraphics/web-bg-small.gif" border="0" alt="" width="13" height="13" style="background-color:#9999FF" />&nbsp;<a href="/bin/view/ReleaseDocumentation/WebHome">ReleaseDocumentation Web</a><span class='twikiSeparator'>&gt;</span><a href="https://twiki.opensciencegrid.org/bin/view/ReleaseDocumentation/Hadoop20Install" title='Topic revision: 43 (07 Feb 2017 - 19:00:46)'>Hadoop20Install</a> <span class='patternRevInfo'>(07 Feb 2017, <a href="/bin/view/Main/BrianBockelman" class="twikiLink">BrianBockelman</a>)</span> (raw view)</span><!-- /patternHomePath--></div><!--/patternTop--><div class="twikiContentHeader"></div><div class="patternContent"><div class="patternTopic"> <textarea name=""  rows="22    " cols="70    " readonly="readonly" style="width: 99%    " id="topic" class="twikiTextarea twikiTextareaRawView">---+!! *Hadoop 20*

%WARNING%
%RED%
WARNING! This page is for an older version of Hadoop.
For newer versions, please visit [[Documentation/Release3.InstallHadoopSE][Hadoop Release 3 Installation]]
%ENDCOLOR%

%DOC_STATUS_TABLE%
%TOC{depth=&quot;2&quot;}%

%WARNING%
%RED%
WARNING! This page is for an older version of Hadoop.
For newer versions, please visit [[Documentation/Release3.InstallHadoopSE][Hadoop Release 3 Installation]]
%ENDCOLOR%

*Purpose*: The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.

%INCLUDE{&quot;Documentation/DocumentationTeam/DocConventions&quot; section=&quot;Header&quot;}%
%INCLUDE{&quot;Documentation/DocumentationTeam/DocConventions&quot; section=&quot;CommandLine&quot;}%

---+ Preparation
---++ Introduction

[[http://hadoop.apache.org/hdfs/][Hadoop Distributed File System]] (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:

   * An [[https://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.html][SRM interface]] for grid access; 
   * !GridFTP-HDFS as transport layer; and  
   * A [[http://fuse.sourceforge.net/][FUSE interface]] for localized POSIX access.
   * [[http://hadoop.apache.org/][Apache Hadoop]]

The VDT packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs. Two YUM repositories are available: 
   * [[http://vdt.cs.wisc.edu/hadoop/stable/2.0/][Stable repository]] for wider deployments and production usage.
   * [[http://vdt.cs.wisc.edu/hadoop/testing/2.0/][Testing repository]] for limited deployments and pre-release evaluation.

The *stable YUM repository* is enabled by default through the *osg-hadoop-20 RPM*, and contains the *golden release* supported by OSG for LHC operations. 

---+++ VDT Downloads webpage

The VDT Downloads webpage is http://vdt.cs.wisc.edu/components/hadoop.html

---+++ VDT Release notes webpage

The VDT Release notes are available at http://vdt.cs.wisc.edu/hadoop/release-notes.html

---+++ Note on upgrading from Hadoop 0.19

If you already have a working Hadoop 0.19 system and would like to upgrade to 0.20, please get familiar with this document first and then proceed to follow the [[Storage.HadoopUpgrade][upgrade instructions here]].

---++ Architecture

This diagram shows the suggested topology and distribution of services at a Hadoop site. Major service components and modules which need to be deployed on the various nodes are listed. Please use this as a recommendation to prepare for the Hadoop deployment procedure at your site.

&lt;img src=&quot;%ATTACHURLPATH%/Hadoop-site-architecture.png&quot;&gt;

%NOTE% Throughout this document it will be stated which node the relevant installation instructions apply to.  It can apply to one of the following:
   * *Namenode*
   * *Datanode*
   * *Secondary Namenode*
   * *GridFTP node* (can be installed on the same machine as a Datanode)
   *  *SRM node*

---+!!Engineering Considerations

Please read the [[Storage.HadoopUnderstanding][planning document]] to understand different components of the system. 

---+!!Help!
Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email osg-storage@opensciencegrid.org and osg-hadoop@opensciencegrid.org.

---+ Installation Procedure

Main server components can be divided in 3 categories: 
   * HDFS core: Namenode, Datanode.
   * Grid extensions: [[https://sdm.lbl.gov/bestman/][BeStMan2 SRM]], [[http://dev.globus.org/wiki/GridFTP][Globus !GridFTP]], [[https://twiki.grid.iu.edu/bin/view/Accounting/ProbeInstallation][Gratia probe]], and [[http://xrootd.slac.stanford.edu/][Xrootd server plugin]] etc.
   * HDFS auxiliary: Secondary Namenode, Hadoop Balancer.

Main client components are [[http://fuse.sourceforge.net/][FUSE]] and Hadoop command line client.

---+ Initializer RPM

%NOTE% This must be done on *all nodes*

---++ Initializing the YUM Repository

Download and install the =osg-hadoop-20= RPM on *all nodes*. This will initialize the OSG YUM repository for Hadoop.

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% rpm -Uvh http://vdt.cs.wisc.edu/hadoop/osg-hadoop-20-3.el5.noarch.rpm
&lt;/pre&gt;

This initializes YUM repository configuration in =/etc/yum.repos.d/osg-hadoop.repo=. 

---++ Choosing Stable or ITB Repository

%NOTE% %RED% *For Trash/Trash/Integration Testbed (ITB) Sites:* %ENDCOLOR% 
   * By default, *Stable Repository* is enabled (=enabled=1=) in the YUM configuration. Production sites should use the default setting.
   * ITB sites doing testing can enable the *Testing Repository* to fetch pre-release packages. 

Simply set =enabled=0= in =[hadoop]= section and =enabled=1= in =[hadoop-testing]= section of =/etc/yum.repos.d/osg-hadoop.repo=.

*YUM Repository types in /etc/yum.repos.d/osg-hadoop.repo*

&lt;table&gt;
&lt;tr colspan=2&gt;
&lt;td&gt;
*Production Sites:*
&lt;pre class=&quot;file&quot;&gt;
[hadoop]
... ...
enabled=1
... ...

[hadoop-testing]
... ...
enabled=0
... ...

[hadoop-unstable]
... ...
enabled=0
... ...
&lt;/pre&gt;
&lt;/td&gt;
&lt;td&gt;
*Trash/Trash/Integration Sites:*
&lt;pre class=&quot;file&quot;&gt;
[hadoop]
... ...
enabled=0
... ...

[hadoop-testing]
... ...
enabled=1
... ...

[hadoop-unstable]
... ...
enabled=0
... ...
&lt;/pre&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---+ Installing Hadoop

%NOTE% Hadoop must be installed on the following nodes:
   * *Namenode*
   * *Datanode*
   * *Secondary Namenode*

%NOTE% On the following nodes Hadoop must also be installed but the Hadoop service itself does not need to be started:
   * *GridFTP node*
   * *SRM node*

---++ Prerequisites

%INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Prereqs&quot;}%

---++ Installation

%INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Prep&quot;}%

The only node that requires a FUSE mount is the *SRM node*.  However to install hadoop, the =hadoop-0.20-osg= rpm requires =fuse= and =fuse-libs= packages to be installed.  If you are using RHEL &gt;= 5.4 this requirement is met and they will be brought in as dependencies.  Otherwise you must find these packages for your platform or refer to [[#TroubFuseMod][Notes on Building a FUSE Module]] in the Troubleshooting section below.

To install hadoop, run:

%INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Install&quot;}%

---++ Configuration

%INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Config&quot; TOC_SHIFT=&quot;+&quot;}%

---++ Running Hadoop

%INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Running&quot; TOC_SHIFT=&quot;+&quot;}%

---++ FUSE

A FUSE mount is only required on the *SRM node* and any other node you would like to use standard POSIX-like commands on the Hadoop filesystem. If these cases don&#39;t apply you may skip to the [[#HadoopValidation][Hadoop Validation]] section.

%NOTE% Before using FUSE you may need to add the module using =modprobe= first:

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% modprobe fuse
&lt;/pre&gt;

#FuseMount
---+++ Mounting FUSE at Boot Time 

You can %INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Fuse&quot;}%

If you have troubles mounting FUSE refer to [[#TroubFuseDeb][Running FUSE in Debug Mode]] in the Troubleshooting section.

#HadoopValidation
---++ Validation

The first thing you may want to do after installing and starting your *Namenode* is to verify that the web interface works.  In your web browser go to:

&lt;pre class=&quot;file&quot;&gt;
http://%RED%namenode.hostname%ENDCOLOR%:50070/dfshealth.jsp
&lt;/pre&gt;

Get familiar with Hadoop commands.  Run hadoop with no arguments to see the list of commands.

&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% hadoop
%TWISTY{%TWISTY_OPTS_OUTPUT%}%
Usage: hadoop [--config confdir] COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  mradmin              run a Map-Reduce admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  fetchdt              fetch a delegation token from the NameNode
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  job                  manipulate MapReduce jobs
  queue                get information regarding JobQueues
  version              print the version
  jar &lt;jar&gt;            run a jar file
  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively
  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive
  oiv                  apply the offline fsimage viewer to an fsimage
  classpath            prints the class path needed to get the
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print help when invoked w/o parameters.
%ENDTWISTY%
&lt;/pre&gt;

For a list of supported filesystem commands:

&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% hadoop fs
%TWISTY{%TWISTY_OPTS_OUTPUT%}%
Usage: java FsShell
           [-ls &lt;path&gt;]
           [-lsr &lt;path&gt;]
           [-df [&lt;path&gt;]]
           [-du &lt;path&gt;]
           [-dus &lt;path&gt;]
           [-count[-q] &lt;path&gt;]
           [-mv &lt;src&gt; &lt;dst&gt;]
           [-cp &lt;src&gt; &lt;dst&gt;]
           [-rm [-skipTrash] &lt;path&gt;]
           [-rmr [-skipTrash] &lt;path&gt;]
           [-expunge]
           [-put &lt;localsrc&gt; ... &lt;dst&gt;]
           [-copyFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]
           [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]
           [-get [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]
           [-getmerge &lt;src&gt; &lt;localdst&gt; [addnl]]
           [-cat &lt;src&gt;]
           [-text &lt;src&gt;]
           [-copyToLocal [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]
           [-moveToLocal [-crc] &lt;src&gt; &lt;localdst&gt;]
           [-mkdir &lt;path&gt;]
           [-setrep [-R] [-w] &lt;rep&gt; &lt;path/file&gt;]
           [-touchz &lt;path&gt;]
           [-test -[ezd] &lt;path&gt;]
           [-stat [format] &lt;path&gt;]
           [-tail [-f] &lt;file&gt;]
           [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]
           [-chown [-R] [OWNER][:[GROUP]] PATH...]
           [-chgrp [-R] GROUP PATH...]
           [-help [cmd]]

Generic options supported are
-conf &lt;configuration file&gt;     specify an application configuration file
-D &lt;property=value&gt;            use value for given property
-fs &lt;local|namenode:port&gt;      specify a namenode
-jt &lt;local|jobtracker:port&gt;    specify a job tracker
-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster
-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.
-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]
%ENDTWISTY%
&lt;/pre&gt;

An online guide is also available at [[http://hadoop.apache.org/common/docs/current/commands_manual.html][Apache Hadoop commands manual]].
You can use Hadoop commands to perform filesystem operations with more consistency.

Example, to look into the internal hadoop namespace:

&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% hadoop fs -ls /
Found 1 items
drwxrwxr-x   - engage engage          0 2011-07-25 06:32 /engage
&lt;/pre&gt;

Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself =/mnt/hadoop= in Hadoop commands):

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% hadoop fs -chown -R engage:engage /engage
&lt;/pre&gt;

Example, compare =hadoop fs= command vs. using FUSE mount:
&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% hadoop fs -ls /engage
Found 3 items
-rw-rw-r--   2 engage engage  733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso
-rw-rw-r--   2 engage engage  215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz
-rw-rw-r--   2 engage engage    9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz

%UCL_PROMPT% ls -l /mnt/hadoop/engage
total 935855
-rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso
-rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz
-rw-rw-r-- 1 engage engage   9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz
&lt;/pre&gt;

---++ Creating VO and User filesystem areas

Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. 

%NOTE% Create (and maintain) usernames and groups with UIDs and GIDs on *all nodes*. These are maintained in basic system files such as =/etc/passwd= and =/etc/group=.

%NOTE% In the examples below It is assumed a FUSE mount is set to =/mnt/hadoop=.  As an alternative =hadoop fs= commands could have been used.

For clean HDFS operations and filesystem management:

(a) Create top-level VO subdirectories under =/mnt/hadoop=.

Example: 

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/cms
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/dzero
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/sbgrid
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/fermigrid
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/cmstest
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/osg
&lt;/pre&gt;

(b) Create individual top-level user areas, under each VO area, as needed.

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/michaelthomas
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/brianbockelman
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/douglasstrain
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana
&lt;/pre&gt;

(c) Adjust username:group ownership of each area. 

&lt;pre class=&quot;rootscreen&quot;&gt;
%UCL_PROMPT_ROOT% chown -R cms:cms /mnt/hadoop/cms
%UCL_PROMPT_ROOT% chown -R sam:sam /mnt/hadoop/dzero

%UCL_PROMPT_ROOT% chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas
&lt;/pre&gt;

---+ Installing !GridFTP

%NOTE% !GridFTP must be installed on the *GridFTP node*

---++ Prerequisites

   1. %INCLUDE{&quot;Storage/Hadoop20GridFTP&quot; section=&quot;HadoopReq&quot;}%
   1. %INCLUDE{&quot;Storage/Hadoop20GridFTP&quot; section=&quot;GumsReq&quot;}%

%INCLUDE{&quot;Storage/Hadoop20GridFTP&quot; section=&quot;Prereqs&quot;}%

---++ Installation

To install gridftp-hdfs server, run:

%INCLUDE{&quot;Storage/Hadoop20GridFTP&quot; section=&quot;Install&quot;}%

---++ Configuration

%INCLUDE{&quot;Storage/Hadoop20GridFTP&quot; section=&quot;Config&quot;}%

---++ Running !GridFTP 

%INCLUDE{&quot;Storage/Hadoop20GridFTP&quot; section=&quot;Running&quot;}%

---++ Validation

%NOTE% The commands used to verify !GridFTP below assume you have access to a node where you can first generate a valid proxy using =voms-proxy-init= or =grid-proxy-init=.  Obtaining grid credentials is beyond the scope of this document.

&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt
&lt;/pre&gt;

If you are having troubles running !GridFTP refer to [[#GridFTPStand][Starting !GridFTP in Standalone Mode]] in the Troubleshooting section.

---+ Installing !BeStMan2

%NOTE% !BeStMan2 must be installed on the *SRM node*

---++ Prerequisites
   1. Make sure FUSE is installed and [[#FuseMount][mounted]] on the *SRM node*.
   1.  A !GridFTP-HDFS server must also be installed, but this does not need to be on the same node as the !BeStMan2 server.  A larger site will prefer to have their !GridFTP and !BeStMan2 servers installed on separate hosts.
   1 In addition to the Java jdk you need the corresponding Java sun-compat package.  For example for =jdk-1.6.0= you need to install =java-1.6.0-sun-compat=.  If you installed the jdk rpm that we supplied you can just let =java-1.6.0-sun-compat= come in as a dependency.  Otherwise you need to find and manually install the correct version before continuing.  See the [[http://www.jpackage.org/installation.php][jpackage installation doc]] for more details.
   1 CA Certificates installed in =/etc/grid-security/certificates=.

%INCLUDE{&quot;Storage/Hadoop20SRM&quot; section=&quot;Prereqs&quot;}%

---++ Installation

%INCLUDE{&quot;Storage/Hadoop20SRM&quot; section=&quot;Install&quot;}%

---++ Configuration

%INCLUDE{&quot;Storage/Hadoop20SRM&quot; section=&quot;Config1&quot;}%

!BeStMan2 SRM uses the Hadoop FUSE mount to perform namespace operations, such as mkdir, rm, and ls.  As per the Hadoop install instructions, edit =/etc/sysconfig/hadoop= and run =service hadoop-firstboot start=.  It is *not* necessary (or even recommended) to start any hadoop services with =service hadoop start=.

%INCLUDE{&quot;Storage/Hadoop20SRM&quot; section=&quot;Config2&quot;}%

---++ Running !BeStMan2

%INCLUDE{&quot;Storage/Hadoop20SRM&quot; section=&quot;Running&quot;}%

---++ Validation

%NOTE% The commands used to verify !BeStMan2 below assume you have access to a node where you can first generate a valid proxy using =voms-proxy-init= or =grid-proxy-init=.  Obtaining grid credentials is beyond the scope of this document.

Check SRM server ping response:
&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% srm-ping srm://devg-1.t2.ucsd.edu:8443/srm/v2/server
srm-ping   2.2.1.3.18    Mon Dec 20 20:16:15 PST 2010
BeStMan and SRM-Clients Copyright(c) 2007-2010,
Lawrence Berkeley National Laboratory. All rights reserved.
Support at SRM@LBL.GOV and documents at http://sdm.lbl.gov/bestman
SRM-CLIENT: Connecting to serviceurl httpg://devg-1.t2.ucsd.edu:8443/srm/v2/server

SRM-PING: Mon Jul 25 06:35:16 PDT 2011  Calling SrmPing Request...
versionInfo=v2.2

Extra information (Key=Value)
backend_type=BeStMan
backend_version=2.2.2.0.13
backend_build_date=2011-06-27T21:13:48.000Z 
gsiftpTxfServers[0]=gsiftp://devg-7.t2.ucsd.edu:2811
GatewayMode=Enabled
clientDN=/DC=org/DC=doegrids/OU=People/CN=Jeffrey M. Dost 948199
gumsIDMapped=engage
&lt;/pre&gt;

Check SRM based remote directory listing:
&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% lcg-ls -l -b -D srmv2 srm://devg-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/mnt/hadoop/engage
----------   1     2     2 733669376              UNKNOWN /mnt/hadoop/engage/CentOS-5.6-x86_64-LiveCD.iso
----------   1     2     2 215387183              UNKNOWN /mnt/hadoop/engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz
----------   1     2     2 9259360              UNKNOWN /mnt/hadoop/engage/glideinWMS_v2_5_1.tgz
----------   1     2     2      45              UNKNOWN /mnt/hadoop/engage/test.txt
&lt;/pre&gt;

Check SRM copy using !GridFTP underneath:
&lt;pre class=&quot;screen&quot;&gt;
%UCL_PROMPT% lcg-cp -v -b -D srmv2 file:/home/users/jdost/test2.txt srm://devg-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/mnt/hadoop/engage/test2.txt
Using grid catalog type: UNKNOWN
Using grid catalog : (null)
VO name: Engage
Checksum type: None
Destination SE type: SRMv2
Destination SRM Request Token: put:2
Source URL: file:/home/users/jdost/test2.txt
File size: 59
Source URL for copy: file:/home/users/jdost/test2.txt
Destination URL: gsiftp://devg-7.t2.ucsd.edu:2811//mnt/hadoop/engage/test2.txt
# streams: 1
           59 bytes      0.04 KB/sec avg      0.04 KB/sec inst
Transfer took 3010 ms

%UCL_PROMPT% lcg-ls -l -b -D srmv2 srm://devg-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/mnt/hadoop/engage
----------   1     2     2 733669376              UNKNOWN /mnt/hadoop/engage/CentOS-5.6-x86_64-LiveCD.iso
----------   1     2     2 215387183              UNKNOWN /mnt/hadoop/engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz
----------   1     2     2 9259360              UNKNOWN /mnt/hadoop/engage/glideinWMS_v2_5_1.tgz
----------   1     2     2      45              UNKNOWN /mnt/hadoop/engage/test.txt
----------   1     2     2      59              UNKNOWN /mnt/hadoop/engage/test2.txt
&lt;/pre&gt;

---+ Installing Gratia Transfer Probe

%NOTE% The Gratia Transfer Probe must be installed on the *GridFTP node*

---++ Prerequisites

   1 !GridFTP is installed and working

%INCLUDE{&quot;Storage/Hadoop20Gratia&quot; section=&quot;Prereqs&quot;}%

---++ Installation

To install the Gratia Transfer Probe, run:

%INCLUDE{&quot;Storage/Hadoop20Gratia&quot; section=&quot;Install&quot;}%

---++ Configuration

%INCLUDE{&quot;Storage/Hadoop20Gratia&quot; section=&quot;Config&quot; TOC_SHIFT=&quot;+&quot;}%

---++ Validation

%INCLUDE{&quot;Storage/Hadoop20Gratia&quot; section=&quot;Validation&quot;}%

---+ Installing Hadoop Storage Probe

%NOTE% The Hadoop Storage Probe must be installed on the *Namenode*

---++ Installation

%INCLUDE{&quot;Storage/HadoopStorageReports&quot; section=&quot;ProbeInstall&quot;}%

---++ Configuration

%INCLUDE{&quot;Storage/HadoopStorageReports&quot; section=&quot;ProbeConfig&quot;}%

---+ Installing Hadoop Storage Reports (Optional)

%NOTE% The Hadoop Storage Reports may be installed on any node that has access to a local Gratia Collector 

%INCLUDE{&quot;Storage/HadoopStorageReports&quot; section=&quot;ReportIntro&quot;}%

---++ Prerequisites

%INCLUDE{&quot;Storage/HadoopStorageReports&quot; section=&quot;ReportPrereqs&quot;}%

---++ Installation

%INCLUDE{&quot;Storage/HadoopStorageReports&quot; section=&quot;ReportInstall&quot;}%

---++ Configuration

%INCLUDE{&quot;Storage/HadoopStorageReports&quot; section=&quot;ReportConfig&quot;}%

---++ Sample report

%TWISTY{%TWISTY_OPTS_OUTPUT%}%
%INCLUDE{&quot;Storage/HadoopStorageReports&quot; section=&quot;ReportSample&quot;}%
%ENDTWISTY%

---+ Troubleshooting

---++ Hadoop

To view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser:

&lt;pre class=&quot;file&quot;&gt;
http://%RED%namenode.hostname%ENDCOLOR%:50070/conf
&lt;/pre&gt;

You will see the entire configuration in XML format, for example:

%TWISTY{%TWISTY_OPTS_OUTPUT%}%
&lt;verbatim class=&quot;file&quot;&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;configuration&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.s3n.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.fs.s3native.NativeS3FileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.cache.levels&lt;/name&gt;&lt;value&gt;2&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;map.sort.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.util.QuickSort&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-site.xml--&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/data1/hadoop//scratch&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.native.lib&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.namenode.decommission.nodes.per.interval&lt;/name&gt;&lt;value&gt;5&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.https.need.client.auth&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;ipc.client.idlethreshold&lt;/name&gt;&lt;value&gt;4000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.system.dir&lt;/name&gt;&lt;value&gt;${hadoop.tmp.dir}/mapred/system&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;&lt;value&gt;755&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.tracker.persist.jobstatus.hours&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.namenode.logging.level&lt;/name&gt;&lt;value&gt;all&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.address&lt;/name&gt;&lt;value&gt;0.0.0.0:50010&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.skip.checksum.errors&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from Unknown--&gt;&lt;name&gt;fs.default.name&lt;/name&gt;&lt;value&gt;hdfs://nagios.t2.ucsd.edu:9000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.child.tmp&lt;/name&gt;&lt;value&gt;./tmp&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.har.impl.disable.cache&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.skip.reduce.max.skip.groups&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.safemode.threshold.pct&lt;/name&gt;&lt;value&gt;0.999f&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.heartbeats.in.second&lt;/name&gt;&lt;value&gt;100&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;&lt;value&gt;40&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.blockreport.initialDelay&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.jobtracker.instrumentation&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.JobTrackerMetricsInst&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.tasktracker.dns.nameserver&lt;/name&gt;&lt;value&gt;default&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;io.sort.factor&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.timeout&lt;/name&gt;&lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.max.tracker.failures&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.rpc.socket.factory.class.default&lt;/name&gt;&lt;value&gt;org.apache.hadoop.net.StandardSocketFactory&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.tracker.jobhistory.lru.cache.size&lt;/name&gt;&lt;value&gt;5&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.hdfs.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.DistributedFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.skip.map.auto.incr.proc.count&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.block.access.key.update.interval&lt;/name&gt;&lt;value&gt;600&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.job.complete.cancel.delegation.tokens&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.mapfile.bloom.size&lt;/name&gt;&lt;value&gt;1048576&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.reduce.shuffle.connect.timeout&lt;/name&gt;&lt;value&gt;180000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.safemode.extension&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-site.xml--&gt;&lt;name&gt;tasktracker.http.threads&lt;/name&gt;&lt;value&gt;50&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.shuffle.merge.percent&lt;/name&gt;&lt;value&gt;0.66&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.ftp.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.fs.ftp.FTPFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.output.compress&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-site.xml--&gt;&lt;name&gt;io.bytes.per.checksum&lt;/name&gt;&lt;value&gt;4096&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.healthChecker.script.timeout&lt;/name&gt;&lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;topology.node.switch.mapping.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.net.ScriptBasedMapping&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.https.server.keystore.resource&lt;/name&gt;&lt;value&gt;ssl-server.xml&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.reduce.slowstart.completed.maps&lt;/name&gt;&lt;value&gt;0.05&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.reduce.max.attempts&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.ramfs.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.fs.InMemoryFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.block.access.token.lifetime&lt;/name&gt;&lt;value&gt;600&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.skip.map.max.skip.records&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.name.edits.dir&lt;/name&gt;&lt;value&gt;${dfs.name.dir}&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;&lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.tracker.persist.jobstatus.dir&lt;/name&gt;&lt;value&gt;/jobtracker/jobsInfo&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-site.xml--&gt;&lt;name&gt;hadoop.log.dir&lt;/name&gt;&lt;value&gt;/var/log/hadoop&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.s3.buffer.dir&lt;/name&gt;&lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.block.size&lt;/name&gt;&lt;value&gt;134217728&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;job.end.retry.attempts&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.file.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.fs.LocalFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.output.compression.type&lt;/name&gt;&lt;value&gt;RECORD&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.local.dir.minspacestart&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.ipc.address&lt;/name&gt;&lt;value&gt;0.0.0.0:50020&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.permissions&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;topology.script.number.args&lt;/name&gt;&lt;value&gt;100&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.mapfile.bloom.error.rate&lt;/name&gt;&lt;value&gt;0.005&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.max.tracker.blacklists&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.profile.maps&lt;/name&gt;&lt;value&gt;0-2&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.https.address&lt;/name&gt;&lt;value&gt;0.0.0.0:50475&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-site.xml--&gt;&lt;name&gt;dfs.umaskmode&lt;/name&gt;&lt;value&gt;002&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.userlog.retain.hours&lt;/name&gt;&lt;value&gt;24&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.secondary.http.address&lt;/name&gt;&lt;value&gt;gratia-1:50090&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.replication.max&lt;/name&gt;&lt;value&gt;32&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.tracker.persist.jobstatus.active&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.security.authorization&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;local.cache.size&lt;/name&gt;&lt;value&gt;10737418240&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.min.split.size&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.namenode.delegation.token.renew-interval&lt;/name&gt;&lt;value&gt;86400000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-site.xml--&gt;&lt;name&gt;mapred.map.tasks&lt;/name&gt;&lt;value&gt;7919&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.child.java.opts&lt;/name&gt;&lt;value&gt;-Xmx200m&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.https.client.keystore.resource&lt;/name&gt;&lt;value&gt;ssl-client.xml&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from Unknown--&gt;&lt;name&gt;dfs.namenode.startup&lt;/name&gt;&lt;value&gt;REGULAR&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.queue.name&lt;/name&gt;&lt;value&gt;default&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.tracker.retiredjobs.cache.size&lt;/name&gt;&lt;value&gt;1000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.https.address&lt;/name&gt;&lt;value&gt;0.0.0.0:50470&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.balance.bandwidthPerSec&lt;/name&gt;&lt;value&gt;2000000000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;ipc.server.listen.queue.size&lt;/name&gt;&lt;value&gt;128&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;job.end.retry.interval&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.inmem.merge.threshold&lt;/name&gt;&lt;value&gt;1000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.skip.attempts.to.start.skipping&lt;/name&gt;&lt;value&gt;2&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;fs.checkpoint.dir&lt;/name&gt;&lt;value&gt;/var/hadoop/checkpoint-a&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-site.xml--&gt;&lt;name&gt;mapred.reduce.tasks&lt;/name&gt;&lt;value&gt;1543&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.merge.recordsBeforeProgress&lt;/name&gt;&lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.userlog.limit.kb&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;webinterface.private.actions&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.max.objects&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.shuffle.input.buffer.percent&lt;/name&gt;&lt;value&gt;0.70&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;io.sort.spill.percent&lt;/name&gt;&lt;value&gt;0.80&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.map.tasks.speculative.execution&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.util.hash.type&lt;/name&gt;&lt;value&gt;murmur&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.dns.nameserver&lt;/name&gt;&lt;value&gt;default&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;&lt;value&gt;3600000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.map.max.attempts&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.job.acl-view-job&lt;/name&gt;&lt;value&gt; &lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.tracker.handler.count&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.client.block.write.retries&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.max.reduces.per.node&lt;/name&gt;&lt;value&gt;-1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.reduce.shuffle.read.timeout&lt;/name&gt;&lt;value&gt;180000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.tasktracker.expiry.interval&lt;/name&gt;&lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.https.enable&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.jobtracker.maxtasks.per.job&lt;/name&gt;&lt;value&gt;-1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.jobtracker.job.history.block.size&lt;/name&gt;&lt;value&gt;3145728&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;keep.failed.task.files&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.failed.volumes.tolerated&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.profile.reduces&lt;/name&gt;&lt;value&gt;0-2&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;ipc.client.tcpnodelay&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.output.compression.codec&lt;/name&gt;&lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;io.map.index.skip&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;ipc.server.tcpnodelay&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.namenode.delegation.key.update-interval&lt;/name&gt;&lt;value&gt;86400000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.running.map.limit&lt;/name&gt;&lt;value&gt;-1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;jobclient.progress.monitor.poll.interval&lt;/name&gt;&lt;value&gt;1000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.default.chunk.view.size&lt;/name&gt;&lt;value&gt;32768&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.logfile.size&lt;/name&gt;&lt;value&gt;10000000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.reduce.tasks.speculative.execution&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.tasktracker.outofband.heartbeat&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.s3n.block.size&lt;/name&gt;&lt;value&gt;67108864&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;&lt;value&gt;10000000000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.security.authentication&lt;/name&gt;&lt;value&gt;simple&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;fs.checkpoint.period&lt;/name&gt;&lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.running.reduce.limit&lt;/name&gt;&lt;value&gt;-1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.reuse.jvm.num.tasks&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.web.ugi&lt;/name&gt;&lt;value&gt;webuser,webgroup&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.jobtracker.completeuserjobs.maximum&lt;/name&gt;&lt;value&gt;100&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.df.interval&lt;/name&gt;&lt;value&gt;60000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.tracker.task-controller&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.DefaultTaskController&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.data.dir&lt;/name&gt;&lt;value&gt;/data1/hadoop//data&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.s3.maxRetries&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.dns.interface&lt;/name&gt;&lt;value&gt;default&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.support.append&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.job.acl-modify-job&lt;/name&gt;&lt;value&gt; &lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.local.dir&lt;/name&gt;&lt;value&gt;${hadoop.tmp.dir}/mapred/local&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.hftp.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.HftpFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.permissions.supergroup&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.s3.sleepTimeSeconds&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.submit.replication&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.replication.min&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.har.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.fs.HarFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt;&lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.tasktracker.dns.interface&lt;/name&gt;&lt;value&gt;default&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.namenode.decommission.interval&lt;/name&gt;&lt;value&gt;30&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from Unknown--&gt;&lt;name&gt;dfs.http.address&lt;/name&gt;&lt;value&gt;nagios:50070&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-site.xml--&gt;&lt;name&gt;mapred.job.tracker&lt;/name&gt;&lt;value&gt;nagios:9000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.seqfile.sorter.recordlimit&lt;/name&gt;&lt;value&gt;1000000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.name.dir&lt;/name&gt;&lt;value&gt;${hadoop.tmp.dir}/dfs/name&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.line.input.format.linespermap&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.jobtracker.taskScheduler&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.JobQueueTaskScheduler&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.tasktracker.instrumentation&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.TaskTrackerMetricsInst&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.datanode.http.address&lt;/name&gt;&lt;value&gt;0.0.0.0:50075&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;jobclient.completion.poll.interval&lt;/name&gt;&lt;value&gt;5000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.max.maps.per.node&lt;/name&gt;&lt;value&gt;-1&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.local.dir.minspacekill&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.replication.interval&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;io.sort.record.percent&lt;/name&gt;&lt;value&gt;0.05&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.kfs.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.fs.kfs.KosmosFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.temp.dir&lt;/name&gt;&lt;value&gt;${hadoop.tmp.dir}/mapred/temp&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-site.xml--&gt;&lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;2&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.checkpoint.edits.dir&lt;/name&gt;&lt;value&gt;${fs.checkpoint.dir}&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.tasktracker.tasks.sleeptime-before-sigkill&lt;/name&gt;&lt;value&gt;5000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.reduce.input.buffer.percent&lt;/name&gt;&lt;value&gt;0.0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.tasktracker.indexcache.mb&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.job.split.metainfo.maxsize&lt;/name&gt;&lt;value&gt;10000000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.skip.reduce.auto.incr.proc.count&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;hadoop.logfile.count&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.automatic.close&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.seqfile.compress.blocksize&lt;/name&gt;&lt;value&gt;1000000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt;&lt;value&gt;/etc/hadoop-0.20/conf/hosts_exclude&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.s3.block.size&lt;/name&gt;&lt;value&gt;67108864&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.tasktracker.taskmemorymanager.monitoring-interval&lt;/name&gt;&lt;value&gt;5000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.acls.enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapreduce.jobtracker.staging.root.dir&lt;/name&gt;&lt;value&gt;${hadoop.tmp.dir}/mapred/staging&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.queue.names&lt;/name&gt;&lt;value&gt;default&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.access.time.precision&lt;/name&gt;&lt;value&gt;3600000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.hsftp.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.HsftpFileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.tracker.http.address&lt;/name&gt;&lt;value&gt;0.0.0.0:50060&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.reduce.parallel.copies&lt;/name&gt;&lt;value&gt;5&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.seqfile.lazydecompress&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.safemode.min.datanodes&lt;/name&gt;&lt;value&gt;0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;io.sort.mb&lt;/name&gt;&lt;value&gt;100&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;ipc.client.connection.maxidletime&lt;/name&gt;&lt;value&gt;10000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.compress.map.output&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.tracker.report.address&lt;/name&gt;&lt;value&gt;127.0.0.1:0&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.healthChecker.interval&lt;/name&gt;&lt;value&gt;60000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;ipc.client.kill.max&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.s3.impl&lt;/name&gt;&lt;value&gt;org.apache.hadoop.fs.s3.S3FileSystem&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.job.tracker.http.address&lt;/name&gt;&lt;value&gt;0.0.0.0:50030&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.file.buffer.size&lt;/name&gt;&lt;value&gt;4096&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.jobtracker.restart.recover&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.serializations&lt;/name&gt;&lt;value&gt;org.apache.hadoop.io.serializer.WritableSerialization&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.task.profile&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-site.xml--&gt;&lt;name&gt;dfs.datanode.handler.count&lt;/name&gt;&lt;value&gt;10&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;mapred.reduce.copy.backoff&lt;/name&gt;&lt;value&gt;300&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.replication.considerLoad&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-default.xml--&gt;&lt;name&gt;jobclient.output.filter&lt;/name&gt;&lt;value&gt;FAILED&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from hdfs-default.xml--&gt;&lt;name&gt;dfs.namenode.delegation.token.max-lifetime&lt;/name&gt;&lt;value&gt;604800000&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from mapred-site.xml--&gt;&lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;io.compression.codecs&lt;/name&gt;&lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;!--Loaded from core-default.xml--&gt;&lt;name&gt;fs.checkpoint.size&lt;/name&gt;&lt;value&gt;67108864&lt;/value&gt;&lt;/property&gt;
&lt;/configuration&gt;
&lt;/verbatim&gt; 
%ENDTWISTY%

Please refer to [[https://twiki.grid.iu.edu/bin/view/Storage/HadoopDebug][OSG Hadoop debug webpage]] and [[http://wiki.apache.org/hadoop/FAQ][Apache Hadoop FAQ webpage]] for answers to common questions/concerns

---++ FUSE

#TroubFuseMod
---+++ Notes on Building a FUSE Module
%INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Fusemod&quot;}%

#TroubFuseDeb
---+++ Running FUSE in Debug Mode

%INCLUDE{&quot;Storage/Hadoop20Installation&quot; section=&quot;Fusedebug&quot;}%

---++ !GridFTP

#GridFTPStand
---+++ Starting !GridFTP in Standalone Mode

%INCLUDE{&quot;Storage/Hadoop20GridFTP&quot; section=&quot;Standalone&quot;}%

---++ Known Issues

---+++ copyFromLocal java IOException

When trying to copy a local file into Hadoop you may come across the following java exception:

&lt;pre class=&quot;screen&quot;&gt;
%TWISTY{%TWISTY_OPTS_OUTPUT%}%
11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0]
nodes == null
11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file
&quot;/osg/ddd&quot; - Aborting...
copyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0
nodes, instead of 1
11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd :
org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only
be replicated to 0 nodes, instead of 1
        at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313)
%ENDTWISTY%
&lt;/pre&gt;

This can occur if you try to install a Datanode on a machine with less than 10GB of disk space available.  This can be changed by lowering the value of the following property in =/usr/lib/hadoop-0.20/conf/hdfs-site.xml=:

&lt;verbatim class=&quot;file&quot;&gt;
&lt;property&gt;
  &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt;
  &lt;value&gt;10000000000&lt;/value&gt;
&lt;/property&gt;
&lt;/verbatim&gt;

Hadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.

---+ References

   * [[https://twiki.grid.iu.edu/bin/view/Trash/ReleaseDocumentationHadoopInstallationHandsOn][Hadoop Hands On Tutorial]].
   * [[Storage.HadoopUpgrade][Instructions for Upgrading from Hadoop 0.19 to Hadoop 0.20]]

---++ Benchmarking

   * [[http://www.iop.org/EJ/article/1742-6596/180/1/012047/jpconf9_180_012047.pdf][Using Hadoop as a Grid Storage Element]], &lt;i&gt;Journal of Physics Conference Series, 2009&lt;/i&gt;.
   * [[http://osg-docdb.opensciencegrid.org/0009/000911/001/Hadoop.pdf][Hadoop Distributed File System for the Grid]], &lt;i&gt;IEEE Nuclear Science Symposium, 2009&lt;/i&gt;.

---+ *Comments*
%COMMENT{type=&quot;tableappend&quot;}%

&lt;!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = JeffDost

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       =  DouglasStrain
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = NehaSharma
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
--&gt;
</textarea> <div class="patternSigLine"><span class="twikiRight twikiMakeVisible" style="text-align:left;"><span class="twikiLeft patternTextareaButton patternButtonFontSelector" title="Switch to monotype or propotional font">&nbsp;</span><span class="twikiLeft patternTextareaButton patternButtonEnlarge" title="Enlarge edit box">&nbsp;</span><span class="twikiLeft patternTextareaButton patternButtonShrink" title="Shrink edit box">&nbsp;</span></span><br class="twikiClear" /></div><!-- /patternSigLine--></div><!-- /patternTopic-->
<div class="twikiContentFooter"></div></div><!-- /patternContent-->
<a name="topic-actions"></a><div class="twikinetRounded twikinetRoundedTopicActions"><div class="rCRounded"><div class="rCTR"><div class="rCTL"></div><!--/rCTL--><div class="patternTopicActions"><div class="patternTopicAction"><span class="patternActionButtons"><span class="patternButton"><a href='https://twiki.opensciencegrid.org/bin/edit/ReleaseDocumentation/Hadoop20Install?t=1487213756;nowysiwyg=1' rel='nofollow' title='Edit this topic text' accesskey='e'><span class='twikiAccessKey'>E</span>dit</a></span><span class='twikiSeparator'>&nbsp;|&nbsp;</span><span class='patternButton'><a href='/bin/attach/ReleaseDocumentation/Hadoop20Install' rel='nofollow' title='Attach an image or document to this topic' accesskey='a'><span class='twikiAccessKey'>A</span>ttach</a></span><span class='twikiSeparator'>&nbsp;|&nbsp;</span><span class='patternButton'><a href='/bin/view/ReleaseDocumentation/Hadoop20Install?cover=print;raw=on' rel='nofollow' title='Printable version of this topic' accesskey='p'><span class='twikiAccessKey'>P</span>rint version</a></span><span class='twikiSeparator'>&nbsp;|&nbsp;</span><span><span><a href='/bin/rdiff/ReleaseDocumentation/Hadoop20Install?type=history' rel='nofollow' title='View total topic history' accesskey='h'><span class='twikiAccessKey'>H</span>istory</a></span>: r43&nbsp;<a rel="nofollow" href="/bin/rdiff/ReleaseDocumentation/Hadoop20Install?rev1=43;rev2=42">&lt;</a>&nbsp;<a rel="nofollow" href="/bin/view/ReleaseDocumentation/Hadoop20Install?rev=42">r42</a>&nbsp;<a rel="nofollow" href="/bin/rdiff/ReleaseDocumentation/Hadoop20Install?rev1=42;rev2=41">&lt;</a>&nbsp;<a rel="nofollow" href="/bin/view/ReleaseDocumentation/Hadoop20Install?rev=41">r41</a>&nbsp;<a rel="nofollow" href="/bin/rdiff/ReleaseDocumentation/Hadoop20Install?rev1=41;rev2=40">&lt;</a>&nbsp;<a rel="nofollow" href="/bin/view/ReleaseDocumentation/Hadoop20Install?rev=40">r40</a>&nbsp;<a rel="nofollow" href="/bin/rdiff/ReleaseDocumentation/Hadoop20Install?rev1=40;rev2=39">&lt;</a>&nbsp;<a rel="nofollow" href="/bin/view/ReleaseDocumentation/Hadoop20Install?rev=39">r39</a></span><span class='twikiSeparator'>&nbsp;|&nbsp;</span><span><a href='/bin/oops/ReleaseDocumentation/Hadoop20Install?template=backlinksweb' rel='nofollow' title='Search the ReleaseDocumentation Web for topics that link to here' accesskey='b'><span class='twikiAccessKey'>B</span>acklinks</a></span><span class='twikiSeparator'>&nbsp;|&nbsp;</span><span><a href='https://twiki.opensciencegrid.org/bin/view/ReleaseDocumentation/Hadoop20Install' rel='nofollow' title='View topic' accesskey='v'><span class='twikiAccessKey'>V</span>iew topic</a></span><span class='twikiSeparator'>&nbsp;|&nbsp;</span><span><a href='/bin/oops/ReleaseDocumentation/Hadoop20Install?template=oopsmore&amp;param1=43&amp;param2=43' rel='nofollow' title='Delete or rename this topic; set parent topic; view and compare revisions' accesskey='m'><span class='twikiAccessKey'>M</span>ore topic actions</a></span></span></div><!--/patternTopicAction--></div><!--/patternTopicActions--></div><!--/rCTR--><div class="rCBR"><div class="rCBL"></div><!--/rCBL--></div><!--/rCBR--></div><!--/rCRounded--></div><!--/twikinetRounded--><div class="patternInfo twikiGrayText"><div class="patternRevInfo">Topic revision: r43 - 07 Feb 2017 - 19:00:46 - <a href="/bin/view/Main/BrianBockelman" class="twikiLink">BrianBockelman</a></div><!-- /patternRevInfo--></div><!-- /patternInfo-->
</div><!-- /patternMainContents-->
</div><!-- /patternMain--><div id="patternLeftBar"><div id="patternClearHeaderLeft"></div>
<div id="patternLeftBarContents">
<head>
	<style type="text/css" media="all">
	
		.float-left {
			float:	left;
		}

		.float-right {
			float:	right;
		}

		.rounded {
      		border-radius:	 		8px;
			-moz-border-radius: 	8px;
			-webkit-border-radius:	8px;		
		}
		
		.rounded-top {
      		border-radius:	 		8px 8px 0px 0px;
			-moz-border-radius: 	8px 8px 0px 0px;
			-webkit-border-radius:	8px 8px 0px 0px;		
		}
		
		.bordered {
			border:	1px solid #AAAAAA;
		}

		.bordered-bottom {
			border-bottom:	1px solid #AAAAAA;
		}

		.high-contrast {
			color:		#FF6600;
			background: #000060;		
		}

                .low-contrast {
			color:		#FF6600;
			background: #ADD8E6;		
		}

	
		.container {
display: inline-block;
			margin:		0;
			padding:	0;
		}
		
		.box {
			display:				block;
			margin:			 0;
			padding:		0;
		}

                #head-block {
                        color:               #FF6600;	
                        background:    #333380;
                }
							
		#menu {
			font-family:	"Lucida Grande", sans-serif;
			font-size:		small;
			margin:		0px 0px 10px 0px;
			padding:	0px 0px 0.3em 0px;
			background: #EAEDF2;
		}
							
		#menul {
			font-family:	"Lucida Grande", sans-serif;
			font-size:		small;
			margin:		0px 0px 10px 0px;
			padding:	0px 0px 0.3em 0px;
			background: #F4F6F9

		}		
		#head {
			margin:		0px;
			padding:	0.2em 5px 0.2em 10px;
			font-size:	normal;
			font-weight:	bold;
			color:			#FF6600;	
		}
		
		#head a {
			text-decoration:	none;
			color:			#FF6600;	
		}

		#head a:hover {
			background: 		#EAEDF2;
			text-decoration:	none;
			color:			#FFBB00;	
		}

		#head a:visited {
			text-decoration:	none;
			color:			#FF6600;	
		}

		#head a:active {
			text-decoration:	none;
			color:			#FF6600;	
		}

		#head a:link {
			text-decoration:	none;
			color:			#FF6600;	
		}

                #iteml {
			padding:	0.1em 5px 0.0em 10px;
                }

		#iteml a {
			background: 		#F4F6F9
			text-decoration:	none;
			color:			#000044;			
		}

		#iteml a:hover {
			background: 		#F4F6F9
			text-decoration:	none;
			color:			#FF6600;			
		}
		#item {
			padding: 0.1em 5px 0.0em 10px;
		}
		
		#item a {
			background: 		#EAEDF2;
			text-decoration:	none;
			color:			#000044;			
		}

		#item a:hover {
			background: 		#EAEDF2;	
			text-decoration:	none;
			color:			#FF6600;			
		}
</style>
</head>
Hello, <a href="/bin/view/Main/TWikiGuest" class="twikiLink">TWikiGuest</a>
<a href="https://twiki.grid.iu.edu/bin/view/TWiki/TWikiRegistration" target="_top"><br/>Register</a>
<p />
<p />
<!-- Google CSE Search Box Ends
<h5><a name="Common_links"></a> Common links </h5> <ul>
<li> <a href="/bin/view/Main/WebHome" class="twikiLink">OSG TWiki home</a>
</li> <li> <a href="/bin/view/Documentation/Release3/WebHome" class="twikiLink">RPM Install guides - OSG 3.0</a>
</li> <li> <a href="/bin/view/ReleaseDocumentation/WebHome" class="twikiCurrentWebHomeLink twikiLink">Pacman Install guides OSG 1.2</a>
</li> <li> <a href="/bin/view/Documentation/WebHome" class="twikiLink">Technical Documentation</a>
</li> <li> <a href="/bin/view/Documentation/SiteAdminResources" class="twikiLink">Contact Info</a> 
</li> <li> <strong><a href="/bin/view/Main/ICantFindItForm" class="twikiLink">I can't find it!</a></strong>  -->
</li></ul> 
<p />
<h5><a name="Introduction"></a> Introduction </h5> <ul>
<li> <a href="/bin/view/Documentation/Release3/WebHome" class="twikiLink">Current Release</a> 
</li> <li> <a href="/bin/view/ReleaseDocumentation/SitePlanning" class="twikiLink">Site Planning</a> 
</li></ul> 
<p />
<h5><a name="Clients"></a> Clients </h5> <ul>
<li> <a href="/bin/view/Documentation/UsingTheGrid" class="twikiLink">Using the OSG</a>
</li></ul> 
<p />
<h5><a name="Central_OSG_Services"></a> Central OSG Services </h5> <ul>
<li> <a href="/bin/view/ReleaseDocumentation/CentralInformationServices" class="twikiLink">Information services</a> 
</li> <li> <a href="/bin/view/Trash/ReleaseDocumentationSiteMonitoringServices" class="twikiLink">Site monitoring services</a> 
</li> <li> <a href="/bin/view/ReleaseDocumentation/AccountingServices" class="twikiLink">Accounting services</a> 
</li> <li> <a href="/bin/view/ReleaseDocumentation/TroubleTickets" class="twikiLink">Trouble tickets</a> 
</li></ul> 
<p />
<h5><a name="Additional_Information"></a> Additional Information </h5> <ul>
<li> <a href="/bin/view/ReleaseDocumentation/EnvironmentVariables" class="twikiLink">OSG Environment Variables</a> 
</li> <li> <a href="/bin/view/Documentation/StorageOverview" class="twikiLink">OSG Storage Models</a>
</li></ul> 
<p />
<h5><a name="Community"></a> Community </h5>
<strong><img src="/twiki/pub/ReleaseDocumentation/WebLeftBar/linkedin-favicon_v3.ico" alt="linkedin-favicon_v3.ico" width='16' height='16' /><a href="http://www.linkedin.com/groups?gid=2183827" target="_top">LinkedIn</a></strong> <br />    
<strong><img src="/twiki/pub/ReleaseDocumentation/WebLeftBar/FaceBook_32x32.png" alt="FaceBook_32x32.png" width='15' height='15' /> <a href="http://www.new.facebook.com/group.php?gid=34781824321" target="_top">Facebook</a></strong> <br />    
<strong><img src="/twiki/pub/ReleaseDocumentation/WebLeftBar/campfire-logo.jpg" alt="campfire-logo.jpg" width='25' height='20' /><span class="twikiNewLink">Chat<a href="/bin/edit/Trash/Trash/SiteCoordination/ChatCalendar?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="Chat (this topic does not yet exist; you can create it)">?</a></span></strong>    
</div><!-- /patternLeftBarContents--></div><!-- /patternLeftBar-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--></div><!-- /patternWrapper--><div id="patternTopBar"><div id="twikinetLogo"><a href="http://www.twiki.net/"><span>TWIKI.NET</span></a></div><div id="patternTopBarContents"><div class="twikiLeft"><span id="twikiLogo" class="twikiImage"><a href="http://www.opensciencegrid.org    "><img src="/twiki/pub/Main/WebHome/osg-logo.png    " border="0" alt="OpenScience Grid Website    " style="border:none;" /></a></span></div>
<div class="twikiRight">
<table cellpadding="0" cellspacing="0" border="0"><tr><td class="twikinetSearchJump twikinetSearchJumpLeft">
 <ul>
<li> <form name="jumpForm" action="/bin/view/ReleaseDocumentation/Hadoop20Install"><input id="jumpFormField" type="text" class="twikiInputField" name="topic" value="" size="16" /><input type="submit" class="twikinetJumpButton" name="submit" value="" /></form>
</li> <li> <form name="quickSearchForm" action="/bin/view/ReleaseDocumentation/WebSearch"><input type="text" class="twikiInputField" id="quickSearchBox" name="search" value="" size="18" /><input type="hidden" name="scope" value="all" /><input type="hidden" name="web" value="ReleaseDocumentation" /><input type="submit" size="5" class="twikinetSearchButton" name="submit" value="" /></form>
</li> <li> </li></ul> 
</td>
<td class="twikinetSearchJumpRight"></td>
</tr></table><br class="twikiClear" /></div></div></div><!-- /patternTopBar--><div id="twikinetTopToolBar"><div id="twikinetTopToolBarContents"><div class="twikinetWebName twikiLeft">
<table cellpadding="0" cellspacing="0" border="0"><tr><td style="vertical-align:middle;">
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="ReleaseDocumentation"></a>  <a href="/bin/view/ReleaseDocumentation/WebHome" class="twikiCurrentWebHomeLink twikiLink">ReleaseDocumentation</a> </span></h2>
</td></tr></table>
</div><!--/twikinetWebName-->
<div class="twikinetToolBar twikiRight">
<table cellpadding="0" cellspacing="0" border="0"><tr><td><a href="https://twiki.opensciencegrid.org/bin/edit/ReleaseDocumentation/Hadoop20Install?t=1487213756;nowysiwyg=1" rel="nofollow">
<span class="rCRounded"><span class="rCTR"><span class="rCTL"></span><!--/rCTL-->
<span class="twikiLinkLabel twikinetEditIcon"><span class='twikiAccessKey'>E</span>dit</span>
</span><!--/rCTR--><span class="rCBR"><span class="rCBL"></span></span><!--/rCBR--></span><!--/rCRounded-->
</a></td><td><a href="/bin/attach/ReleaseDocumentation/Hadoop20Install" rel="nofollow">
<span class="rCRounded"><span class="rCTR"><span class="rCTL"></span><!--/rCTL-->
<span class="twikiLinkLabel"><span class='twikiAccessKey'>A</span>ttach</span>
</span><!--/rCTR--><span class="rCBR"><span class="rCBL"></span></span><!--/rCBR--></span><!--/rCRounded-->
</a></td><td><a href="/bin/genpdf/ReleaseDocumentation/Hadoop20Install?pdftitle=Hadoop20Install" rel="nofollow">
<span class="rCRounded"><span class="rCTR"><span class="rCTL"></span><!--/rCTL-->
<span class="twikiLinkLabel">PDF</span>
</span><!--/rCTR--><span class="rCBR"><span class="rCBL"></span></span><!--/rCBR--></span><!--/rCRounded-->
</a></td></tr></table>
<br class="twikiClear" /></div><!--/patternToolBar--></div><!--/twikinetTopToolBarContents--></div><!--/twikinetTopToolBar--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> Hadoop20Install &lt; ReleaseDocumentation &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/ReleaseDocumentation/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/ReleaseDocumentation/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/ReleaseDocumentation/Hadoop20Install?_T=16 Feb 2017" type="application/x-wiki" title="edit Hadoop20Install" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/bin/view/ReleaseDocumentation/WebRss" />    
<base href="https://twiki.opensciencegrid.org/bin/view/ReleaseDocumentation/Hadoop20Install"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#9999FF;
	}
	.patternBookView {
		border-color:#9999FF;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">

@import url('https://twiki.opensciencegrid.org/twiki/pub/ReleaseDocumentation/Hadoop20Install/centerpageborder-rd.css');

</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--TWISTYPLUGIN_TWISTY--><style type="text/css" media="all">
@import url("https://twiki.opensciencegrid.org/twiki/pub/TWiki/TwistyContrib/twist.css");
</style>
<script type='text/javascript' src='https://twiki.opensciencegrid.org/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js'></script>
<script type="text/javascript" src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiJavascripts/twikiPref.js"></script>
<script type="text/javascript" src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TwistyContrib/twist.compressed.js"></script>
<script type="text/javascript">
// <![CDATA[
var styleText = '<style type="text/css" media="all">.twikiMakeVisible{display:inline;}.twikiMakeVisibleInline{display:inline;}.twikiMakeVisibleBlock{display:block;}.twikiMakeHidden{display:none;}</style>';
document.write(styleText);
// ]]>
</script>

<!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <h1><a name="Hadoop_20"></a>  <strong>Hadoop 20</strong> </h1>
<p />
on    
<font color="#ff0000">
WARNING! This page is for an older version of Hadoop.
For newer versions, please visit <a href="/bin/view/Documentation/Release3/InstallHadoopSE" class="twikiLink">Hadoop Release 3 Installation</a>
</font>
<p />
<p />
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Preparation"> Preparation</a> <ul>
<li> <a href="?cover=print#Introduction"> Introduction</a>
</li> <li> <a href="?cover=print#Architecture"> Architecture</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installation_Procedure"> Installation Procedure</a>
</li> <li> <a href="?cover=print#Initializer_RPM"> Initializer RPM</a> <ul>
<li> <a href="?cover=print#Initializing_the_YUM_Repository"> Initializing the YUM Repository</a>
</li> <li> <a href="?cover=print#Choosing_Stable_or_ITB_Repositor"> Choosing Stable or ITB Repository</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installing_Hadoop"> Installing Hadoop</a> <ul>
<li> <a href="?cover=print#Prerequisites"> Prerequisites</a>
</li> <li> <a href="?cover=print#Installation"> Installation</a>
</li> <li> <a href="?cover=print#Configuration"> Configuration</a>
</li> <li> <a href="?cover=print#Running_Hadoop"> Running Hadoop</a>
</li> <li> <a href="?cover=print#FUSE"> FUSE</a>
</li> <li> <a href="?cover=print#Validation"> Validation</a>
</li> <li> <a href="?cover=print#Creating_VO_and_User_filesystem"> Creating VO and User filesystem areas</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installing_GridFTP"> Installing GridFTP</a> <ul>
<li> <a href="?cover=print#Prerequisites_AN1"> Prerequisites</a>
</li> <li> <a href="?cover=print#Installation_AN1"> Installation</a>
</li> <li> <a href="?cover=print#Configuration_AN1"> Configuration</a>
</li> <li> <a href="?cover=print#Running_GridFTP"> Running GridFTP </a>
</li> <li> <a href="?cover=print#Validation_AN1"> Validation</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installing_BeStMan2"> Installing BeStMan2</a> <ul>
<li> <a href="?cover=print#Prerequisites_AN2"> Prerequisites</a>
</li> <li> <a href="?cover=print#Installation_AN2"> Installation</a>
</li> <li> <a href="?cover=print#Configuration_AN2"> Configuration</a>
</li> <li> <a href="?cover=print#Running_BeStMan2"> Running BeStMan2</a>
</li> <li> <a href="?cover=print#Validation_AN2"> Validation</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installing_Gratia_Transfer_Probe"> Installing Gratia Transfer Probe</a> <ul>
<li> <a href="?cover=print#Prerequisites_AN3"> Prerequisites</a>
</li> <li> <a href="?cover=print#Installation_AN3"> Installation</a>
</li> <li> <a href="?cover=print#Configuration_AN3"> Configuration</a>
</li> <li> <a href="?cover=print#Validation_AN3"> Validation</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installing_Hadoop_Storage_Probe"> Installing Hadoop Storage Probe</a> <ul>
<li> <a href="?cover=print#Installation_AN4"> Installation</a>
</li> <li> <a href="?cover=print#Configuration_AN4"> Configuration</a>
</li></ul> 
</li> <li> <a href="?cover=print#Installing_Hadoop_Storage_Report"> Installing Hadoop Storage Reports (Optional)</a> <ul>
<li> <a href="?cover=print#Prerequisites_AN4"> Prerequisites</a>
</li> <li> <a href="?cover=print#Installation_AN5"> Installation</a>
</li> <li> <a href="?cover=print#Configuration_AN5"> Configuration</a>
</li> <li> <a href="?cover=print#Sample_report"> Sample report</a>
</li></ul> 
</li> <li> <a href="?cover=print#Troubleshooting"> Troubleshooting</a> <ul>
<li> <a href="?cover=print#Hadoop"> Hadoop</a>
</li> <li> <a href="?cover=print#FUSE_AN1"> FUSE</a>
</li> <li> <a href="?cover=print#GridFTP"> GridFTP</a>
</li> <li> <a href="?cover=print#Known_Issues"> Known Issues</a>
</li></ul> 
</li> <li> <a href="?cover=print#References"> References</a> <ul>
<li> <a href="?cover=print#Benchmarking"> Benchmarking</a>
</li></ul> 
</li> <li> <a href="?cover=print#Comments"> Comments</a>
</li></ul> 
</div>
<p />
on    
<font color="#ff0000">
WARNING! This page is for an older version of Hadoop.
For newer versions, please visit <a href="/bin/view/Documentation/Release3/InstallHadoopSE" class="twikiLink">Hadoop Release 3 Installation</a>
</font>
<p />
<strong>Purpose</strong>: The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.
<p />
<p />
Conventions used in this document:
<p />
<p />
<font color="#808080">A <i>User Command Line</i> is illustrated by a green box that displays a prompt:</font>
<p />
<pre class="screen">
  [user@client ~]$
</pre>
<p />
<font color="#808080">A <i>Root Command Line</i> is illustrated by a red box that displays the <em>root</em> prompt:</font>
<p />
<pre class="rootscreen">
  [root@client ~]$
</pre>
<p />
<font color="#808080"><i>Lines in a file</i> are illustrated by a yellow box that displays the desired lines in a file:</font>
<pre class="file">
priorities=1
</pre>
<p />
<p />
<h1><a name="Preparation"></a> Preparation </h1>
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Introduction"></a> Introduction </span></h2>
<p />
<a href="http://hadoop.apache.org/hdfs/" target="_top">Hadoop Distributed File System</a> (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:
<p /> <ul>
<li> An <a href="https://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.html" target="_top">SRM interface</a> for grid access; 
</li> <li> GridFTP-HDFS as transport layer; and  
</li> <li> A <a href="http://fuse.sourceforge.net/" target="_top">FUSE interface</a> for localized POSIX access.
</li> <li> <a href="http://hadoop.apache.org/" target="_top">Apache Hadoop</a>
</li></ul> 
<p />
The VDT packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs. Two YUM repositories are available:  <ul>
<li> <a href="http://vdt.cs.wisc.edu/hadoop/stable/2.0/" target="_top">Stable repository</a> for wider deployments and production usage.
</li> <li> <a href="http://vdt.cs.wisc.edu/hadoop/testing/2.0/" target="_top">Testing repository</a> for limited deployments and pre-release evaluation.
</li></ul> 
<p />
The <strong>stable YUM repository</strong> is enabled by default through the <strong>osg-hadoop-20 RPM</strong>, and contains the <strong>golden release</strong> supported by OSG for LHC operations. 
<p />
<h3><a name="VDT_Downloads_webpage"></a> VDT Downloads webpage </h3>
<p />
The VDT Downloads webpage is <a href="http://vdt.cs.wisc.edu/components/hadoop.html" target="_top">http://vdt.cs.wisc.edu/components/hadoop.html</a>
<p />
<h3><a name="VDT_Release_notes_webpage"></a> VDT Release notes webpage </h3>
<p />
The VDT Release notes are available at <a href="http://vdt.cs.wisc.edu/hadoop/release-notes.html" target="_top">http://vdt.cs.wisc.edu/hadoop/release-notes.html</a>
<p />
<h3><a name="Note_on_upgrading_from_Hadoop_0"></a> Note on upgrading from Hadoop 0.19 </h3>
<p />
If you already have a working Hadoop 0.19 system and would like to upgrade to 0.20, please get familiar with this document first and then proceed to follow the <a href="/bin/view/Storage/HadoopUpgrade" class="twikiLink">upgrade instructions here</a>.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Architecture"></a> Architecture </span></h2>
<p />
This diagram shows the suggested topology and distribution of services at a Hadoop site. Major service components and modules which need to be deployed on the various nodes are listed. Please use this as a recommendation to prepare for the Hadoop deployment procedure at your site.
<p />
<img src="/twiki/pub/ReleaseDocumentation/Hadoop20Install/Hadoop-site-architecture.png">
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  Throughout this document it will be stated which node the relevant installation instructions apply to.  It can apply to one of the following:
</dd>  </dl><ul>
<li> <strong>Namenode</strong>
</li> <li> <strong>Datanode</strong>
</li> <li> <strong>Secondary Namenode</strong>
</li> <li> <strong>GridFTP node</strong> (can be installed on the same machine as a Datanode)
</li> <li>  <strong>SRM node</strong>
</li></ul> 
<p />
<h1><a name="Engineering_Considerations"></a> Engineering Considerations </h1>
<p />
Please read the <a href="/bin/view/Storage/HadoopUnderstanding" class="twikiLink">planning document</a> to understand different components of the system. 
<p />
<h1><a name="Help"></a> Help! </h1>
Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email <a href="mailto&#58;osg&#45;storage&#64;opensciencegrid&#46;org">osg-storage&#64;opensciencegrid.org</a> and <a href="mailto&#58;osg&#45;hadoop&#64;opensciencegrid&#46;org&#46;">osg-hadoop&#64;opensciencegrid.org.</a>
<p />
<h1><a name="Installation_Procedure"></a> Installation Procedure </h1>
<p />
Main server components can be divided in 3 categories:  <ul>
<li> HDFS core: Namenode, Datanode.
</li> <li> Grid extensions: <a href="https://sdm.lbl.gov/bestman/" target="_top">BeStMan2 SRM</a>, <a href="http://dev.globus.org/wiki/GridFTP" target="_top">Globus GridFTP</a>, <a href="https://twiki.grid.iu.edu/bin/view/Accounting/ProbeInstallation" target="_top">Gratia probe</a>, and <a href="http://xrootd.slac.stanford.edu/" target="_top">Xrootd server plugin</a> etc.
</li> <li> HDFS auxiliary: Secondary Namenode, Hadoop Balancer.
</li></ul> 
<p />
Main client components are <a href="http://fuse.sourceforge.net/" target="_top">FUSE</a> and Hadoop command line client.
<p />
<h1><a name="Initializer_RPM"></a> Initializer RPM </h1>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  This must be done on <strong>all nodes</strong>
</dd></dl> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Initializing_the_YUM_Repository"></a> Initializing the YUM Repository </span></h2>
<p />
Download and install the <code>osg-hadoop-20</code> RPM on <strong>all nodes</strong>. This will initialize the OSG YUM repository for Hadoop.
<p />
<pre class="rootscreen">
[root@client ~]$ rpm -Uvh http://vdt.cs.wisc.edu/hadoop/osg-hadoop-20-3.el5.noarch.rpm
</pre>
<p />
This initializes YUM repository configuration in <code>/etc/yum.repos.d/osg-hadoop.repo</code>. 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Choosing_Stable_or_ITB_Repositor"></a> Choosing Stable or ITB Repository </span></h2>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  <font color="#ff0000"> <strong>For Trash/Trash/Integration Testbed (ITB) Sites:</strong> </font> 
</dd>  </dl><ul>
<li> By default, <strong>Stable Repository</strong> is enabled (<code>enabled=1</code>) in the YUM configuration. Production sites should use the default setting.
</li> <li> ITB sites doing testing can enable the <strong>Testing Repository</strong> to fetch pre-release packages. 
</li></ul> 
<p />
Simply set <code>enabled=0</code> in <code>[hadoop]</code> section and <code>enabled=1</code> in <code>[hadoop-testing]</code> section of <code>/etc/yum.repos.d/osg-hadoop.repo</code>.
<p />
<strong>YUM Repository types in /etc/yum.repos.d/osg-hadoop.repo</strong>
<p />
<table>
<tr colspan=2>
<td>
<strong>Production Sites:</strong>
<pre class="file">
[hadoop]
... ...
enabled=1
... ...

[hadoop-testing]
... ...
enabled=0
... ...

[hadoop-unstable]
... ...
enabled=0
... ...
</pre>
</td>
<td>
<strong>Trash/Trash/Integration Sites:</strong>
<pre class="file">
[hadoop]
... ...
enabled=0
... ...

[hadoop-testing]
... ...
enabled=1
... ...

[hadoop-unstable]
... ...
enabled=0
... ...
</pre>
</td>
</tr>
</table>
<p />
<h1><a name="Installing_Hadoop"></a> Installing Hadoop </h1>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  Hadoop must be installed on the following nodes:
</dd>  </dl><ul>
<li> <strong>Namenode</strong>
</li> <li> <strong>Datanode</strong>
</li> <li> <strong>Secondary Namenode</strong>
</li></ul> 
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  On the following nodes Hadoop must also be installed but the Hadoop service itself does not need to be started:
</dd>  </dl><ul>
<li> <strong>GridFTP node</strong>
</li> <li> <strong>SRM node</strong>
</li></ul> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Prerequisites"></a> Prerequisites </span></h2>
<p />
<p />
<p />
Hadoop will run anywhere that Java is supported (including Solaris).  However, these instructions are for RedHat 5 derivants (including Scientific Linux) because of the RPM based installation.
<p />
The HDFS prerequisites are: <ul>
<li> Minimum of 1 headnode (the namenode), although 2 recommended (the namenode and the secondary namenode)
</li> <li> At least one node which will hold data, preferably at least 2.  Most sites will have 20 to 200 datanodes.
</li> <li> The namenode and secondary name node are <strong>not</strong> datanodes.
</li> <li> Working Yum and RPM installation on every system.
</li> <li> <code>fuse</code> kernel module and <code>fuse-libs</code>.
</li> <li> Java RPM.  If java isn't already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency.  Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.
</li></ul> 
<p />
<strong>Compatibility Note</strong> Note that versions of OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux OpenAFS client, make sure you're running at least OpenAFS 1.4.7.
<p />
<strong>Note</strong>: The rpm/yum installation will create a 'hadoop' system account and group  (uid,gid &lt; 500) on the host system for running the datanode services.  If you would like to control the uid/gid that is used, then you should create the 'hadoop' user and group manually before installing the rpms.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Installation"></a> Installation </span></h2>
<p />
<p />
<p />
The Hadoop init script assumes that you are not running multiple hadoop services (datanode, namenode, secondary namenode) on the same host.
<p />
<p />
<p />
The only node that requires a FUSE mount is the <strong>SRM node</strong>.  However to install hadoop, the <code>hadoop-0.20-osg</code> rpm requires <code>fuse</code> and <code>fuse-libs</code> packages to be installed.  If you are using RHEL &gt;= 5.4 this requirement is met and they will be brought in as dependencies.  Otherwise you must find these packages for your platform or refer to <a href="/bin/view/ReleaseDocumentation/Hadoop20Install#TroubFuseMod" class="twikiCurrentTopicLink twikiAnchorLink">Notes on Building a FUSE Module</a> in the Troubleshooting section below.
<p />
To install hadoop, run:
<p />
<p />
<p />
<pre class="rootscreen">
[root@client ~]$ yum install hadoop-0.20-osg
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Configuration"></a> Configuration </span></h2>
<p />
<p />
<p />
The Hadoop RPMs install files into the standard system locations.  The following table highlights some of the more interesting locations, and documents whether you might ever want to edit them.
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table1" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> File Type </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Location </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Needs editing? </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Log files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> <code>/var/log/hadoop/*</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> PID files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> <code>/var/run/hadoop/*.pid</code> </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> init scripts </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> <code>/etc/init.d/hadoop</code>, <code>/etc/init.d/hadoop-firstboot</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> init script config file </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> <code>/etc/sysconfig/hadoop</code> </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Yes </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> runtime config files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> <code>/etc/hadoop/conf/*</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Maybe </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> System binaries </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> <code>/usr/bin/hadoop</code> </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> JARs </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLast"> <code>/usr/lib/hadoop/*</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> No </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<h3><a name="Edit_etc_sysconfig_hadoop"></a> Edit /etc/sysconfig/hadoop </h3>
<p />
The most common site configuration settings can be changed in <code>/etc/sysconfig/hadoop</code>.  In most cases, this file will be identical on the namenode and datanodes.  The configuration settings are documented in the file itself, but we document some of the most commonly edited ones in the table below:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table2" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Option Name </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs editing? </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Suggested value </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_NAMENODE </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The host name of your namenode; should match the output 'hostname -s' on the namenode server </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_NAMEPORT </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> 9000 </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_SECONDARY_NAMENODE </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The host name of the secondary namenode; should match the output of 'hostname -s' </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_CHECKPOINT_DIRS </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Comma-separated (<strong>important:</strong> no spaces between commas!) list of directories to store checkpoints on.  The safest configuration is to store 2 checkpoints locally on 2 block devices and 1 checkpoint on a NFS server.  At least 1 checkpoint directory is required. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_CHECKPOINT_PERIOD </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The time, in seconds, between checkpoints.  600 is suggested for small sites </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_REPLICATION_DEFAULT </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Default number of replications.  Suggested: 2 </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_REPLICATION_MIN </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Minimum number of replications; below this, an error will be thrown.  Suggested: 1 or 2. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_REPLICATION_MAX </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Maximum number of replications.  Suggested: 512 </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_GANGLIA_ADDRESS </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Hostname or IP of your Ganglia gmetad.  If left empty then hadoop will try to extract the ganglia metad address from /etc/gmond.conf.  If you aren't using Ganglia just leave it blank. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_DATADIR </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> The base directory where HDFS temp and management data will be written.  On datanodes this is usually the parent of the first data partition. It is safe to leave this empty for client-only installations. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_DATA </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> A comma-separated list of directories (no spaces!) where the HDFS data blocks will be stored.  The first one is typicall the same as $HADOOP_DATADIR/data.  It is safe to leave this empty for client-only installations. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_USER </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> The username that the hadoop datanode daemons will run under.  Suggested: hadoop </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_NAMENODE_HEAP </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The Java heap size for the namenode; bigger is better, but the node shouldn't swap.  Minimum: 2048m.  Suggested: 8192m </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> HADOOP_MIN_DATANODE_SIZE </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLast"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> A value in GB; if the data directory is smaller than this size, HDFS will refuse to start.  Safeguards against starting the datanode daemon on non-datanodes.  Suggested: 300 (this value will vary widely with your datanode size). Set to zero or an empty string to bypass this check. </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
After making changes to the file, you must run:
<p />
<pre class="rootscreen">
[root@client ~]$ service hadoop-firstboot start
</pre>
<p />
This propagates the changes to the hadoop configuration files in <code>/etc/hadoop</code> and must be run every time you make changes to <code>/etc/sysconfig/hadoop</code>.
<p />
<strong>NOTE:</strong> If you just installed Hadoop for the first time, you must log in/out of your shell or source /etc/profile.d/hadoop.sh before your you try playing with the command line tools.
<p />
<strong>Upgrade note:</strong> Configuration files will be saved with a <code>.rpmsave</code> extension if you ever update your hadoop rpms with rpm or yum.  <strong>Make sure to copy your settings from <code>/etc/sysconfig/hadoop.rpmsave</code> to <code>/etc/sysconfig/hadoop</code> if you ever update your hadoop rpms.</strong>  Any manual changes to the hadoop configuration files in <code>/etc/hadoop/</code> should be preserved during an upgrade, but may be overwritten when running <code>hadoop-firstboot</code>.
<p />
<h4><a name="Side_topic_Multiple_data_directo"></a> Side topic: Multiple data directories on a datanode. </h4>
<p />
Hadoop has the ability to store data in multiple directories on a datanode.  This can be useful if you have multiple drives on your datanode and don't want to run them in a raid array, or if you have multiple large storage volumes mounted on your datanode.  To configure a datanode to use multiple directories, you need to enter each directory in the <code>HADOOP_DATA</code> setting in <code>/etc/sysconfig/hadoop</code> as a comma-separated list of directories (no spaces!) and then run <code>service hadoop-firstboot start</code>.  Here is an example of a datanode with 4 storage directories:
<p />
<pre class="file">
HADOOP_DATA=/data1/hadoop/data,/data2/hadoop/data,/data3/hadoop/data,/data4/hadoop/data
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Running_Hadoop"></a> Running Hadoop </span></h2>
<p />
<p />
<p />
The Hadoop rpms install a startup script in <code>/etc/init.d/hadoop</code>.  The same command is used to start hadoop services on a datanode, namenode, or secondary namenode:
<p />
<pre class="rootscreen">
[root@client ~]$ service hadoop start
</pre>
<p />
You will also want to configure hadoop to start at boot time with:
<p />
<pre class="rootscreen">
[root@client ~]$  chkconfig hadoop on
</pre>
<p />
<h4><a name="Side_topic_Client_only_installat"></a> Side topic: Client-only installation </h4>
<p />
Sometimes it is handy to configure a node to be a client, that is, a system that has access to hadoop but will not serve as a datanode or namenode.  The installation and configuration for such a node is the same as above, except that you do not need to start any hadoop services with <code>/etc/init.d/hadoop</code>.  It is still necessary to modify <code>/etc/sysconfig/hadoop</code>, but it is not necessary to specify any datanode directories in <code>HADOOP_DATA</code>.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="FUSE"></a> FUSE </span></h2>
<p />
A FUSE mount is only required on the <strong>SRM node</strong> and any other node you would like to use standard POSIX-like commands on the Hadoop filesystem. If these cases don't apply you may skip to the <a href="/bin/view/ReleaseDocumentation/Hadoop20Install#HadoopValidation" class="twikiCurrentTopicLink twikiAnchorLink">Hadoop Validation</a> section.
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  Before using FUSE you may need to add the module using <code>modprobe</code> first:
</dd></dl> 
<p />
<pre class="rootscreen">
[root@client ~]$ modprobe fuse
</pre>
<p />
<a name="FuseMount"></a>
<h3><a name="Mounting_FUSE_at_Boot_Time"></a> Mounting FUSE at Boot Time </h3>
<p />
You can  mount FUSE by adding the following line to <code>/etc/fstab</code> (Be sure to change the <code>/mnt/hadoop</code> mount point and <code>namenode.host</code> to match your local configuration.  To match the help documents, we recommend using <code>/mnt/hadoop</code> as your mountpoint):
<p />
<pre class="file">
hdfs# <font color="#ff0000">/mnt/hadoop</font> fuse server=<font color="#ff0000">namenode.host</font>,port=9000,rdbuffer=131072,allow_other 0 0
</pre>
<p />
Alternatively this can be taken care of automatically when running <code>hadoop-firstboot</code> if in your <code>/etc/sysconfig/hadoop</code> file you set the following line:
<p />
<pre class="file">
HADOOP_UPDATE_FSTAB=1
</pre>
<p />
Once your <code>/etc/fstab</code> is updated, to mount FUSE run:
<p />
<pre class="rootscreen">
[root@client ~]$ mount /mnt/hadoop
</pre>
<p />
When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:
<p />
<pre class="rootscreen">
# mount /mnt/hadoop
port=32767,server=(
fuse-dfs didn't recognize /mnt/hadoop,-2
fuse-dfs ignoring option allow_other
</pre>
<p />
<p />
<p />
If you have troubles mounting FUSE refer to <a href="/bin/view/ReleaseDocumentation/Hadoop20Install#TroubFuseDeb" class="twikiCurrentTopicLink twikiAnchorLink">Running FUSE in Debug Mode</a> in the Troubleshooting section.
<p />
<a name="HadoopValidation"></a>
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Validation"></a> Validation </span></h2>
<p />
The first thing you may want to do after installing and starting your <strong>Namenode</strong> is to verify that the web interface works.  In your web browser go to:
<p />
<pre class="file">
http://<font color="#ff0000">namenode.hostname</font>:50070/dfshealth.jsp
</pre>
<p />
Get familiar with Hadoop commands.  Run hadoop with no arguments to see the list of commands.
<p />
<pre class="screen">
[user@client ~]$ hadoop
<div class="twistyPlugin twikiMakeVisibleInline">  <span id="twistyIdReleaseDocumentationHadoop20Install1show" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Show Full Output</span></a> </span> <span id="twistyIdReleaseDocumentationHadoop20Install1hide" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyHidden twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Hide Full Output</span></a> </span>  </div><!--/twistyPlugin twikiMakeVisibleInline--> <div class="twistyPlugin"><div id="twistyIdReleaseDocumentationHadoop20Install1toggle" class="twistyRememberSetting twistyStartHide twistyContent twikiMakeHidden twistyInited0">
Usage: hadoop [--config confdir] COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  mradmin              run a Map-Reduce admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  fetchdt              fetch a delegation token from the NameNode
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  job                  manipulate MapReduce jobs
  queue                get information regarding JobQueues
  version              print the version
  jar <jar>            run a jar file
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  oiv                  apply the offline fsimage viewer to an fsimage
  classpath            prints the class path needed to get the
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print help when invoked w/o parameters.
</div></div> <!--/twistyPlugin-->
</pre>
<p />
For a list of supported filesystem commands:
<p />
<pre class="screen">
[user@client ~]$ hadoop fs
<div class="twistyPlugin twikiMakeVisibleInline">  <span id="twistyIdReleaseDocumentationHadoop20Install2show" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Show Full Output</span></a> </span> <span id="twistyIdReleaseDocumentationHadoop20Install2hide" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyHidden twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Hide Full Output</span></a> </span>  </div><!--/twistyPlugin twikiMakeVisibleInline--> <div class="twistyPlugin"><div id="twistyIdReleaseDocumentationHadoop20Install2toggle" class="twistyRememberSetting twistyStartHide twistyContent twikiMakeHidden twistyInited0">
Usage: java FsShell
           [-ls <path>]
           [-lsr <path>]
           [-df [<path>]]
           [-du <path>]
           [-dus <path>]
           [-count[-q] <path>]
           [-mv <src> <dst>]
           [-cp <src> <dst>]
           [-rm [-skipTrash] <path>]
           [-rmr [-skipTrash] <path>]
           [-expunge]
           [-put <localsrc> ... <dst>]
           [-copyFromLocal <localsrc> ... <dst>]
           [-moveFromLocal <localsrc> ... <dst>]
           [-get [-ignoreCrc] [-crc] <src> <localdst>]
           [-getmerge <src> <localdst> [addnl]]
           [-cat <src>]
           [-text <src>]
           [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>]
           [-moveToLocal [-crc] <src> <localdst>]
           [-mkdir <path>]
           [-setrep [-R] [-w] <rep> <path/file>]
           [-touchz <path>]
           [-test -[ezd] <path>]
           [-stat [format] <path>]
           [-tail [-f] <file>]
           [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
           [-chown [-R] [OWNER][:[GROUP]] PATH...]
           [-chgrp [-R] GROUP PATH...]
           [-help [cmd]]

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|jobtracker:port>    specify a job tracker
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]
</div></div> <!--/twistyPlugin-->
</pre>
<p />
An online guide is also available at <a href="http://hadoop.apache.org/common/docs/current/commands_manual.html" target="_top">Apache Hadoop commands manual</a>.
You can use Hadoop commands to perform filesystem operations with more consistency.
<p />
Example, to look into the internal hadoop namespace:
<p />
<pre class="screen">
[user@client ~]$ hadoop fs -ls /
Found 1 items
drwxrwxr-x   - engage engage          0 2011-07-25 06:32 /engage
</pre>
<p />
Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself <code>/mnt/hadoop</code> in Hadoop commands):
<p />
<pre class="rootscreen">
[root@client ~]$ hadoop fs -chown -R engage:engage /engage
</pre>
<p />
Example, compare <code>hadoop fs</code> command vs. using FUSE mount:
<pre class="screen">
[user@client ~]$ hadoop fs -ls /engage
Found 3 items
-rw-rw-r--   2 engage engage  733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso
-rw-rw-r--   2 engage engage  215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz
-rw-rw-r--   2 engage engage    9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz

[user@client ~]$ ls -l /mnt/hadoop/engage
total 935855
-rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso
-rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz
-rw-rw-r-- 1 engage engage   9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz
</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Creating_VO_and_User_filesystem"></a> Creating VO and User filesystem areas </span></h2>
<p />
Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. 
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  Create (and maintain) usernames and groups with UIDs and GIDs on <strong>all nodes</strong>. These are maintained in basic system files such as <code>/etc/passwd</code> and <code>/etc/group</code>.
</dd></dl> 
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  In the examples below It is assumed a FUSE mount is set to <code>/mnt/hadoop</code>.  As an alternative <code>hadoop fs</code> commands could have been used.
</dd></dl> 
<p />
For clean HDFS operations and filesystem management:
<p />
(a) Create top-level VO subdirectories under <code>/mnt/hadoop</code>.
<p />
Example: 
<p />
<pre class="rootscreen">
[root@client ~]$ mkdir /mnt/hadoop/cms
[root@client ~]$ mkdir /mnt/hadoop/dzero
[root@client ~]$ mkdir /mnt/hadoop/sbgrid
[root@client ~]$ mkdir /mnt/hadoop/fermigrid
[root@client ~]$ mkdir /mnt/hadoop/cmstest
[root@client ~]$ mkdir /mnt/hadoop/osg
</pre>
<p />
(b) Create individual top-level user areas, under each VO area, as needed.
<p />
<pre class="rootscreen">
[root@client ~]$ mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina
[root@client ~]$ mkdir -p /mnt/hadoop/cms/store/user/michaelthomas
[root@client ~]$ mkdir -p /mnt/hadoop/cms/store/user/brianbockelman
[root@client ~]$ mkdir -p /mnt/hadoop/cms/store/user/douglasstrain
[root@client ~]$ mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana
</pre>
<p />
(c) Adjust username:group ownership of each area. 
<p />
<pre class="rootscreen">
[root@client ~]$ chown -R cms:cms /mnt/hadoop/cms
[root@client ~]$ chown -R sam:sam /mnt/hadoop/dzero

[root@client ~]$ chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas
</pre>
<p />
<h1><a name="Installing_GridFTP"></a> Installing GridFTP </h1>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd> GridFTP must be installed on the <strong>GridFTP node</strong>
</dd></dl> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Prerequisites_AN1"></a> Prerequisites </span></h2>
<p /> <ol>
<li> Install the Hadoop RPM on your GridFTP node, edit <code>/etc/sysconfig/hadoop</code>, and verify your installation
</li> <li> We assume your site is running a sufficiently recent GUMS server &gt;= 1.3 (grid-mapfiles are not currently tested or supported).
</li></ol> 
<p />
<p />
<p />
The GridFTP server for Hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration.
You should plan accordingly to provision enough GridFTP servers to handle the bandwidth that your site can support.
<p />
The installation includes the latest CA Certificates package from the OSG as well as the fetch-crl CRL updater. <strong>NOTE</strong> the fetch-crl service does not start by default after installing GridFTP.  To have fetch-crl update automatically, run:
<p />
<pre class="rootscreen">
[root@client ~]$ service fetch-crl-cron start
</pre>
<p />
cron will check for CRL updates every 6 hours.  If this is your first time installing you may need to run it immediately:
<p />
<pre class="rootscreen">
[root@client ~]$ /usr/sbin/fetch-crl -r 20  -a 24 --quiet
</pre>
<p />
<strong>Note:</strong> You do not need FUSE mounted on GridFTP nodes,
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Installation_AN1"></a> Installation </span></h2>
<p />
To install gridftp-hdfs server, run:
<p />
<p />
<p />
<pre class="rootscreen">
[root@client ~]$ yum install gridftp-hdfs
</pre>
<p />
Updates can be installed with:
<p />
<pre class="rootscreen">
[root@client ~]$ yum upgrade gridftp-hdfs
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Configuration_AN1"></a> Configuration </span></h2>
<p />
<p />
<p />
The installation of gridftp-hdfs and its dependencies creates several directories.
In addition to the Hadoop installation files, you will also find:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table3" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Log files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/var/log/gridftp-auth.log</code>, <code>/var/log/gridftp.log</code> </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> xinetd files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/etc/xinetd.d/gridftp-hdfs</code> </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> runtime config files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/etc/gridftp-hdfs/*</code> </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> System binaries </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/usr/bin/gridftp-hdfs*</code> </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> System libraries </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/usr/lib64/libglobus_gridftp_server_hdfs.so*</code> </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> GUMS client (called LCMAPS) configuration </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/etc/lcmaps/lcmaps.db</code> </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> CA certificates </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLastCol twikiLast"> <code>/etc/grid-security/certificates/*</code> </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<code>lcmaps.db</code> is provided by the globus-mapping-osg package.
<p />
gridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop.
As per the prerequisites section, you should have already edited <code>/etc/sysconfig/hadoop</code> and run <code>service hadoop-firstboot start</code>.
If you did not follow the directions, please do that now.
<p />
It is <strong>not</strong> necessary to start any Hadoop services with <code>service hadoop start</code> if you are running a dedicated GridFTP server (that is, no datanode or namenode services will be run on the host).
<p />
In <code>/etc/lcmaps/lcmaps.db</code> you will need to enter the URL for your GUMS server, as well as the path to your host certificate and key:
<p />
<pre class="file">
             "--endpoint https://<font color="#ff0000">gums.hostname</font>:8443/gums/services/GUMSXACMLAuthorizationServicePort"
</pre>
<p />
The default settings in <code>/etc/gridftp-hdfs/*.conf</code> should be ok for most installations.
The file <code>gridftp-inetd.conf</code> is used by the xinetd service for starting up the GridFTP server.
The file <code>gridftp.conf</code> is used by <code>/usr/bin/gridftp-hdfs-standalone</code> for starting up the GridFTP server in a testing mode.
<code>gridftp-hdfs-local.conf</code> contains additional site-specific environment variables that are used by the gridftp-hdfs dsi module in both the xinetd and standalone GridFTP server.
Some of the environment variables that can be used in <code>gridftp-hdfs-local.conf</code> include:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table4" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Option Name </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing? </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Suggested value </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> GRIDFTP_HDFS_REPLICA_MAP </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> File containing a list of paths and replica values for setting the default # of replicas for specific file paths </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> GRIDFTP_BUFFER_COUNT </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> GRIDFTP_FILE_BUFFER_COUNT </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> GRIDFTP_SYSLOG </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set this to 1 in case if you want to send transfer activity data to syslog (only used for the <span class="twikiNewLink">HadoopViz<a href="/bin/edit/Storage/HadoopViz?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="HadoopViz (this topic does not yet exist; you can create it)">?</a></span> application) </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> GRIDFTP_HDFS_MOUNT_POINT </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The location of the FUSE mount point used during the Hadoop installation.  Defaults to /mnt/hadoop.  This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths. <strong>Note:</strong> this does not imply you need FUSE mounted on GridFTP nodes! </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> GRIDFTP_LOAD_LIMIT </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> TMPDIR </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLast"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> The temp directory where the file-based buffers are stored.  Defaults to /tmp. </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<code>gridftp-hdfs-local.conf</code> is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files (<code>ulimit -n</code>).
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Running_GridFTP"></a> Running GridFTP </span></h2>
<p />
<p />
<p />
If you were not already running the xinetd service (by default it is not installed on RHEL5), then you will need to start it with the command:
<p />
<pre class="rootscreen">
[root@client ~]$ service xinetd restart
</pre>
<p />
Otherwise, the gridftp-hdfs service should be configured to run automatically as soon as the installation is finished.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Validation_AN1"></a> Validation </span></h2>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  The commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using <code>voms-proxy-init</code> or <code>grid-proxy-init</code>.  Obtaining grid credentials is beyond the scope of this document.
</dd></dl> 
<p />
<pre class="screen">
[user@client ~]$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt
</pre>
<p />
If you are having troubles running GridFTP refer to <a href="/bin/view/ReleaseDocumentation/Hadoop20Install#GridFTPStand" class="twikiCurrentTopicLink twikiAnchorLink">Starting GridFTP in Standalone Mode</a> in the Troubleshooting section.
<p />
<h1><a name="Installing_BeStMan2"></a> Installing BeStMan2 </h1>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd> BeStMan2 must be installed on the <strong>SRM node</strong>
</dd></dl> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Prerequisites_AN2"></a> Prerequisites </span></h2> <ol>
<li> Make sure FUSE is installed and <a href="/bin/view/ReleaseDocumentation/Hadoop20Install#FuseMount" class="twikiCurrentTopicLink twikiAnchorLink">mounted</a> on the <strong>SRM node</strong>.
</li> <li>  A GridFTP-HDFS server must also be installed, but this does not need to be on the same node as the BeStMan2 server.  A larger site will prefer to have their GridFTP and BeStMan2 servers installed on separate hosts.
</li> <li> In addition to the Java jdk you need the corresponding Java sun-compat package.  For example for <code>jdk-1.6.0</code> you need to install <code>java-1.6.0-sun-compat</code>.  If you installed the jdk rpm that we supplied you can just let <code>java-1.6.0-sun-compat</code> come in as a dependency.  Otherwise you need to find and manually install the correct version before continuing.  See the <a href="http://www.jpackage.org/installation.php" target="_top">jpackage installation doc</a> for more details.
</li> <li> CA Certificates installed in <code>/etc/grid-security/certificates</code>.
</li></ol> 
<p />
<p />
<p />
BeStMan2 is preconfigured to look for the <strong>host</strong> certificate and key in <code>/etc/grid-security/http/http*.pem</code>.
These files <strong>must</strong> exist and be <strong>owned</strong> by the <strong>bestman</strong> user.  Using certificates in a different directory or with different names is not supported.
<p />
<strong>NOTE:</strong> The names are misleading.  You must copy over your <code>hostcert.pem</code> and <code>hostkey.pem</code> as <code>httpcert.pem</code> and <code>httpkey.pem</code> respectively in the  <code>/etc/grid-security/http/</code> directory.  http certs / keys will <strong>NOT</strong> work.
<p />
<strong>NOTE</strong> This rpm no longer brings in the OSG CA Certificates package or fetch-crl CRL updater as dependencies.  However we still provide the packages in the repo.  If you would like them run:
<p />
<pre class="rootscreen">
[root@client ~]$ yum install osg-ca-certs
</pre>
<p />
<pre class="rootscreen">
[root@client ~]$ yum install fetch-crl
</pre>
<p />
<strong>NOTE</strong> if you chose not to install them you must install the certificates in another way. BeStMan2 still assumes you have them located in <code>/etc/grid-security/certificates</code>.
<p />
 <strong>NOTE</strong> the fetch-crl service does not start by default on installation.  To have fetch-crl update automatically, run:
<p />
<pre class="rootscreen">
[root@client ~]$ service fetch-crl-cron start
</pre>
<p />
cron will check for CRL updates every 6 hours.  If this is your first time installing you may need to run it immediately:
<p />
<pre class="rootscreen">
[root@client ~]$ /usr/sbin/fetch-crl -r 20  -a 24 --quiet
</pre>
<p />
The rpm/yum installation will create a 'bestman' system account and group (uid,gid &lt; 500) on the host system for running the BeStMan2 SRM process. If you would like to control the uid/gid that is used, then you should create the 'bestman' user and group manually before installing the rpms.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Installation_AN2"></a> Installation </span></h2>
<p />
<p />
<p />
<pre class="rootscreen">
[root@client ~]$ yum install bestman2-server
</pre>
<p />
Updates can be installed with:
<p />
<pre class="rootscreen">
[root@client ~]$ yum upgrade bestman2-server
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Configuration_AN2"></a> Configuration </span></h2>
<p />
<p />
<p />
For those familiar with the VDT installation of BeStMan, you will know about the <code>configure_bestman</code> script for configuring the BeStMan server.  This script is not supported or included in the RPM package.  Certain operations that you would normally do with <code>configure_bestman</code>, such as changing the certificate location, are not supported.
<p />
The installation of BeStMan2 and its dependencies creates several directories.  In addition to the Hadoop installation files, you will also find:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table5" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Log files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/var/log/bestman2</code> </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> main config file </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/etc/bestman2/conf/bestman2.rc</code> </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> other runtime config files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/etc/bestman2/conf/*</code> </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> BeStMan2 lib files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLastCol"> <code>/usr/share/java/bestman2/</code> </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> init.d startup script </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLastCol twikiLast"> <code>/etc/init.d/bestman2</code> </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<p />
<p />
BeStMan2 SRM uses the Hadoop FUSE mount to perform namespace operations, such as mkdir, rm, and ls.  As per the Hadoop install instructions, edit <code>/etc/sysconfig/hadoop</code> and run <code>service hadoop-firstboot start</code>.  It is <strong>not</strong> necessary (or even recommended) to start any hadoop services with <code>service hadoop start</code>.
<p />
<p />
<p />
The BeStMan2 SRM configuration file is located in <code>/etc/bestman2/conf/bestman2.rc</code>.  There are a few settings that you need to add or change manually, depending on your site configuration:
<p />
<pre class="file">
supportedProtocolList=gsiftp://<font color="#ff0000">your.gridftp.server1</font>:2811;gsiftp://<font color="#ff0000">your.gridftp.server2</font>:2811
GUMSserviceURL=https://<font color="#ff0000">your.gums.host</font>:8443/gums/services/GUMSAuthorizationServicePort
localPathListAllowed=/mnt/hadoop;/tmp
</pre>
<p />
BeStMan2 uses sudo to perform changes to the filesystem namespace.  This ensures that directories get created and file get removed with the proper permissions.  You must manually add permissions.  Append the following to the end of the <code>/etc/sudoers</code> file with the <code>visudo</code> command:
<p />
<pre class="file">
Cmnd_Alias SRM_CMD = /bin/rm, /bin/mkdir, /bin/rmdir, /bin/mv, /bin/ls 
Runas_Alias SRM_USR = ALL, !root 
bestman ALL=(SRM_USR) NOPASSWD:SRM_CMD
</pre>
<p />
If you are running SL5, comment out the following line in /etc/sudoers:
<pre class="file">
Defaults    requiretty
</pre>
With this option enabled, BeStMan2 will be unable to use sudo because it doesn't use a console.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Running_BeStMan2"></a> Running BeStMan2 </span></h2>
<p />
<p />
<p />
Start the BeStMan2 SRM server with the command
<p />
<pre class="rootscreen">
[root@client ~]$ service bestman2 start
</pre>
<p />
To start BeStMan2 SRM automatically at boot time:
<p />
<pre class="rootscreen">
[root@client ~]$ chkconfig bestman2 on
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Validation_AN2"></a> Validation </span></h2>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  The commands used to verify BeStMan2 below assume you have access to a node where you can first generate a valid proxy using <code>voms-proxy-init</code> or <code>grid-proxy-init</code>.  Obtaining grid credentials is beyond the scope of this document.
</dd></dl> 
<p />
Check SRM server ping response:
<pre class="screen">
[user@client ~]$ srm-ping srm://devg-1.t2.ucsd.edu:8443/srm/v2/server
srm-ping   2.2.1.3.18    Mon Dec 20 20:16:15 PST 2010
BeStMan and SRM-Clients Copyright(c) 2007-2010,
Lawrence Berkeley National Laboratory. All rights reserved.
Support at SRM@LBL.GOV and documents at http://sdm.lbl.gov/bestman
SRM-CLIENT: Connecting to serviceurl httpg://devg-1.t2.ucsd.edu:8443/srm/v2/server

SRM-PING: Mon Jul 25 06:35:16 PDT 2011  Calling SrmPing Request...
versionInfo=v2.2

Extra information (Key=Value)
backend_type=BeStMan
backend_version=2.2.2.0.13
backend_build_date=2011-06-27T21:13:48.000Z 
gsiftpTxfServers[0]=gsiftp://devg-7.t2.ucsd.edu:2811
GatewayMode=Enabled
clientDN=/DC=org/DC=doegrids/OU=People/CN=Jeffrey M. Dost 948199
gumsIDMapped=engage
</pre>
<p />
Check SRM based remote directory listing:
<pre class="screen">
[user@client ~]$ lcg-ls -l -b -D srmv2 srm://devg-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/mnt/hadoop/engage
----------   1     2     2 733669376              UNKNOWN /mnt/hadoop/engage/CentOS-5.6-x86_64-LiveCD.iso
----------   1     2     2 215387183              UNKNOWN /mnt/hadoop/engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz
----------   1     2     2 9259360              UNKNOWN /mnt/hadoop/engage/glideinWMS_v2_5_1.tgz
----------   1     2     2      45              UNKNOWN /mnt/hadoop/engage/test.txt
</pre>
<p />
Check SRM copy using GridFTP underneath:
<pre class="screen">
[user@client ~]$ lcg-cp -v -b -D srmv2 file:/home/users/jdost/test2.txt srm://devg-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/mnt/hadoop/engage/test2.txt
Using grid catalog type: UNKNOWN
Using grid catalog : (null)
VO name: Engage
Checksum type: None
Destination SE type: SRMv2
Destination SRM Request Token: put:2
Source URL: file:/home/users/jdost/test2.txt
File size: 59
Source URL for copy: file:/home/users/jdost/test2.txt
Destination URL: gsiftp://devg-7.t2.ucsd.edu:2811//mnt/hadoop/engage/test2.txt
# streams: 1
           59 bytes      0.04 KB/sec avg      0.04 KB/sec inst
Transfer took 3010 ms

[user@client ~]$ lcg-ls -l -b -D srmv2 srm://devg-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/mnt/hadoop/engage
----------   1     2     2 733669376              UNKNOWN /mnt/hadoop/engage/CentOS-5.6-x86_64-LiveCD.iso
----------   1     2     2 215387183              UNKNOWN /mnt/hadoop/engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz
----------   1     2     2 9259360              UNKNOWN /mnt/hadoop/engage/glideinWMS_v2_5_1.tgz
----------   1     2     2      45              UNKNOWN /mnt/hadoop/engage/test.txt
----------   1     2     2      59              UNKNOWN /mnt/hadoop/engage/test2.txt
</pre>
<p />
<h1><a name="Installing_Gratia_Transfer_Probe"></a> Installing Gratia Transfer Probe </h1>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  The Gratia Transfer Probe must be installed on the <strong>GridFTP node</strong>
</dd></dl> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Prerequisites_AN3"></a> Prerequisites </span></h2>
<p /> <ol>
<li> GridFTP is installed and working
</li></ol> 
<p />
<p />
<p />
The Gratia probe requires the file <code>osg-user-vo-map.txt</code> to exist and be up to date. We provide an rpm package that takes care of creating and updating this file as needed.  To get it run:
<p />
<pre class="rootscreen">
[root@client ~]$ yum install osg-user-vo-map-cron
</pre>
<p />
It will pull in the <code>gums-client</code> rpm as a dependency.  Details on setting up <code>osg-user-vo-map-cron</code> are below.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Installation_AN3"></a> Installation </span></h2>
<p />
To install the Gratia Transfer Probe, run:
<p />
<p />
<p />
<pre class="rootscreen">
[root@client ~]$ yum install gratia-probe-gridftp-transfer
</pre>
<p />
Updates can be installed with:
<p />
<pre class="rootscreen">
[root@client ~]$ yum upgrade gratia-probe-gridftp-transfer
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Configuration_AN3"></a> Configuration </span></h2>
<p />
<p />
<p />
This RPM <strong>does not</strong> use Linux-standard file locations.  Here are the most relevant file and directory locations:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table6" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Purpose </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing? </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Location </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Probe Configuration </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> /opt/vdt/gratia/probe/gridftp-transfer/ProbeConfig </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Probe Executables </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> /opt/vdt/gratia/probe/gridftp-transfer </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Log files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> /opt/vdt/gratia/var/logs </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Temporary files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> /opt/vdt/gratia/var/tmp </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> Gums configuration </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLast"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> /etc/gums/gums-client.properties </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
The RPM installs the Gratia probe into the system crontab, but does not configure it.  The configuration of the probe is controlled by the file
<p />
<pre>
/opt/vdt/gratia/probe/gridftp-transfer/ProbeConfig
</pre>
<p />
This is usually one XML node spread over multiple lines.  Note that comments (#) have no effect on this file.  You will need to edit the following:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table7" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Attribute </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Value </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> ProbeName </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> This should be set to "gridftp-transfer:&lt;hostname&gt;", where &lt;hostname&gt; is the fully-qualified domain name of your gridftp host. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> CollectorHost </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the hostname and port of the central collector.  By default it sends to the OSG collector.  See below. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> SiteName </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the resource group name of your site as registered in OIM. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> GridftpLogDir </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set to /var/log, or wherever your current gridftp logs are located </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Grid </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Set to "ITB" if this is a test resource; otherwise, leave as OSG. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> UserVOMapFile </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the location of your osg-user-vo-map.txt; see below for information about this file. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> SuppressUnknownVORecords </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Set to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> SuppressNoDNRecords </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> EnableProbe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLast"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> Set to 1 to enable the probe. </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<h3><a name="Selecting_a_collector_host"></a> Selecting a collector host </h3>
<p />
The collector is the central server which logs the GridFTP transfers into a database.  There are usually three options:
<p /> <ol>
<li> <strong>OSG Transfer Collector</strong>: This is the primary collector for transfers in the OSG.  Use CollectorHost="gratia-osg-transfer.opensciencegrid.org:80".
</li> <li> <strong>OSG-ITB Transfer Collector</strong>: This is the test collector for transfers in the OSG.  Use CollectorHost="gratia-osg-transfer.opensciencegrid.org:8881".
</li> <li> <strong>Site local collector</strong>: If your site has set up its own collector, then your admin will be able to give you an endpoint to use.  Typically, this is along the lines of CollectorHost="collector.example.com:8880".
</li></ol> 
<p />
<h3><a name="Generating_osg_user_vo_map_txt"></a> Generating osg-user-vo-map.txt </h3>
<p />
The <code>osg-user-vo-map.txt</code> is a simple, space-separated format that contains 2 columns; the first is a unix username and the second is the VO which that username correspond to.  In order to create it you must install the <code>osg-user-vo-map-cron</code> rpm as mentioned above.  Once <code>osg-user-vo-map-cron</code> is installed you need to configure the gums client.
<p />
The primary configuration file for the gums-client utilities is located in <code>/etc/gums/gums-client.properties</code>.  The two properties that you must change are:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table8" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Attribute </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Value </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> gums.location </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> This should be set to the admin URL for your gums server, usually of the form gums.location=https://GUMS_HOSTNAME:8443/gums/services/GUMSAdmin </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> gums.authz </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLast"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> This should be set to the authorization interface URL for your gums server, usually of the form gums.authz=https://GUMS_HOSTNAME:8443/gums/services/GUMSXACMLAuthorizationServicePort </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
After <code>osg-user-vo-map-cron</code> is installed and the gums client is configured <code>osg-user-vo-map.txt</code> should be created in the following location:
<p />
<pre>/etc/grid-security/osg-user-vo-map.txt</pre>
<p />
Make sure the <strong>UserVOMapFile</strong> field is set to this location in
<p />
<pre>/opt/vdt/gratia/probe/gridftp-transfer/ProbeConfig</pre>
<p />
Without <code>osg-user-vo-map.txt</code> , all gridftp transfers will show up as belonging to the VO "Unknown".
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Validation_AN3"></a> Validation </span></h2>
<p />
<p />
<p />
Run the Gratia probe once by hand to check for functionality:
<p />
<pre class="rootscreen">
[root@client ~]$ /opt/vdt/gratia/probe/gridftp-transfer/gridftp-transfer_meter.cron.sh
</pre>
<p />
Look for any abnormal termination and report it if it is a non-trivial site issue.  Look in the log files in <code>/opt/vdt/gratia/var/logs/&lt;date&gt;.log</code> and make sure there are no error messages printed.
<p />
<p />
<p />
<h1><a name="Installing_Hadoop_Storage_Probe"></a> Installing Hadoop Storage Probe </h1>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  The Hadoop Storage Probe must be installed on the <strong>Namenode</strong>
</dd></dl> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Installation_AN4"></a> Installation </span></h2>
<p />
<p />
<p />
<pre class="rootscreen">
[root@client ~]$ yum install gratia-probe-hadoop-storage
</pre>
<p />
Updates can be installed with:
<p />
<pre class="rootscreen">
[root@client ~]$ yum upgrade gratia-probe-hadoop-storage
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Configuration_AN4"></a> Configuration </span></h2>
<p />
<p />
<p />
This RPM <strong>does not</strong> using Linux-standard file locations.  Here are the most relevant file and directory locations:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table9" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Purpose </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing? </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Location </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Probe Configuration </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> /opt/vdt/gratia/probe/hadoop-storage/ProbeConfig </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Probe Executable </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> /opt/vdt/gratia/probe/hadoop-storage/hadoop_storage_probe </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Log files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> /opt/vdt/gratia/var/logs </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> Temporary files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLast"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> /opt/vdt/gratia/var/tmp </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
The RPM installs the Gratia probe into the system crontab, but does not configure it.  The configuration of the probe is controlled by two files
<p />
<pre>
/opt/vdt/gratia/probe/hadoop-storage/ProbeConfig
/opt/vdt/gratia/probe/hadoop-storage/storage.cfg
</pre>
<p />
<h3><a name="ProbeConfig"></a> ProbeConfig </h3>
This is usually one XML node spread over multiple lines.  Note that comments (#) have no effect on this file.  You will need to edit the following:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table10" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Attribute </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Value </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> CollectorHost </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the hostname and port of the central collector.  By default it sends to the OSG collector.  You probably do not want to change it. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> SiteName </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the resource group name of your SE as registered in OIM. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Grid </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Set to "ITB" if this is a test resource; otherwise, leave as OSG. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> EnableProbe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLast"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> Set to 1 to enable the probe. </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<h3><a name="storage_cfg"></a> storage.cfg </h3>
This file controls which paths in HDFS should be monitored.  This is in the Windows INI format.
<p />
For each logical "area" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area.  Unix globs are accepted.
<p />
To configure an area named "CMS /store" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file.
<p />
<pre class="file">
[Area CMS /store]
Name = CMS /store
Path = /user/cms/store/*
Trim = /user/cms
</pre>
<p />
For each such area, add a section to your configuration file.
<p />
<h4><a name="Example_file"></a> Example file </h4>
Below is a configuration file that includes three distinct areas.  Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above:
<p />
<pre class="file">
[Gratia]
gratia_location = /opt/vdt/gratia
Storage.ProbeConfig = %(gratia_location)s/probe/hadoop-storage/ProbeConfig

[Area /store]
Name = CMS /store
Path = /store/*

[Area /store/user]
Name = CMS /store/user
Path = /store/user/*

[Area /user]
Name = Hadoop /user
Path = /user/*
</pre>
<p />
<p />
<p />
<h1><a name="Installing_Hadoop_Storage_Report"></a> Installing Hadoop Storage Reports (Optional) </h1>
<p />
<p /> <dl>
<dt> <img src="/twiki/pub/TWiki/TWikiDocGraphics/help.gif" alt="HELP" title="HELP" width="16" height="16" border="0" />     <strong><font color="#ff0000"> NOTE </font></strong> </dt><dd>  The Hadoop Storage Reports may be installed on any node that has access to a local Gratia Collector 
</dd></dl> 
<p />
<p />
<p />
The Hadoop storage reports provides a daily report on the status and usage of your SE.  This serves as a handy tool for both site administrators and site executives.  An example report is copied at the end of this guide.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Prerequisites_AN4"></a> Prerequisites </span></h2>
<p />
<p />
<p /> <ol>
<li> A working HDFS installation
</li> <li> A local <a href="/bin/view/Trash/ReleaseDocumentationGratiaSiteCollector" class="twikiLink">Gratia Collector</a> installed
</li> <li> A Hadoop Storage Probe installed and configured to point to the local Gratia Collector
</li></ol> 
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Installation_AN5"></a> Installation </span></h2>
<p />
<p />
<p />
<pre class="rootscreen">
&#91;root&#64;client ~]$ yum install GratiaReporting
</pre>
<p />
Updates can be installed with:
<p />
<pre class="rootscreen">
&#91;root&#64;client ~]$ yum upgrade GratiaReporting
</pre>
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Configuration_AN5"></a> Configuration </span></h2>
<p />
<p />
<p />
This RPM uses Linux-standard file locations.  Here are the most relevant file and directory locations:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table11" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Purpose </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing? </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Location </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Report Configuration </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> /etc/gratia_reporting </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Cron template </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" align="right" valign="top" class="twikiTableCol2 twikiLastCol"> /etc/gratia_reporting/gratia_reporting/gratia_reporting.cron (move to /etc/cron.d) </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Logging Configuration </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> No </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> /etc/gratia_reporting/logging.cfg </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> Log files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLast"> No </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> /var/log/gratia_reporting.log </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<h3><a name="Configuration_file"></a> Configuration file </h3>
Copy the file <code>/etc/gratia_reporting/reporting.cfg</code> to a new filename in <code>/etc/gratia_reporting</code> (for example, <code>/etc/gratia_reporting/reporting_cms.cfg</code>).  You will do this once for every report you want to send out.
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table12" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Attribute </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs Editing </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Value </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> SiteName </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the resource group name of your SE as registered in OIM. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> database </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the database section containing the login details for your Gratia Collector (a few, non-functioning examples sections are included). Installing a Gratia Collector is <a href="/bin/view/Trash/ReleaseDocumentationGratiaSiteCollector" class="twikiLink">covered here</a>, but ask around on osg-hadoop: Nebraska will usually run these reports for you if requested. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> toNames </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Python list for the "to names" for the report email. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> toEmails </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Python list for the "to emails" for the report email. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> smtphost </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Hostname of a SMTP server that accepts email from this host. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> fromName </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Set to the "from name" for the report email. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> fromEmail </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLast"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> Set to the "from email" for the report email. </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<h3><a name="Cron"></a> Cron </h3>
Copy the file <code>/etc/gratia_reporting/gratia_reporting.cron</code> to <code>/etc/cron.d</code>.  There is one line per report; comment out all except the hadoop report.  It is the line containing <code>-n hadoop</code>.  Update the line to point at your new configuration file.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Sample_report"></a> Sample report </span></h2>
<p />
<div class="twistyPlugin twikiMakeVisibleInline">  <span id="twistyIdReleaseDocumentationHadoop20Install3show" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Show Full Output</span></a> </span> <span id="twistyIdReleaseDocumentationHadoop20Install3hide" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyHidden twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Hide Full Output</span></a> </span>  </div><!--/twistyPlugin twikiMakeVisibleInline--> <div class="twistyPlugin"><div id="twistyIdReleaseDocumentationHadoop20Install3toggle" class="twistyRememberSetting twistyStartHide twistyContent twikiMakeHidden twistyInited0">
<p />
<p />
This is a sample report from the Nebraska HDFS instance.
<pre class="file">
============================================================
  The Hadoop Chronicle | 85 % | 2009-09-25
============================================================

--------------------
| Global Storage   |
-----------------------------------------------------
|                  |  Today  | Yesterday | One Week |
-----------------------------------------------------
| Total Space (GB) | 311,470 |   357,818 |  368,711 |
| Free Space (GB)  |  47,304 |    93,719 |  128,391 |
| Used Space (GB)  | 264,166 |   264,100 |  240,320 |
| Used Percentage  |     85% |       74% |      65% |
-----------------------------------------------------
--------------
| CMS /store |
-------------------------------------------------------------------------------------------------------------------------------------
|           Path           | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change | Remaining |
-------------------------------------------------------------------------------------------------------------------------------------
| /store/user              |      771 |            0 | UNKNOWN      | NO QUOTA  |   4,859 |            0 | UNKNOWN      | NO QUOTA  |
| /store/mc                |   95,865 |         -353 | UNKNOWN      | NO QUOTA  |  86,830 |         -171 | UNKNOWN      | NO QUOTA  |
| /store/test              |        0 |            0 | UNKNOWN      | NO QUOTA  |     569 |           25 | UNKNOWN      | NO QUOTA  |
| /store/results           |      237 |            0 | UNKNOWN      | NO QUOTA  |     198 |            0 | UNKNOWN      | NO QUOTA  |
| /store/phedex_monarctest |      729 |            0 | UNKNOWN      | NO QUOTA  |     257 |            0 | UNKNOWN      | NO QUOTA  |
| /store/unmerged          |    3,681 |            3 | UNKNOWN      | NO QUOTA  |  35,687 |           23 | UNKNOWN      | NO QUOTA  |
| /store/CSA07             |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA  |
| /store/data              |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA  |
| /store/PhEDEx_LoadTest07 |        0 |          -21 | UNKNOWN      | NO QUOTA  |       1 |          -22 | UNKNOWN      | NO QUOTA  |
-------------------------------------------------------------------------------------------------------------------------------------

-------------------
| CMS /store/user |
----------------------------------------------------------------------------------------------------------------------------------
|          Path         | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change | Remaining |
----------------------------------------------------------------------------------------------------------------------------------
| /store/user/hpi       |        0 |            0 | UNKNOWN      |     1,099 |      15 |            0 | UNKNOWN      |     9,985 |
| /store/user/gattebury |        0 |            0 | UNKNOWN      |     1,100 |       1 |            0 | UNKNOWN      |     9,999 |
| /store/user/mkirn     |        0 |            0 | UNKNOWN      |     1,100 |       3 |            0 | UNKNOWN      |     9,997 |
| /store/user/spadhi    |       12 |            0 | UNKNOWN      |     1,062 |   1,114 |            0 | UNKNOWN      |     8,886 |
| /store/user/creed     |        0 |            0 | UNKNOWN      |     1,100 |       0 |            0 | UNKNOWN      |    10,000 |
| /store/user/rossman   |        0 |            0 | UNKNOWN      |     1,099 |       5 |            0 | UNKNOWN      |     9,995 |
| /store/user/eluiggi   |        0 |            0 | UNKNOWN      |     1,099 |       6 |            0 | UNKNOWN      |     9,994 |
| /store/user/ewv       |        7 |            0 | UNKNOWN      |     1,081 |     284 |            0 | UNKNOWN      |     9,716 |
| /store/user/test      |        0 |            0 | UNKNOWN      | NO QUOTA  |     167 |            0 | UNKNOWN      |     9,833 |
| /store/user/schiefer  |      751 |            0 | UNKNOWN      |     1,044 |   3,264 |            0 | UNKNOWN      |     6,736 |
----------------------------------------------------------------------------------------------------------------------------------

----------------
| Hadoop /user |
----------------------------------------------------------------------------------------------------------------------------------
|       Path      | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change |    Remaining    |
----------------------------------------------------------------------------------------------------------------------------------
| /user/djbender  |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |
| /user/lhcb      |        0 |            0 | UNKNOWN      |        54 |       0 |            0 | UNKNOWN      | NO QUOTA        |
| /user/dzero     |      897 |            0 | UNKNOWN      |       347 |  89,376 |            0 | UNKNOWN      |         410,624 |
| /user/bloom     |      454 |            0 | UNKNOWN      | NO QUOTA  |   1,410 |            0 | UNKNOWN      | NO QUOTA        |
| /user/uscms01   |  101,384 |         -362 | UNKNOWN      | NO QUOTA  | 129,739 |         -141 | UNKNOWN      | NO QUOTA        |
| /user/cdf       |        0 |            0 | UNKNOWN      | NO QUOTA  |       6 |            0 | UNKNOWN      | 536,870,911,994 |
| /user/osg       |        1 |            0 | UNKNOWN      | NO QUOTA  |       3 |            0 | UNKNOWN      |   5,368,709,117 |
| /user/dweitzel  |       20 |            0 | UNKNOWN      | NO QUOTA  |   2,282 |            0 | UNKNOWN      | NO QUOTA        |
| /user/gattebury |        5 |            0 | UNKNOWN      | NO QUOTA  |  10,002 |            0 | UNKNOWN      | NO QUOTA        |
| /user/brian     |       72 |            0 | UNKNOWN      | NO QUOTA  |   2,697 |            0 | UNKNOWN      | NO QUOTA        |
| /user/usatlas   |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA        |
| /user/powers    |        1 |            1 | UNKNOWN      | NO QUOTA  |     211 |          211 | UNKNOWN      | NO QUOTA        |
| /user/ifisk     |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |
| /user/gpn       |      261 |           -5 | UNKNOWN      |     1,360 |   3,805 |            1 | UNKNOWN      |         996,195 |
| /user/engage    |      461 |          367 | UNKNOWN      | NO QUOTA  |      16 |           13 | UNKNOWN      |         999,984 |
| /user/clundst   |        0 |            0 | UNKNOWN      | NO QUOTA  |       6 |            0 | UNKNOWN      | NO QUOTA        |
| /user/che       |        0 |            0 | UNKNOWN      | NO QUOTA  |      13 |            0 | UNKNOWN      | NO QUOTA        |
| /user/store     |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA        |
| /user/dteam     |        0 |            0 | UNKNOWN      |        53 |      18 |            0 | UNKNOWN      | NO QUOTA        |
| /user/root      |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |
----------------------------------------------------------------------------------------------------------------------------------

-------------
| FSCK Data |
-------------
 Total size:	114592906796932 B (Total open files size: 38923141120 B)
 Total dirs:	41293
 Total files:	295431 (Files currently being written: 38)
 Total blocks (validated):	1356788 (avg. block size 84458962 B) (Total open file blocks (not validated): 297)
 Minimally replicated blocks:	1356788 (100.0 %)
 Over-replicated blocks:	1 (7.370348E-5 %)
 Under-replicated blocks:	0 (0.0 %)
 Mis-replicated blocks:		0 (0.0 %)
 Default replication factor:	3
 Average block replication:	2.2943976
 Corrupt blocks:		0
 Missing replicas:		0 (0.0 %)
 Number of data-nodes:		101
 Number of racks:		1
The filesystem under path '/' is HEALTHY
</pre>
<p />
<p />
</div></div> <!--/twistyPlugin-->
<p />
<h1><a name="Troubleshooting"></a> Troubleshooting </h1>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Hadoop"></a> Hadoop </span></h2>
<p />
To view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser:
<p />
<pre class="file">
http://<font color="#ff0000">namenode.hostname</font>:50070/conf
</pre>
<p />
You will see the entire configuration in XML format, for example:
<p />
<div class="twistyPlugin twikiMakeVisibleInline">  <span id="twistyIdReleaseDocumentationHadoop20Install4show" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Show Full Output</span></a> </span> <span id="twistyIdReleaseDocumentationHadoop20Install4hide" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyHidden twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Hide Full Output</span></a> </span>  </div><!--/twistyPlugin twikiMakeVisibleInline--> <div class="twistyPlugin"><div id="twistyIdReleaseDocumentationHadoop20Install4toggle" class="twistyRememberSetting twistyStartHide twistyContent twikiMakeHidden twistyInited0">
<pre class="file">
&#60;?xml version&#61;&#34;1.0&#34; encoding&#61;&#34;UTF-8&#34; standalone&#61;&#34;no&#34;?&#62;&#60;configuration&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.s3n.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.fs.s3native.NativeS3FileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.cache.levels&#60;/name&#62;&#60;value&#62;2&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;map.sort.class&#60;/name&#62;&#60;value&#62;org.apache.hadoop.util.QuickSort&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-site.xml--&#62;&#60;name&#62;hadoop.tmp.dir&#60;/name&#62;&#60;value&#62;/data1/hadoop//scratch&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.native.lib&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.namenode.decommission.nodes.per.interval&#60;/name&#62;&#60;value&#62;5&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.https.need.client.auth&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;ipc.client.idlethreshold&#60;/name&#62;&#60;value&#62;4000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.system.dir&#60;/name&#62;&#60;value&#62;${hadoop.tmp.dir}/mapred/system&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.data.dir.perm&#60;/name&#62;&#60;value&#62;755&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.tracker.persist.jobstatus.hours&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.namenode.logging.level&#60;/name&#62;&#60;value&#62;all&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.address&#60;/name&#62;&#60;value&#62;0.0.0.0:50010&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.skip.checksum.errors&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.block.access.token.enable&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from Unknown--&#62;&#60;name&#62;fs.default.name&#60;/name&#62;&#60;value&#62;hdfs://nagios.t2.ucsd.edu:9000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.child.tmp&#60;/name&#62;&#60;value&#62;./tmp&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.har.impl.disable.cache&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.skip.reduce.max.skip.groups&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.safemode.threshold.pct&#60;/name&#62;&#60;value&#62;0.999f&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.heartbeats.in.second&#60;/name&#62;&#60;value&#62;100&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.namenode.handler.count&#60;/name&#62;&#60;value&#62;40&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.blockreport.initialDelay&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.jobtracker.instrumentation&#60;/name&#62;&#60;value&#62;org.apache.hadoop.mapred.JobTrackerMetricsInst&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.tasktracker.dns.nameserver&#60;/name&#62;&#60;value&#62;default&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;io.sort.factor&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.timeout&#60;/name&#62;&#60;value&#62;600000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.max.tracker.failures&#60;/name&#62;&#60;value&#62;4&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.rpc.socket.factory.class.default&#60;/name&#62;&#60;value&#62;org.apache.hadoop.net.StandardSocketFactory&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.tracker.jobhistory.lru.cache.size&#60;/name&#62;&#60;value&#62;5&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.hdfs.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.hdfs.DistributedFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.skip.map.auto.incr.proc.count&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.block.access.key.update.interval&#60;/name&#62;&#60;value&#62;600&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.job.complete.cancel.delegation.tokens&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.mapfile.bloom.size&#60;/name&#62;&#60;value&#62;1048576&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.reduce.shuffle.connect.timeout&#60;/name&#62;&#60;value&#62;180000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.safemode.extension&#60;/name&#62;&#60;value&#62;30000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-site.xml--&#62;&#60;name&#62;tasktracker.http.threads&#60;/name&#62;&#60;value&#62;50&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.shuffle.merge.percent&#60;/name&#62;&#60;value&#62;0.66&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.ftp.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.fs.ftp.FTPFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.output.compress&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-site.xml--&#62;&#60;name&#62;io.bytes.per.checksum&#60;/name&#62;&#60;value&#62;4096&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.healthChecker.script.timeout&#60;/name&#62;&#60;value&#62;600000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;topology.node.switch.mapping.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.net.ScriptBasedMapping&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.https.server.keystore.resource&#60;/name&#62;&#60;value&#62;ssl-server.xml&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.reduce.slowstart.completed.maps&#60;/name&#62;&#60;value&#62;0.05&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.reduce.max.attempts&#60;/name&#62;&#60;value&#62;4&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.ramfs.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.fs.InMemoryFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.block.access.token.lifetime&#60;/name&#62;&#60;value&#62;600&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.skip.map.max.skip.records&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.name.edits.dir&#60;/name&#62;&#60;value&#62;${dfs.name.dir}&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.security.group.mapping&#60;/name&#62;&#60;value&#62;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.tracker.persist.jobstatus.dir&#60;/name&#62;&#60;value&#62;/jobtracker/jobsInfo&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-site.xml--&#62;&#60;name&#62;hadoop.log.dir&#60;/name&#62;&#60;value&#62;/var/log/hadoop&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.s3.buffer.dir&#60;/name&#62;&#60;value&#62;${hadoop.tmp.dir}/s3&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.block.size&#60;/name&#62;&#60;value&#62;134217728&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;job.end.retry.attempts&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.file.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.fs.LocalFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.output.compression.type&#60;/name&#62;&#60;value&#62;RECORD&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.local.dir.minspacestart&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.ipc.address&#60;/name&#62;&#60;value&#62;0.0.0.0:50020&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.permissions&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;topology.script.number.args&#60;/name&#62;&#60;value&#62;100&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.mapfile.bloom.error.rate&#60;/name&#62;&#60;value&#62;0.005&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.max.tracker.blacklists&#60;/name&#62;&#60;value&#62;4&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.profile.maps&#60;/name&#62;&#60;value&#62;0-2&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.https.address&#60;/name&#62;&#60;value&#62;0.0.0.0:50475&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-site.xml--&#62;&#60;name&#62;dfs.umaskmode&#60;/name&#62;&#60;value&#62;002&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.userlog.retain.hours&#60;/name&#62;&#60;value&#62;24&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.secondary.http.address&#60;/name&#62;&#60;value&#62;gratia-1:50090&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.replication.max&#60;/name&#62;&#60;value&#62;32&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.tracker.persist.jobstatus.active&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.security.authorization&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;local.cache.size&#60;/name&#62;&#60;value&#62;10737418240&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.min.split.size&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.namenode.delegation.token.renew-interval&#60;/name&#62;&#60;value&#62;86400000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-site.xml--&#62;&#60;name&#62;mapred.map.tasks&#60;/name&#62;&#60;value&#62;7919&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.child.java.opts&#60;/name&#62;&#60;value&#62;-Xmx200m&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.https.client.keystore.resource&#60;/name&#62;&#60;value&#62;ssl-client.xml&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from Unknown--&#62;&#60;name&#62;dfs.namenode.startup&#60;/name&#62;&#60;value&#62;REGULAR&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.queue.name&#60;/name&#62;&#60;value&#62;default&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.tracker.retiredjobs.cache.size&#60;/name&#62;&#60;value&#62;1000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.https.address&#60;/name&#62;&#60;value&#62;0.0.0.0:50470&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.balance.bandwidthPerSec&#60;/name&#62;&#60;value&#62;2000000000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;ipc.server.listen.queue.size&#60;/name&#62;&#60;value&#62;128&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;job.end.retry.interval&#60;/name&#62;&#60;value&#62;30000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.inmem.merge.threshold&#60;/name&#62;&#60;value&#62;1000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.skip.attempts.to.start.skipping&#60;/name&#62;&#60;value&#62;2&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;fs.checkpoint.dir&#60;/name&#62;&#60;value&#62;/var/hadoop/checkpoint-a&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-site.xml--&#62;&#60;name&#62;mapred.reduce.tasks&#60;/name&#62;&#60;value&#62;1543&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.merge.recordsBeforeProgress&#60;/name&#62;&#60;value&#62;10000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.userlog.limit.kb&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;webinterface.private.actions&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.max.objects&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.shuffle.input.buffer.percent&#60;/name&#62;&#60;value&#62;0.70&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;io.sort.spill.percent&#60;/name&#62;&#60;value&#62;0.80&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.map.tasks.speculative.execution&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.util.hash.type&#60;/name&#62;&#60;value&#62;murmur&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.dns.nameserver&#60;/name&#62;&#60;value&#62;default&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.blockreport.intervalMsec&#60;/name&#62;&#60;value&#62;3600000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.map.max.attempts&#60;/name&#62;&#60;value&#62;4&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.job.acl-view-job&#60;/name&#62;&#60;value&#62; &#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.tracker.handler.count&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.client.block.write.retries&#60;/name&#62;&#60;value&#62;3&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.max.reduces.per.node&#60;/name&#62;&#60;value&#62;-1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.reduce.shuffle.read.timeout&#60;/name&#62;&#60;value&#62;180000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.tasktracker.expiry.interval&#60;/name&#62;&#60;value&#62;600000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.https.enable&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.jobtracker.maxtasks.per.job&#60;/name&#62;&#60;value&#62;-1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.jobtracker.job.history.block.size&#60;/name&#62;&#60;value&#62;3145728&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;keep.failed.task.files&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.failed.volumes.tolerated&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.profile.reduces&#60;/name&#62;&#60;value&#62;0-2&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;ipc.client.tcpnodelay&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.output.compression.codec&#60;/name&#62;&#60;value&#62;org.apache.hadoop.io.compress.DefaultCodec&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;io.map.index.skip&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;ipc.server.tcpnodelay&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.namenode.delegation.key.update-interval&#60;/name&#62;&#60;value&#62;86400000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.running.map.limit&#60;/name&#62;&#60;value&#62;-1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;jobclient.progress.monitor.poll.interval&#60;/name&#62;&#60;value&#62;1000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.default.chunk.view.size&#60;/name&#62;&#60;value&#62;32768&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.logfile.size&#60;/name&#62;&#60;value&#62;10000000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.reduce.tasks.speculative.execution&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.tasktracker.outofband.heartbeat&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.s3n.block.size&#60;/name&#62;&#60;value&#62;67108864&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.datanode.du.reserved&#60;/name&#62;&#60;value&#62;10000000000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.security.authentication&#60;/name&#62;&#60;value&#62;simple&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;fs.checkpoint.period&#60;/name&#62;&#60;value&#62;3600&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.running.reduce.limit&#60;/name&#62;&#60;value&#62;-1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.reuse.jvm.num.tasks&#60;/name&#62;&#60;value&#62;1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.web.ugi&#60;/name&#62;&#60;value&#62;webuser,webgroup&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.jobtracker.completeuserjobs.maximum&#60;/name&#62;&#60;value&#62;100&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.df.interval&#60;/name&#62;&#60;value&#62;60000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.tracker.task-controller&#60;/name&#62;&#60;value&#62;org.apache.hadoop.mapred.DefaultTaskController&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.data.dir&#60;/name&#62;&#60;value&#62;/data1/hadoop//data&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.s3.maxRetries&#60;/name&#62;&#60;value&#62;4&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.dns.interface&#60;/name&#62;&#60;value&#62;default&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.support.append&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.job.acl-modify-job&#60;/name&#62;&#60;value&#62; &#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.local.dir&#60;/name&#62;&#60;value&#62;${hadoop.tmp.dir}/mapred/local&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.hftp.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.hdfs.HftpFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.permissions.supergroup&#60;/name&#62;&#60;value&#62;root&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.trash.interval&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.s3.sleepTimeSeconds&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.submit.replication&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.replication.min&#60;/name&#62;&#60;value&#62;1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.har.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.fs.HarFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.map.output.compression.codec&#60;/name&#62;&#60;value&#62;org.apache.hadoop.io.compress.DefaultCodec&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.tasktracker.dns.interface&#60;/name&#62;&#60;value&#62;default&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.namenode.decommission.interval&#60;/name&#62;&#60;value&#62;30&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from Unknown--&#62;&#60;name&#62;dfs.http.address&#60;/name&#62;&#60;value&#62;nagios:50070&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-site.xml--&#62;&#60;name&#62;mapred.job.tracker&#60;/name&#62;&#60;value&#62;nagios:9000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.heartbeat.interval&#60;/name&#62;&#60;value&#62;3&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.seqfile.sorter.recordlimit&#60;/name&#62;&#60;value&#62;1000000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.name.dir&#60;/name&#62;&#60;value&#62;${hadoop.tmp.dir}/dfs/name&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.line.input.format.linespermap&#60;/name&#62;&#60;value&#62;1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.jobtracker.taskScheduler&#60;/name&#62;&#60;value&#62;org.apache.hadoop.mapred.JobQueueTaskScheduler&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.tasktracker.instrumentation&#60;/name&#62;&#60;value&#62;org.apache.hadoop.mapred.TaskTrackerMetricsInst&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.datanode.http.address&#60;/name&#62;&#60;value&#62;0.0.0.0:50075&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;jobclient.completion.poll.interval&#60;/name&#62;&#60;value&#62;5000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.max.maps.per.node&#60;/name&#62;&#60;value&#62;-1&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.local.dir.minspacekill&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.replication.interval&#60;/name&#62;&#60;value&#62;3&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;io.sort.record.percent&#60;/name&#62;&#60;value&#62;0.05&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.kfs.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.fs.kfs.KosmosFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.temp.dir&#60;/name&#62;&#60;value&#62;${hadoop.tmp.dir}/mapred/temp&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-site.xml--&#62;&#60;name&#62;mapred.tasktracker.reduce.tasks.maximum&#60;/name&#62;&#60;value&#62;4&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.replication&#60;/name&#62;&#60;value&#62;2&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.checkpoint.edits.dir&#60;/name&#62;&#60;value&#62;${fs.checkpoint.dir}&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.tasktracker.tasks.sleeptime-before-sigkill&#60;/name&#62;&#60;value&#62;5000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.reduce.input.buffer.percent&#60;/name&#62;&#60;value&#62;0.0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.tasktracker.indexcache.mb&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.job.split.metainfo.maxsize&#60;/name&#62;&#60;value&#62;10000000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.skip.reduce.auto.incr.proc.count&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;hadoop.logfile.count&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.automatic.close&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.seqfile.compress.blocksize&#60;/name&#62;&#60;value&#62;1000000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.hosts.exclude&#60;/name&#62;&#60;value&#62;/etc/hadoop-0.20/conf/hosts&#95;exclude&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.s3.block.size&#60;/name&#62;&#60;value&#62;67108864&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.tasktracker.taskmemorymanager.monitoring-interval&#60;/name&#62;&#60;value&#62;5000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.acls.enabled&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapreduce.jobtracker.staging.root.dir&#60;/name&#62;&#60;value&#62;${hadoop.tmp.dir}/mapred/staging&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.queue.names&#60;/name&#62;&#60;value&#62;default&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.access.time.precision&#60;/name&#62;&#60;value&#62;3600000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.hsftp.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.hdfs.HsftpFileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.tracker.http.address&#60;/name&#62;&#60;value&#62;0.0.0.0:50060&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.reduce.parallel.copies&#60;/name&#62;&#60;value&#62;5&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.seqfile.lazydecompress&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.safemode.min.datanodes&#60;/name&#62;&#60;value&#62;0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;io.sort.mb&#60;/name&#62;&#60;value&#62;100&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;ipc.client.connection.maxidletime&#60;/name&#62;&#60;value&#62;10000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.compress.map.output&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.tracker.report.address&#60;/name&#62;&#60;value&#62;127.0.0.1:0&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.healthChecker.interval&#60;/name&#62;&#60;value&#62;60000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;ipc.client.kill.max&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;ipc.client.connect.max.retries&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.s3.impl&#60;/name&#62;&#60;value&#62;org.apache.hadoop.fs.s3.S3FileSystem&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.job.tracker.http.address&#60;/name&#62;&#60;value&#62;0.0.0.0:50030&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.file.buffer.size&#60;/name&#62;&#60;value&#62;4096&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.jobtracker.restart.recover&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.serializations&#60;/name&#62;&#60;value&#62;org.apache.hadoop.io.serializer.WritableSerialization&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.task.profile&#60;/name&#62;&#60;value&#62;false&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-site.xml--&#62;&#60;name&#62;dfs.datanode.handler.count&#60;/name&#62;&#60;value&#62;10&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;mapred.reduce.copy.backoff&#60;/name&#62;&#60;value&#62;300&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.replication.considerLoad&#60;/name&#62;&#60;value&#62;true&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-default.xml--&#62;&#60;name&#62;jobclient.output.filter&#60;/name&#62;&#60;value&#62;FAILED&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from hdfs-default.xml--&#62;&#60;name&#62;dfs.namenode.delegation.token.max-lifetime&#60;/name&#62;&#60;value&#62;604800000&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from mapred-site.xml--&#62;&#60;name&#62;mapred.tasktracker.map.tasks.maximum&#60;/name&#62;&#60;value&#62;4&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;io.compression.codecs&#60;/name&#62;&#60;value&#62;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec&#60;/value&#62;&#60;/property&#62;
&#60;property&#62;&#60;!--Loaded from core-default.xml--&#62;&#60;name&#62;fs.checkpoint.size&#60;/name&#62;&#60;value&#62;67108864&#60;/value&#62;&#60;/property&#62;
&#60;/configuration&#62;
</pre> 
</div></div> <!--/twistyPlugin-->
<p />
Please refer to <a href="https://twiki.grid.iu.edu/bin/view/Storage/HadoopDebug" target="_top">OSG Hadoop debug webpage</a> and <a href="http://wiki.apache.org/hadoop/FAQ" target="_top">Apache Hadoop FAQ webpage</a> for answers to common questions/concerns
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="FUSE_AN1"></a> FUSE </span></h2>
<p />
<a name="TroubFuseMod"></a>
<h3><a name="Notes_on_Building_a_FUSE_Module"></a> Notes on Building a FUSE Module </h3>
 If you are running a custom kernel, then be sure to enable the <code>fuse</code> module with <code>CONFIG_FUSE_FS=m</code> in your kernel config.  Building and installing a <code>fuse</code> kernel module for your custom kernel is beyond the scope of this document.
<p />
<strong>Note:</strong> If you cannot find a <code>fuse</code> kernel module to match your kernel, ATRPMs has a <a href="http://people.atrpms.net/~pcavalcanti/LCG_kernel_modules.html" target="_top">guide for using their RPM spec files</a> in order to generate a module.  That page mostly works, although sections are a bit out dated.  Contact the <a href="mailto&#58;osg&#45;hadoop&#64;opensciencegrid&#46;org">osg-hadoop&#64;opensciencegrid.org</a> list if you need help.
<p />
<p />
<p />
<a name="TroubFuseDeb"></a>
<h3><a name="Running_FUSE_in_Debug_Mode"></a> Running FUSE in Debug Mode </h3>
<p />
<p />
<p />
To start the FUSE mount in debug mode, you can run the FUSE mount command by hand:
<p />
<pre class="rootscreen">
[root@client ~]$  /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server=<font color="#ff0000">namenode.host</font>,port=9000,rdbuffer=131072,allow_other -d
</pre>
<p />
Debug output will be printed to stderr, which you will probably want to redirect to a file.  Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="GridFTP"></a> GridFTP </span></h2>
<p />
<a name="GridFTPStand"></a>
<h3><a name="Starting_GridFTP_in_Standalone_M"></a> Starting GridFTP in Standalone Mode </h3>
<p />
 If you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command:
<p />
<pre class="rootscreen">
[root@client ~]$ gridftp-hdfs-standalone
</pre>
<p />
The standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr.
<p />
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Known_Issues"></a> Known Issues </span></h2>
<p />
<h3><a name="copy_FromLocal_java_IOException"></a> copyFromLocal java IOException </h3>
<p />
When trying to copy a local file into Hadoop you may come across the following java exception:
<p />
<pre class="screen">
<div class="twistyPlugin twikiMakeVisibleInline">  <span id="twistyIdReleaseDocumentationHadoop20Install5show" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Show Full Output</span></a> </span> <span id="twistyIdReleaseDocumentationHadoop20Install5hide" class="twistyRememberSetting twistyStartHide twistyTrigger twikiUnvisited twistyHidden twistyInited0"><a href="#"><img src="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" border="0" alt="" /><span class="twikiLinkLabel twikiUnvisited">Hide Full Output</span></a> </span>  </div><!--/twistyPlugin twikiMakeVisibleInline--> <div class="twistyPlugin"><div id="twistyIdReleaseDocumentationHadoop20Install5toggle" class="twistyRememberSetting twistyStartHide twistyContent twikiMakeHidden twistyInited0">
11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0]
nodes == null
11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file
"/osg/ddd" - Aborting...
copyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0
nodes, instead of 1
11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd :
org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only
be replicated to 0 nodes, instead of 1
        at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313)
</div></div> <!--/twistyPlugin-->
</pre>
<p />
This can occur if you try to install a Datanode on a machine with less than 10GB of disk space available.  This can be changed by lowering the value of the following property in <code>/usr/lib/hadoop-0.20/conf/hdfs-site.xml</code>:
<p />
<pre class="file">
&#60;property&#62;
  &#60;name&#62;dfs.datanode.du.reserved&#60;/name&#62;
  &#60;value&#62;10000000000&#60;/value&#62;
&#60;/property&#62;
</pre>
<p />
Hadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.
<p />
<h1><a name="References"></a> References </h1>
<p /> <ul>
<li> <a href="https://twiki.grid.iu.edu/bin/view/Trash/ReleaseDocumentationHadoopInstallationHandsOn" target="_top">Hadoop Hands On Tutorial</a>.
</li> <li> <a href="/bin/view/Storage/HadoopUpgrade" class="twikiLink">Instructions for Upgrading from Hadoop 0.19 to Hadoop 0.20</a>
</li></ul> 
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Benchmarking"></a> Benchmarking </span></h2>
<p /> <ul>
<li> <a href="http://www.iop.org/EJ/article/1742-6596/180/1/012047/jpconf9_180_012047.pdf" target="_top">Using Hadoop as a Grid Storage Element</a>, <i>Journal of Physics Conference Series, 2009</i>.
</li> <li> <a href="http://osg-docdb.opensciencegrid.org/0009/000911/001/Hadoop.pdf" target="_top">Hadoop Distributed File System for the Grid</a>, <i>IEEE Nuclear Science Symposium, 2009</i>.
</li></ul> 
<p />
<h1><a name="Comments"></a> <strong>Comments</strong> </h1>
<form method="post" action="https://twiki.opensciencegrid.org/bin/save/ReleaseDocumentation/Hadoop20Install" enctype="multipart/form-data" name="tableappend0" id="tableappend0">
<p />
<div class="commentPlugin commentPluginPromptBox">
<table><tr valign="middle"><td><textarea  rows="3" cols="70" name="comment" wrap="soft" onfocus="if(this.value=='')this.value=''" onblur="if(this.value=='')this.value=''"></textarea></td><td><input  type="submit" value="Add comment" /></td></tr></table>
</div><!--/commentPlugin-->
<p />
<input type="hidden" name="comment_action" value="save"  />
<input type="hidden" name="comment_type" value="tableappend"  />
<input type="hidden" name="comment_index" value="0"  /></form>
<p />
<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 <code><b>===============</b></code>
<p />
 Thank you for claiming ownership for this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/ReleaseDocumentation/FirstLast?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local OWNER = <span class="twikiNewLink">JeffDost<a href="/bin/edit/ReleaseDocumentation/JeffDost?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="JeffDost (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (<span class="twikiNewLink">ComputeElement<a href="/bin/edit/ReleaseDocumentation/ComputeElement?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="ComputeElement (this topic does not yet exist; you can create it)">?</a></span>|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3) <ul>
<li> Local DOC_AREA       = Storage
</li></ul> 
<p />
 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (<span class="twikiNewLink">EndUser<a href="/bin/edit/ReleaseDocumentation/EndUser?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="EndUser (this topic does not yet exist; you can create it)">?</a></span>|Student|Developer|SysAdmin|VOManager) <ul>
<li> Local DOC_ROLE       = <span class="twikiNewLink">SysAdmin<a href="/bin/edit/ReleaseDocumentation/SysAdmin?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="SysAdmin (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge) <ul>
<li> Local DOC_TYPE       = Installation  Please define if this document in general needs to be reviewed before release ( 1 | 0 )
</li> <li> Local INCLUDE_REVIEW = 1
</li></ul> 
<p />
 Please define if this document in general needs to be tested before release ( 1 | 0 ) <ul>
<li> Local INCLUDE_TEST   = 1
</li></ul> 
<p />
 change to 1 once the document is ready to be reviewed and back to 0 if that is not the case <ul>
<li> Local REVIEW_READY   = 1
</li></ul> 
<p />
 change to 1 once the document is ready to be tested and back to 0 if that is not the case <ul>
<li> Local TEST_READY     = 1
</li></ul> 
<p />
 change to 1 only if the document has passed the review and the test (if applicable) and is ready for release <ul>
<li> Local RELEASE_READY  = 0
</li></ul> 
<p />
<p />
 DEAR DOCUMENT REVIEWER
 <code><b>==================</b></code>
<p />
 Thank for reviewing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/ReleaseDocumentation/FirstLast?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local REVIEWER       =  <span class="twikiNewLink">DouglasStrain<a href="/bin/edit/ReleaseDocumentation/DouglasStrain?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="DouglasStrain (this topic does not yet exist; you can create it)">?</a></span> Please define the review status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 )
</li> <li> Local REVIEW_PASSED  = 2
</li></ul> 
<p />
<p />
 DEAR DOCUMENT TESTER
 <code><b>================</b></code>
<p />
 Thank for testing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/ReleaseDocumentation/FirstLast?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local TESTER         = <span class="twikiNewLink">NehaSharma<a href="/bin/edit/ReleaseDocumentation/NehaSharma?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="NehaSharma (this topic does not yet exist; you can create it)">?</a></span> Please define the test status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 )
</li> <li> Local TEST_PASSED    = 2
</li></ul> 
############################################################################################################
--></div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: ReleaseDocumentation<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span><span class="twikiNewLink">Trash/Trash/Integration<a href="/bin/edit/Trash/Trash/Integration/WebHome?topicparent=ReleaseDocumentation.Hadoop20Install" rel="nofollow" title="Trash/Trash/Integration (this topic does not yet exist; you can create it)">?</a></span> &gt; <a href="/bin/view/ReleaseDocumentation/WebHome" class="twikiCurrentWebHomeLink twikiLink">WebHome</a><span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>Hadoop20Install</span> <br />    
Topic revision: r43 - 07 Feb 2017 - 19:00:46 - <a href="/bin/view/Main/BrianBockelman" class="twikiLink">BrianBockelman</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
+ OSG Area Coordinators Meeting

---++ Meeting Phone Coordinates

%TABLE{tablewidth=&quot;550&quot; cellpadding=&quot;6&quot; dataalign=&quot;left&quot; tablerules=&quot;all&quot; tableborder=&quot;1&quot; databg=&quot;#FFFFFF, #FFFFFF&quot;}%
| Wednesday,  2:00 PM Central Time |
| Dial-In: (866) 740-1260; meeting ID 840-5618, followed by # |
| !VoIP: http://www.readytalk.com/?ac=8405618  |



---++ Attendees
Brian, Emily for Rob G., Tim C., Tim T., Kateherine, Chander, Bo, Shawn, Rob Q, Gabriele


---++ Top of Meeting Business

---+++ New Items for [[https://indico.fnal.gov/categoryDisplay.py?categId=86][OSG Calendar]], 
---+++ Current plan for [[https://twiki.grid.iu.edu/bin/view/Management/NewsletterArticles][Newsletter Articles]]
Please note that all newsletter articles are due around the middle of the month before the targeted newsletter.

**We want to get an update on the area goals we had established at the staff retreat; these goals are available at https://osg-docdb.opensciencegrid.org:440/cgi-bin/ShowDocument?docid=1217

Also please remember that the next OSG annual report for each area will explicitly state the goals and the results.


---++ Technology - Brian Bockelman
  * [[%ATTACHURL%/OSG-Area-Coordinators-Jan-20.pdf][OSG-Area-Coordinators-Jan-20.pdf]]: Technology update

---++ Stakeholder Request Management - Gabriele Garzoglio

https://jira.opensciencegrid.org/secure/Dashboard.jspa?selectPageId=10030

---++ Minutes
Key Initiatives
   1. CVMFS-over-StashCache development is complete; plan release by March and widespread availability by July. Files metadata are served from OASIS, data from StashCache.
   2. OASIS: planning to release improved banning of repository in March. Aiming for OASIS to become a configuration of CVMFS in 12 mo.
   3. Considering to drop OSG-CE-BOSCO for lack of demand

Top Issues / Concerns
   1. NOvA use of StashCache in production pushed to winter 2016. CVMFS-over-StashCache makes it simpler to use StashCache.

Key Accomplishments
   1. LIGO used ~4M CPU h on OSG: ~400K of total was on Stampede (of their 2M h allocation). Store 5 TB / Read 1 PB at Nebraska.
   2. 58 HTCondor CE registered in OSG. HTCondor technology is being discussed and utilized at CERN.




-- Main.JemiseLockhart - 17 Nov 2015


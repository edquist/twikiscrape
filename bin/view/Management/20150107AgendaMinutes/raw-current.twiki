---+ OSG Area Coordinators Meeting

---++ Meeting Phone Coordinates

%TABLE{tablewidth=&quot;550&quot; cellpadding=&quot;6&quot; dataalign=&quot;left&quot; tablerules=&quot;all&quot; tableborder=&quot;1&quot; databg=&quot;#FFFFFF, #FFFFFF&quot;}%
| Wednesday,  2:00 PM Central Time |
| Dial-In: (866) 740-1260; meeting ID 840-5618, followed by # |
| !VoIP: http://www.readytalk.com/?ac=8405618  |


---++ Attendees
Chander, Gabriele, Tim T., Rob Q., Rob G., Clemmie


---++ Top of Meeting Business

---+++ New Items for [[https://indico.fnal.gov/categoryDisplay.py?categId=86][OSG Calendar]], 
---+++ Current plan for [[https://twiki.grid.iu.edu/bin/view/Management/NewsletterArticles][Newsletter Articles]]
Please note that all newsletter articles are due by the 20th of each month.

---++ Campus Grids - Rob Gardner

---+++ Key initiatives

   * Distributed Environment Modules *update* &lt;pre&gt;[rwg@login01 ~]$ module avail

------------------ /cvmfs/oasis.opensciencegrid.org/osg/modules/modulefiles/Core -------------------
   R/3.1.1                  ectools                   java/8u25       (D)    python/2.7        (D)
   SitePackage              espresso/5.1              jpeg                   python/3.4
   SparseSuite/4.2.1        expat/2.1.0               lammps/2.0             qhull/2012.1
   ant/1.9.4                ffmpeg/0.10.15            lapack                 root/5.34-21
   apr/1.5.1                ffmpeg/2.5.2       (D)    libffi/3.2.1           ruby/2.1
   apr-util/1.5.3           fftw/3.3.4-gromacs        lmod/5.6.2             samtools/0.1.17
   aprutil/1.5.3            fftw/3.3.4         (D)    madgraph/2.1.2         scons/2.3.4
   atk/1.30.0               fpc/2.6.4                 madgraph/2.2.2  (D)    sdpa/7.3.8
   atlas                    gamess/2013               matlab/2013b           serf/1.37
   autodock/4.2.6           gcc/4.6.2                 matlab/2014a    (D)    settarg/5.6.2
   bedtools/2.21            geos/3.4.2                mercurial/1.9.1        shrimp/2.2.3
   blasr/1.3.1              git/1.9.0                 mplayer/1.1            siesta/3.2
   blast                    glib/2.28.8               mrbayes/3.2.2          subversion/1.8.10
   blender                  glpk/4.54                 muscle/3.8.31          sundials/2.5
   boost/1.50.0             gnome-libs/1.0            namd/2.9               swift/0.94.1
   boost/1.56        (D)    gnuplot/4.6.5             nco/4.3.0              tcl/8.6.2
   bowtie/2.2.3             graphviz/2.38.0           netcdf/4.2.0           uclust/2.22
   bwa/2014                 gromacs/4.6.5             octave/3.8.1           udunits/2.2.17
   cairo/1.8.8              gromacs/5.0.0      (D)    openbabel/2.3.2        valgrind/3.10
   canopy/1.4.1             gtk/2.24.23               papi/5.3.2             vmd/1.9.1
   casino/2.13.211          hdf5/1.8.9                pbsuite/14.9.9         wget/1.15
   cdo/1.6.4                hdf5/1.8.12               pcre/8.35              xrootd/4.1.1
   cmake/3.0.1              hdf5/1.8.13        (D)    pixman/0.32.4
   cp2k/2.5.1               hmmer/3.1                 proot/2014
   curl/7.37.1              java/7u71                 protobuf/2.5&lt;/pre&gt;

   * More development on &quot;tutorial&quot; command *update*
      * tutorial modules maintained in github and connect-book
      * Available on login.osgconnect.net:&lt;pre&gt;$ tutorial
usage: tutorial list                 - show available tutorials
       tutorial info &lt;tutorial-name&gt; - show details of a tutorial
       tutorial &lt;tutorial-name&gt;      - set up a tutorial

Currently available tutorials: 
R ..................... Estimate Pi using the R programming language
cp2k .................. How-to for the electronic structure package CP2K
dagman-namd ........... Launch a series of NAMD simulations via Condor DAG
error101 .............. Use condor_q -better-analyze to analyze stuck jobs
exitcode .............. Use HTCondor&#39;s periodic_release to retry failed jobs
htcondor-transfer ..... Transfer data via HTCondor&#39;s own mechanisms
namd .................. Run a molecular dynamics simulation using NAMD
nelle-nemo ............ Running Nelle Nemo&#39;s goostats on the grid
oasis-parrot .......... Software access with OASIS and Parrot
octave ................ Matrix manipulation via the Octave programming language
pegasus ............... An introduction to the Pegasus job workflow manager
photodemo ............. A complete analysis workflow using HTTP transfer
quickstart ............ How to run your first OSG job
root .................. Inspect ntuples using the ROOT analysis framework
scaling ............... Learn to steer jobs to particular resources
scaling-up-resources .. A simple multi-job demonstration
software .............. Software access tutorial
stash-chirp ........... Use the chirp I/O protocol for remote data access
stash-http ............ Retrieve job input files from Stash via HTTP
stash-namd ............ Provide input files for NAMD via Stash&#39;s HTTP interface
swift ................. Introduction to the SWIFT parallel scripting language

Enter &quot;tutorial name-of-tutorial&quot; to clone and try out a tutorial.&lt;/pre&gt;

   * Connect client development
      * Goal is to give campus grids a client tool:
         * Pre-installed, configured, and supported by local campus cluster admin
         * Submission to OSG via grid universe
         * Submission to local scheduler via vanilla universe, using local campus allocation
         * Plus &quot;connect&quot; and &quot;tutorial&quot; commands
         * As a modules command: allow switching between local modules collection and OASIS modules
   * !StashCache development - Anna Olson
      * A distributed data caching service for OSG Connect Stash
      * Instructions to install a cache endpoint:  https://confluence.grid.iu.edu/display/STAS/Installing+an+XRootD+server+for+Stash+Cache
      * Stash data now exposed via Xrootd (read only)
      * Set of caches deployed: two inside EC2 zones (California, Virginia); one at Chicago, one at Indiana University (IUPUI). 
      * client tool =stashcp= developed to abstract it away Xrootd from users (will be distributed in OASIS)
         * Tries the nearest cache first (use a geo-ip and lookup table)
         *  If fails, pulls from the &quot;trunk&quot; - i.e. Stash.
      * Condor log files parsed to glean pattern of OSG Connect file transfer usage (covers internal Condor transfers)
         * Finding most users apparently are using wget or curl.
      * Testing using =stashcp=.  Test files. Scale so far: files less than 1 GB (1 kB to 1 GB stored in Stash, job pull randomly), and 100 jobs per hour, each job pulling one file. 
      * Our next step is to scale up our testing to see how xrootd deals with increased load.  We’ll be inducing churning in the xrootd caches, and we’ll be pulling lots of files at once.  We expect to have preliminary results early next week.

   * Consultancy - Bala
      * Prepared a very long time NAMD simulation as a demonstrator for DAGMAN and Pegasus usage.  Working with Mats.  Both are now running.  Intended scale: 1M jobs requiring 3 days.  The key motivation for implementing and providing workflows that allow potential users to utilize applications such as =gromacs= that require long runtimes in a DHTC environment like OSG.  Users would be able to take workflow examples that we provide, change a few arguments and input files in the workflow and then run simulations that may take significant amounts of time without having to worry about checkpointing their applications and then transferring and restarting their application as a new job. This would allow some classes of potential users to utilize OSG as a free opportunistic alternative to resources on XSEDE or other HPC resources.
      * Will include in both OSG !ConnectBook and Software Carpentry materials.
      * Working on OSG Connect User Guide (a quick reference to complement the !ConnectBook which is more pedagogical (wordy).
      * Software Carpentry + OSG Campus Bootcamp at Indiana University (March 2-6 at IUPUI) planning with !RobQ
      * Wisconsin visit next week (SWC and ACI-Ref w/ Lauren Michael)
      * SWC-OSG workshops to target: Duke, Syracuse 

   * Operations
      * All services exceedingly stable during holiday break
      * Continued Ceph-RBD testing and development with the Ceph team which does occasionally interrupt Stash service.
      * Plan in place to transition OSG Connect account requests and support issues to GOC ticketing system.

---+++ Top Issues/Concerns
   * Preparing for SWC-OSG workshop for IUPUI
   * Campus grid client - some necessary re-thinking of the Bosco here, will require splitting services.
   


---+++ Key Accomplishments
   * Flocking to !OrangeGrid via OSG Connect in production
   * Demonstration of local campus client, and its limitations using UChicago campus grid, http://ci-connect.uchicago.edu/. 
   * Software Carpentry + OSG Campus Bootcamp at UChicago: http://swc-osg-workshop.github.io/2014-12-15-UChicago/; Kyle and Soichi in attendance.
   * Supported very successful ACI-REF webinar covering OSG Connect usage (Emelie Harstad)
  

---++ Update on Fermilab SCD re-org and how it affects OSG - Gabriele

---++ Stakeholder Request Management - Gabriele Garzoglio

https://jira.opensciencegrid.org/secure/Dashboard.jspa?selectPageId=10030

---++ Minutes

Key Initiatives
   1. User environment: goal to make all XSEDE modules available (currently 97 software modules installed); improving &quot;tutorial command&quot; and Connect client.
   2. Distributed data caching service for OSG Connect Stash; distribution based on xrootd.

Top Issues / Concerns
   1. Preparing for the Software Carpentry workshop
   2. Connect client: may require splitting BOSCO up

Key Accomplishments
   1. Integrated OrangeGrid with OSG Connect
   2. Software Carpentry + OSG Connect bootcamp in Dec at UChicago.
   3. Covered OSG Connect  at the ACI-REF webinar


-- Main.GabrieleGarzoglio - 07 Jan 2015

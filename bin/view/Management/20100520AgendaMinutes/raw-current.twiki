---+ OSG Area Coordinators Meeting
---+++ Meeting Coordinates

| &lt;b&gt;Date&lt;/b&gt; | Thursday, May 20, 2010 |
| &lt;b&gt;Time&lt;/b&gt; | Noon Central |
| &lt;b&gt;Telephone Number&lt;/b&gt; | 510-665-5437 |
| &lt;b&gt;Teleconference ID&lt;b&gt; | 2222  |


---+++ Attendees
Abhishek Rana, Maxim Potekhin, Igor Sfiligoi, Mine Altunay, Alain Roy, Kent Blackburn, Peter Doherty, Rob Quick, Tanya Levshina, Brian Bockelman, John McGee, Chander Sehgal

---++ Agenda

   * 1.4 VOs – Abhishek Rana _(confirmed)_

   * 1.5 Engagement - John McGee

   * 1.7 Security – Mine Altunay (confirmed) 

   * [[%ATTACHURL%/Plan_for_June_2010_OSG_NSF_Report.doc][Plan for June 2010 OSG NSF Annual Report]] - Chander Sehgal


---++ 1.4 Virtual Organizations

---+++ Facilitated VO participation for peer Areas

   * Regular Weekly VO Forum; venue to collect input from VOs, and for discussions between VOs and OSG Areas. 
   * VO Job Problems compilation with Jim W and Dan.
      * https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/DirectFeedbackOnProductionProblemsOrBottlenecks
      * https://twiki.grid.iu.edu/bin/view/Production/ProblemsEncounteredByVOsDuringJobSubmission
   * Familiarizing VOs with Software Evolution Proposals/SEPs.
   * Production resources prediction and planning with Dan 
      * https://twiki.grid.iu.edu/bin/view/Production/OSGResourcePredictions
      * Annual estimates for 2010 based on direct VO input.
   * Bi-annual collection of stakeholder scientific publications is in progress with David.
   * Working to strengthen liaison with Collective VOs Council Representatives.

---+++ Ground work and impact on VOs production

   * *D0* monte-carlo production remains cyclic; fluctuated between 5 to 10 Million events/week. Achieved a new annual peak of 13.3 Million Events/week. This is also the highest all-time peak noted in D0 collaboration so far. Focus has been on increasing efficiency at important sites; D0 and VOs Group analyzing site issues every week. | [[https://twiki.grid.iu.edu/twiki/pub/Trash/Trash/Trash/Trash/VirtualOrganizations/JointTaskForces/D0_Event_Production_volume_on_OSG.ppt][Peaks Tracking]]

   * *CDF* and *Fermilab-VO* production remains steady. Regular communication of operational feedback and requirements from Fermilab-VO to VOs Group, to peer Areas. Usage projections for Fermilab-VO and sub-VOs are now available for calendar years 2010, 2011, 2012.

   * Ongoing work with *SBGrid/NEBioGrid* to sustain an increased scale of production. Getting good concurrent job runs; but efficiency is not consistent from day to day. Peak 70,000 hours/day; usage in bursts; average 8-10,000 hours/day. Massive overloading issues at FNAL T1 and UNL sites (many factors: cleaning NFS ~/.globus/.gass_cache directory; using curl; recursive wrapper calls). Resolved now. Continuing to use OSG/Engage !MatchMaker for production. Feedback: need for client-side functionality to remotely diagnose job failures. E.g., whether jobs are pre-empted, and why. Starting to investigate !GlideinWMS. Plans are to also evaluate Panda; targeting 8 sites. Multiple factors evaluated/addressed; thus, progress steady but slow.

   * Moving *IceCube* from testing to production. Planning to use UCSD, UNL, GLOW. !IceCube and GLOW adding a dedicated submit infrastructure node. Using Condor DAGs, !GlideinWMS, Squid. Synopsis of accomplished changes in workflow: Jobs refactored with DAGMan to read from Photonic tables in parallel steps; Multiple jobs in the same glide-in; Each worker node processes a single photonics bin (subset of the full table); Multiple jobs can re-use the worker node reducing the traffic of copying the bin.
   
   * Facilitated usage of *GLUE-X* site UConn-OSG; now in use by Engage, GEANT4, GPN, LIGO, SBGrid, GLOW. Pointed out bugs/improvements in !MyOSG and GIP | [[https://ticket.grid.iu.edu/goc/viewer?id=8343][ticket]]. Working to reach closure. GLUE-X is interested in offering SRM/dCache opportunistic storage to other VOs; facilitating D0 now. Glideins from GPN VO were failing at GLUE-X site; Related to bug in Condor; condor team notified. Interested in trying out !GlideinWMS. Next step for GLUE-X workflow management - need to decide on a scheme for job dispatch. Abhishek &amp; Dan discussing options with Richard Jones. Policy related feedback: Lack of OSG policy on POSIX account requirements; a site&#39;s support for multiple VOs; multiple sites&#39; support for a specific VO. GLUE-X&#39;s own application/production to follow in coming months. 

   * Sustained *nanoHUB* production at 200-700 wall hours/day. 5+ types of nanotechnology applications. Opened some &#39;end-user&#39; production jobs; &#39;probe&#39; jobs ongoing as earlier. OSG accounting view -  Probe Jobs: &quot;/CN=nanoHUB Service07&quot;, Application Testing: &quot;/CN=nanoHUB Service06&quot;, End-users Production runs &quot;/CN=nanoHUB Service02&quot;. nanoHUB accounting view - reporting websites that correspond to these categories are: [[http://nanohub.org/usage/gridprobe][Probe]] | [[http://nanohub.org/usage/gridapptest][AppTest]] | [[http://nanohub.org/usage/gridappprod][Production]]. Problems were noticed in DAGMan after upgrading submit/workflow infrastructure to Condor 7.4.1; recurring schedd crashes; multiple copies of the same jobs were being submitted; resulted in apparent peaks in consumption. Condor team supplied fix as a patched binary; evaluated and deployed by nanoHUB.

   * Good success with *DOSAR* and *GridUNESP*; researchers running MPI jobs through full site/VO infrastructure; running on OSG. | [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/DOSAR_GridUNESP_OSG][Experiences Blog]]

   * *CompBioGrid* Local site running at full capacity; continuing to provide cycles for Engage and SBGrid. Virtual Cell integration being planned. | [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/CompBioGrid_OSG][Experiences Blog]]

   * *GPN* and *GROW* have active sites; but scale is very moderate.
 
   * *CIGI, GRASE, NWICG* valuable partners in Consortium; but science production is moderate. Plans need to be encouraged.

   * Restarted work with *CHARMM* team. Production running on OSG now; publication expected in coming months. Running under *OSG-VO* using VOMS proxy. Actively doing production and getting results. Studying molecular dynamics simulations of mutant proteins. Have a few more models to run, then get results to JHU collaborators and get feedback. Using Panda; has been steady and reliable. VO Group in contact with Tim Miller and Maxim to track any resulting scientific publication.

   * *GEANT4* production accounting discrepancy (as reported in previous Area Coordinator meeting) in wasted wall hours. 50% wasted in OSG resource-view, 0% wasted in Geant4-view. Reason likely to be OSG-side issue: exit-code discrepancy due to Pilots. Remains an item of concern.

   * New VOs: Discussions with !DayaBay, HCC. PEGrid expected.

---+++ Continued general concerns pointed out by VOs

   * Looking for more advice on Globus 5&#39;s adoption timeline; for VOs preparedness.
   * Workload Management solutions and choices.
   * Opportunistic storage availability.
   * Accounting discrepancies - e.g., uncatched/mismatched exit codes, Pilots.
   * Lack of real-time job status monitoring.  
   * Lack of accuracy in advertisement of heterogenous subcluster parameters.
      * Fermilab VO implemented GIP configuration changes; working to get documentation; expected later this year.
   * Preemption/Eviction remains to be fully addressed | 
 [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/PreemptionHandling_OSG][More background]].   

---++ 1.5 Engagement
---+++Ongoing Items last reported on Feb 11, 2010

   * hiring: have selected a candidate, procedural issues with UNC hr requiring a repost of the position, will be able to make an offer in 30 days.
   * usage demand remains reasonably strong; increasing demand for HTPC

 
---+++ New issues
   * TG10 Tutorial was accepted 
   * Notre Dame CI Days tutorial completed, article submitted for OSG newsletter
   * working closely with WUSTL: call with Tanya, conversations with BrianB. Issue is large amount of data that needs to be available at the CE for computations (eg BLAST databases). severly limits the number of OSG sites this work can run on.
   * Have requested papers/citations from user community in prep for June 2010 NSF report. Have received two from Jose Caballero, though it&#39;s not entirely clear if these are Engagement. they are citations however.
   * have been having some issues with one users code segfaulting and difficulty in communicating with them, in final round of communication before elevating to talk with their PI
   * Interested in gathering input, ideas, feedback, comments on Next Gen OSG Engagement 




---++ 1.7 Security
---+++Ongoing Items last reported on Feb25, 2010

   * The new certificate layout from IGTF. We made a plan for ITB testing. the new CA package have been created and put into ITB testing. It will be tested by the robot job submission system (panda). Currently, rsv and some cert scripts have been broken. 
   * Pakiti testing has been completed. The code that provides access control per sys admin is completed and tested. We are waiting to check out the official release version. We tested with the development repository. 
   * We held the successive ID Mgmt meeting at OSG All Hands Meeting. As a short term solution, we decided to prepare an OSG specific certificate request web page. We completed the page and we are testing the page on different browsers. We made a survey of RA Agents to understand desired features of the new page. As a result, we incorporated the VO specific sponsor information on the page. In the past, an end user was supposed to know this information and they struggled identifying their sponsors. Our next steps are to test the usability of the page with VO agents. If the results turn out to be positive, we will direct OSG users to this web page rather than DOEGrids web page. 
   * End-end authentication infrastructure. We started writing the authentication roadmap for the next cycle of the OSG. Met with CMS web group which is building an openID based solution. 
   * Worked with DOEgrids CA mgmt to create a joint workplan for Doug&#39;s efforts. Doug has 50% at ESNet and 50% at OSG. 
   * CRL distribution problem. FNAL is experimenting with running a central distribution server. We will wait until they gather some results. 
 
---+++ New issues
   * GUMS testing. Dave Stampf is coming to FNAL during the week June 21. I will also start building the code with Maven to understand the process. 
   * We are visiting SBgrid in June to work on their authentication infrastructure. We found ways to further reduce their registration workflow. 
   * Documentation work is continuing. Barlow is leading this effort
   * We performed a security audit of NEBioGrid per executive director&#39;s request. The results are published at docdb.  
   * A new security challenge is performed by EGEE against atlas infrastructure. We are working to understand what is tested and how. 


   
-- Main.ChanderSehgal - 10 May 2010


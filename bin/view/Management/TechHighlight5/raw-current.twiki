---+ High-Tech Barn Raising

&lt;object width=&quot;640&quot; height=&quot;360&quot;&gt;&lt;param name=&quot;movie&quot; value=&quot;http://www.youtube.com/v/GQxUCNfD_pc&amp;hl=en_US&amp;feature=player_embedded&amp;version=3&quot;&gt;&lt;/param&gt;&lt;param name=&quot;allowFullScreen&quot; value=&quot;true&quot;&gt;&lt;/param&gt;&lt;param name=&quot;allowScriptAccess&quot; value=&quot;always&quot;&gt;&lt;/param&gt;&lt;embed src=&quot;http://www.youtube.com/v/GQxUCNfD_pc&amp;hl=en_US&amp;feature=player_embedded&amp;version=3&quot; type=&quot;application/x-shockwave-flash&quot; allowfullscreen=&quot;true&quot; allowScriptAccess=&quot;always&quot; width=&quot;640&quot; height=&quot;360&quot;&gt;&lt;/embed&gt;&lt;/object&gt;
On May 5th...
They arrive
One thousand machines
One day
Installation day
The day we rack
May 5, 2008

Haiku from the Installation day video. Video copyright of ITaP, Purdue University.
In a kind of high-tech barn raising on May 5, Purdue University staff and friends assembled their newest addition to the Open Science Grid and TeraGrid: the Steele cluster.

The event even attracted volunteers from Indiana University, Purdue’s Big Ten athletic rival, to unpack, install and test components.  A community contribution in more ways than one, over two dozen Purdue faculty members had pooled research money to pay for it.

“The goal was to build the cluster in a day and we basically had it done by lunch,” says Preston Smith, senior Unix system administrator for Purdue’s Rosen Center for Advanced Computing. Past barn raisings of these 18 wheeler-sized arrays have taken days. Assembling a cluster usually takes weeks.

Bigger, faster

Steele is one of a handful of facilities in the U.S. integrated with the two principal government-funded grid systems for scientific computing, OSG and TeraGrid. It replaces the older Lear cluster at Purdue, parts of which will be reassembled at Purdue’s Calumet and Fort Wayne satellite campuses. Steele has more than six times the processing power of Lear and at maximum speed is expected to reach 60 trillion operations per second.

 “We got the new cluster running on the grid within a couple of hours,” says Purdue physics professor Norbert Neumeister. Smith says that it happened quickly for the same reasons the cluster itself came together in a few hours: careful planning and the automation recently built into the clustering and grid install procedures.
 

Clockwise from upper left: Phil Cheeseman carries boxes; Donna Cumberland coordinates movement of parts from staging area to the machine room; Tammy Clark and Jim Schmitz unbox components; David King helps set up the cluster in the machine room.



&quot;Steeled&quot; for the LHC deluge

One Steele “investor”—and user—is the CMS particle physics experiment Tier-2 center at Purdue, which makes Steele available to colleagues on campus and around the world through the Open Science Grid.

The CMS Tier-2 centers support the computational needs of CMS physicists, both for physics analyses and simulations. It and the other six Tier-2 centers in the U.S. work in concert with the Tier-0 at CERN and the half-dozen international Tier-1 centers, primarily the U.S. Tier-1 at Fermilab. The world-wide Tier structure will manage and distribute the deluge of data expected from the CMS detector when the Large Hadron Collider turns on later this year.

Neumeister says Steele has already enabled the kind of large-scale simulation and data analysis that is the stock in trade of the CMS project, but increasingly characterizes science, engineering and social science research of many kinds, from modeling molecules to modeling the cosmos.

Not to mention its contribution to Boilermaker-Hoosier relations.

~Greg Kline, Purdue University
July, 2008

This article also appeared in iSGTW. 

 



-- Main.RuthPordes - 21 Jan 2012

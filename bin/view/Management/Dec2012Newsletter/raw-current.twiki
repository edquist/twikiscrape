---+!! *OSG Newsletter, December 2012* 

---+++_*Save the date, March 11-14th, 2013, [[http://rac.uits.iu.edu/osgahm-2013][Open Science Grid All Hands]]*_

*IN THIS ISSUE*

%TOC%

---+++Harnessing OSG resources to realize the full potential of functional brain mapping
 
 The Brain Trauma Research Center (BTRC) at the University of Pittsburgh is a multidisciplinary research program funded by the NIH’s National Institute of Neurological Disorders and Stroke. The Center’s primary goal is to improve outcomes following brain injury. David O. Okonkwo, M.D., Ph.D., is the clinical director of the BTRC and director of the scientific effort. Donald N. Krieger, Ph.D., gathers magnetoencephalography (MEG) data from patient and normal control volunteers, devises data analysis methods, and handles programming. Mats Rynge, OSG User Support, assists with the program’s high throughput computing effort on the OSG. Access to the OSG via XSEDE provides full scale implementation of a revolutionary method for extracting functional brain imaging information.
 
Brain function is mediated by electrical current within populations of neurons. Magnetoencephalography (MEG) samples the fluctuating magnetic fields produced by these currents with an array of sensors positioned outside the patient’s head. Moreover, MEG enables the Brain Trauma Research Center’s functional brain mapping method – which for the first time is producing high resolution virtual recordings, as if from 1M plus electrodes within the brain.
 
The clinical impetus for the effort is to develop methods that improve our ability to diagnose and treat concussion, also known as mild traumatic brain injury. US emergency rooms see over 2M concussions every year, but less than 20% show structural abnormalities on computed tomography(CT) or magnetic resonance (MR) imaging.
 
Functional tests using neuropsychological or computerized behavioral measures are more sensitive, but are not specific to concussions. Our effort fuses structural images with functional brain recordings to generate information that is both sensitive and specific enough to promote understanding, diagnosis, and treatment of concussions.  
 
To optimize information extraction from MEG recordings, a “referee consensus” metric replaces the widely used mean squared error cost function enabling spatial localization with resolution approaching 1mm from raw data.  This multiplies the information provided about ongoing brain function by more than 1,000 compared with any other functional brain mapping method.
 
The fundamental problem is that the number of detectable sources via MEG recordings, their locations within the brain, and their vector amplitudes, are all unknown. To solve this, we must search a space of nonlinear variables with an unknown number of dimensions. Our method enables us to reduce the search to three dimensions at a time, i.e. the three location coordinates of a single detectable source.  However, since the referee consensus metric converges only when used to search within a few mm of a true source, the search covering the brain must be divided into 3,000 plus separate searches, each covering about 1/2 cm3.
 
The referee consensus optimization method is implemented in a small executable image (300 MB).  Each instance runs independently of all others and is short lived (about four hours).  In short, this application is ideally suited to the grid computing resources of the OSG. Access to the OSG and several other supercomputing resources is provided by XSEDE.  On the OSG our effort requires about 500,000 CPU hours to handle a 20 minute data set from each volunteer.  Without the OSG or a comparable supercomputing resource, our effort would stall.
 
The Glidein-based Workflow Management System (GlideinWMS) works on top of Condor to provide simple access to grid resources. We then use generic tools, e.g. csh, scp, rsync, tar, and ssh, to interface with the OSG.  These work reliably, and script development has required only a modest effort and time commitment. OSG staff members have provided knowledgeable and rapid responses to our questions, markedly reducing the effort of porting our software. The ready and competent expertise delivered by both organizations, and in particular by Mats Rynge (OSG) and Marcela Madrid (XSEDE – PSC), has been invaluable.
 
~ Donald N. Krieger, Brain Trauma Research Center, University of Pittsburgh
[Edited by Sarah Engel and Greg Moore]
 
---+++From the Blog of &quot;Memories of a Product Manager&quot;: An interview with Miron Livny : Bosco, HTCondor and more 
 
Hot of the presses is a full interview of Miron, our very own PI, Technical Director,  HTCondor Project Lead and visionary by Miha Ahronovitz newly part of the OSG as product manager of the Campus Infrastructure software Bosco. You can go to this link and read the whole article.
 
Here  are a few  sections to whet your appetite:  
 
Miha:  In essence, Miron&#39;s  research is driven by the following challenge:
 
How can we accommodate an unbounded need for computing and an unbounded amount of data with an unbounded amount of resources?
This is a daring thought to have and it inspired  a fascinating conversation  that was triggered by the upcoming release of the BOSCO tool by the Open Science Grid (OSG) . 
…
Miha: Let&#39;s talk  Bosco,  Who had the idea to develop it separately  from  HTCondor?

Miron: The idea developed naturally. You just have to look at the HTCondor architecture under the hood.  The philosophy was always &quot;Submit locally and run globally&quot; – namely submit on your workstation and run on any workstation in the organization. Now,  why don&#39;t we  go to the researchers and say: &quot;OK, we&#39;ll give you just this one  piece of software, called Bosco, that is running on your side.  You will  learn and install Bosco, in a few hours,and it will enable you to Submit locally and run on machines all over the world!&quot; 

You can view Bosco  &quot;My  Personal High Throughput  Manager&quot;

Miha:  This is the empathy Bosco seeks. Make the researchers happy. Offer them a simple interface to create a galaxy of HTC resources
Miron::  It is not just a job submission interface. It is a High-End HTC Manager. 

Miron:   Bosco can help us by spreading the practical incarnation of the &quot;Submit Locally, Run Globally&quot; concept in HTC. If you submit say to SGE (Sun Grid Engine on one its many flavors), PBS, LSF clusters,  you can get in, but you can not get out to another cluster. You are stuck with SGE or PBS or LSF  When you are submitting to Bosco, you can go everywhere. And that&#39;s the concept: Bosco helps a lot the researcher to build bridges to different on and off campus resources and thus help improve throughput. Regardless of the software that manages these remote resources, Bosco should get your jobs there and bring back the results! 

NOTE:  Bosco beta v1.1 is available for download now

 
---+++Science Verification Processing for the Dark Energy Survey
 
The Dark Energy Survey  uses  a large format CCD camera (500 Megapixels, liquid nitrogen cooled) mounted at the prime focus of the 4-meter diameter mirror Blanco telescope atop Cerro Tololo, Chile.The scientific goals of the project are to construct a large, deep multi-color map of the southern sky&#39;s stars and galaxies to increase our understanding of the mysterious effect known as &#39;Dark Energy&#39;, which apparently is causing the Universe to expand at an ever-increasing, unexpectedly-accelerating, rate. The main data archives and data management initiative is led from National Center for Supercomputing Applications (NCSA), with many other institutions contributing to the work. 

The software to process the basic camera images has been developed over the last several years by a distributed team of researchers and was in reasonable shape ahead of first light data.  The full DES software system supports about a half-dozen heterogeneous types of processing, each one consisting of a number of steps.

As firstlight data arrived in late September, it was realized that the amount of data expected during the science verification phase, scheduled for the nights of Nov 1-24, 2012, would benefit greatly from rapid turn-around (processing a night&#39;s data within one 24 hour period of an on-sky observation) and an ability to make changes to the software quickly and robustly.

To address this need, we focused on developing a Fermilab campus-based system for local DES processing, which augments the standard DES processing activity.

Dark Energy Survey Data Management (DESDM) scientists, together with local Fermilab support and the OSG User Support Team, assembled a package for the Survey&#39;s November rush processing order.   The DES Camera (DECam) obtains a set of about 300 mosaic (see Figure below) camera images each night (each image is about 1 Gigabyte in size, consisting of 62 individual 2k x 4k CCD image files). In order to process each image, about 5GB of associated calibration and reference catalogs are required.  The executable code is about 0.3 GB compressed.

The output data volume consists of about 7 GB of finely calibrated images with associated catalogs listing properties of every object in the field (typically 50,000 individual stars and galaxies per exposure). Transient intermediate working files amount to about 40GB per exposure. The whole processing takes about 15 hours per exposure using a single core.  Peak memory usage is 2.3GB/exposure.  The daily processing utilized up to 500 computing slots and produced 40TB of output during November 2012.

A team of graduate students &#39;eyeballed&#39; over 3,000 raw exposures in order to look for defects and other signs of trouble.  At the same time, the DESDM software developers and DES astronomers poured over early processed exposures, checking the astrometric registration of the images and photometric accuracy of the catalogs.  The standalone application system running on FermiGrid through the OSG interfaces offered one day turn-around of results to the astronomers, who then reported back to the software developers for quick turn-around changes in the algorithms or tunings to the parameter settings.

The Dark Energy Survey will begin full operations on or about January 1, 2013, and run for at least 5 years producing beautiful, deep images of the sky (Figure on the right).  All parties see many other opportunities for continued collaboration on the distributed computing solutions!
~ Brian Yanny

---+++Registering Your Campus Grid with OSG

Some of you may have seen new Campus Grids flags on the OSG Topology Map. Several campus grids such as UCSD, UNL, IU, UC, UW Madison, VT and SSERCA have already registered and appear on the map. Step-by-step instructions to register your new campus grid to OSG are as follows:
 
1.	Visit the topology database and register your contact information. 
 
This can be done by visiting OSG Information Management (OIM) service with a browser loaded with your IGTF or other approved CA credentials. If you have already registered, this step is unnecessary, OIM will let you know if you need to do this. 
 
2.	Click the Campus Grid button on the menu bar or go directly to the Campus Grid page and click the &quot;Add New Campus Grid&quot; button at the top right corner. 
 
a.	Enter basic information about the campus grid including: Name, Description, Maturity (this lets us know how evolved your campus grid is and ranges from &quot;No organized effort&quot; to &quot;Campus grids are a way of life&quot;), Fields of Science, Latitude/Longitude (OIM will try and determine this for you), and Alternate Contacts.
 
Note: the URL of the Campus Grid Gratia Accounting field can be left blank, this is for future accounting functionality. If you have a submit node in place, it can be entered in the &quot;Submit Node FQDNs&quot; -  this can be added or changed later. 
 
3.	Click the Submit button. 
 
You  are now registered. The OSG Ops Group and Campus Grids effort will review your registration and let you know if there are any questions. Your campus grid flag on the OSG Topology Map should appear immediately. 
 
Please contact OSG Operations at goc@opensciencegrid.org if you have any questions regarding this process. 
 
~ Rob Quick
 
---+++Thoughts from OSG Communications
 
With the final OSG newsletter of 2012, I want to acknowledge and thank  Jemise Lockhart and Ginny Werner (No picture available)  for their continued administrative support for the OSG. They variously help over many years with proposals, financials and project management support. Ginny provides detailed, patient guidance and oversight of  all last minute details and tasks for submissions and contracts – always on  the weekend, always with just in time panics. Jemise coordinates this newsletter every month – carefully ensuring that we receive the articles, that the editing and reviews take place, and that the monthly editions get out on schedule.  We all need and appreciate these kinds of &quot;behind the front line&quot; support. Thank you!
 
And, as we move into new activities and challenges and scientific outcomes in 2013, we  wish all OSG contributors, staff and collaborators a healthy, happy and peaceful 2013.
 
~ Ruth Pordes on behalf of the OSG Project and Consortium.


-- Main.JemiseLockhart - 20 Dec 2012

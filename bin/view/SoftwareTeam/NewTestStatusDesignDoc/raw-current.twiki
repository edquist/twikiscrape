---+ New OSG-Test Test Statuses

%TOC%

---++ Overview

We need to be able to separate the following conditions in order to gain insight into the test results:

	1. A test is skipped due to an acceptable condition, e.g. a package that the test depends on is not installed.%BR%
   This state needs to be distinguished from a successful test, but should not raise any red flags.
	1. A test is skipped due to a previous test or action failing, e.g. a required service could not be started.%BR%
   In other words, a cascading failure. This needs to be distinguished from the original failure to avoid distracting the developer, but should still raise a red flag.

The existing unittest framework will be updated with two new statuses, tentatively named =OkSkip= and =BadSkip=.
This must not cause any backward-incompatible changes, i.e. all existing tests must work without modification.
The tests should be reworked one at a time to make use of the new feature.

Other requirements are: it must work on both Python 2.4 and 2.6, and should not bring in additional dependencies.

Also note that we will have to update the nightly test wrapper to handle the new output.
To make full use of the new features, the test aggregator must also be updated.

---++ New Interfaces

---+++ Testing API Changes

---++++ New Assertions

The new assertions are functions of =OSGTestCase=, which is the class from which all tests will derive (someday). Currently, tests are derived from =unittest.TestCase=, and they can be converted independently.

   $ =skip_ok(message=None)= : An unconditional ok skip, with optional message.
   $ =skip_bad(message=None)= : An unconditional bad skip, with optional message.
   $ =skip_ok_if(expr, message=None)= : Ok skip if =expr= is True, with optional message.
   $ =skip_bad_if(expr, message=None)= : Bad skip if =expr= is True, with optional message.
   $ =skip_ok_unless(expr, message=None)= : Ok skip if =expr= is False, with optional message.
   $ =skip_bad_unless(expr, message=None)= : Bad skip if =expr= is False, with optional message.

---++++ New Library Function(s)

New helper function(s) in =osgtest.library.core= cover the most common use cases for skipping.
Currently only one is planned, but more may be added as other use cases are identified.

   $ =skip_ok_unless_installed(*rpms, message=None)= : Verifies that all RPMs in =rpms= are installed.%BR%
   Raises =OkSkip= with the optional message if any of them are missing.

---+++ Using !OkSkip in Test Functions

Changing tests to use =OkSkip= is fairly straightforward. The following are common replacement patterns where the use of =OkSkip= would be appropriate:

---++++!! Old Code

Current tests use two different methods for verifying necessary RPMs are installed. One method:

&lt;pre class=&quot;file&quot;&gt;
if not core.rpm_is_installed(&#39;foo&#39;):
    core.skip(&quot;rpm foo not installed&quot;)
    return
&lt;/pre&gt;

The other common method:

&lt;pre class=&quot;file&quot;&gt;
if core.missing_rpm(&#39;rsv&#39;, &#39;rsv-consumers&#39;, &#39;rsv-core&#39;, &#39;rsv-metrics&#39;):
    return
&lt;/pre&gt;

---++++!! New Code

Both cases are now covered by a single, flexible helper function. Examples:

To check for a single RPM and use the default (generated) message:

&lt;pre class=&quot;file&quot;&gt;
core.skip_ok_unless_installed(&#39;ndt&#39;)
&lt;/pre&gt;

To check for multiple RPMs and provide a message:

&lt;pre class=&quot;file&quot;&gt;
core.skip_ok_unless_installed(&#39;rsv&#39;, &#39;rsv-consumers&#39;, &#39;rsv-core&#39;, &#39;rsv-metrics&#39;, message=&#39;RSV not installed&#39;)
&lt;/pre&gt;

---+++ Using !BadSkip in Test Functions

The =BadSkip= result is to be used when the test run was configured to allow a test to run but actual conditions prevent the tests from succeeding. For example, if a service is installed but does not seem to be running, tests depending on the service should =BadSkip=. Currently, tests that depend on a service either =core.skip()= and =return= if the service seems to be unavailable, or continue execution and allow individual tests to fail; neither approach makes it easy for a developer to understand what happened.

Generally, this change implies that the code separately checks and/or tracks RPM availability (which is a test configuration issue) and service or other resource availability (which is an actual conditions issue). Some tests already track resource availability via the =core.state= dictionary, others may need that kind of tracking added.

Examples follow.

---++++!! Old Code

&lt;pre class=&quot;file&quot;&gt;
def test_03_voms_proxy_info(self):
    if core.missing_rpm(&#39;voms-client&#39;):
        return
    if not core.state[&#39;voms.got-proxy&#39;]:
        core.skip(&#39;no proxy&#39;)
        return
&lt;/pre&gt;

---++++!! New Code

&lt;pre class=&quot;file&quot;&gt;
def test_03_voms_proxy_info(self):
    %ORANGE%core.skip_ok_unless_installed(&#39;voms-client&#39;)
    self.skip_bad_unless(core.state[&#39;voms.got-proxy&#39;], &#39;no proxy&#39;)%ENDCOLOR%
&lt;/pre&gt;

---+++ Long Example
As an example, here is how the tests for HTCondor would be rewritten (not including setup and shutdown).

---++++!! Old Code

---+++++!! test_41_jobs.py
&lt;pre class=&quot;file&quot;&gt;
class TestGlobusJobRun(unittest.TestCase):
    # ...

    def test_02_condor_job(self):
        if core.missing_rpm(&#39;globus-gram-job-manager-condor&#39;,
                            &#39;globus-gram-client-tools&#39;, &#39;globus-proxy-utils&#39;):
            return

        command = (&#39;globus-job-run&#39;, self.contact_string(&#39;condor&#39;), &#39;/bin/echo&#39;, &#39;hello&#39;)
        stdout = core.check_system(command, &#39;globus-job-run on Condor job&#39;, user=True)[0]
        self.assertEqual(stdout, &#39;hello\n&#39;,
                         &#39;Incorrect output from globus-job-run on Condor job&#39;)
&lt;/pre&gt;


---++++!! New Code
Changes are highlighted.

---+++++!! test_41_jobs.py
&lt;pre class=&quot;file&quot;&gt;
class TestGlobusJobRun(OSGTestCase):
    # ...

   def test_02_condor_job(self):
       %ORANGE%core.skip_ok_unless_installed(&#39;globus-gram-job-manager-condor&#39;,
                                     &#39;globus-gram-client-tools&#39;, &#39;globus-proxy-utils&#39;,
                                     message=&#39;Globus GRAM not installed&#39;)
   
       self.skip_bad_if(core.state[&#39;condor.running-service&#39;], &quot;HTCondor not running&quot;)%ENDCOLOR%
       command = (&#39;globus-job-run&#39;, self.contact_string(&#39;condor&#39;), &#39;/bin/echo&#39;, &#39;hello&#39;)
       stdout = core.check_system(command, &#39;globus-job-run on &lt;b&gt;HT&lt;/b&gt;Condor job&#39;, user=True)[0]
       self.assertEqual(stdout, &#39;hello\n&#39;,
                        &#39;Incorrect output from globus-job-run on &lt;b&gt;HT&lt;/b&gt;Condor job&#39;)
&lt;/pre&gt;

---+++ Example Output

Here is what output should look like in a test run with the new features.

---++++!! Current Output
For comparison, this is what a failure looks like currently:
&lt;pre class=&quot;screen&quot;&gt;
test_01_fetch_crl (osgtest.tests.test_06_fetch_crl.TestFetchCrl) ... FAIL
...
======================================================================
FAIL: test_01_fetch_crl (osgtest.tests.test_06_fetch_crl.TestFetchCrl)
----------------------------------------------------------------------
Traceback (most recent call last):
  File &quot;/usr/lib/python2.6/site-packages/osgtest/tests/test_06_fetch_crl.py&quot;, line 11, in test_01_fetch_crl
    stdout, _, fail = core.check_system(command, &#39;Start fetch-crl&#39;)
  File &quot;/usr/lib/python2.6/site-packages/osgtest/library/core.py&quot;, line 184, in check_system
    assert status == exit, fail
AssertionError: Start fetch-crl
EXIT STATUS: 1
STANDARD OUTPUT:
ERROR CRL verification failed for JUnet-CA/0 (JUnet-CA)
VERBOSE(0) JUnet-CA/0: CRL has nextUpdate time in the past
STANDARD ERROR: [none]
&lt;/pre&gt;

---++++!! New output

A multi-line error message complete with stack backtrace is overkill for a skip.
Instead, skips will be grouped together (i.e. all !BadSkips, then all !OkSkips below), one line per item.

&lt;pre class=&quot;screen&quot;&gt;
test_02_condor_job (osgtest.tests.test_41_jobs.TestGlobusJobRun) ... okskip
...
test_03_voms_proxy_info (osgtest.tests.test_50_voms.TestVOMS) ... BADSKIP
...

======================================================================
BADSKIPS:
----------------------------------------------------------------------
test_03_voms_proxy_info (osgtest.tests.test_50_voms.TestVOMS) no proxy
...

======================================================================
OKSKIPS:
----------------------------------------------------------------------
test_02_condor_job (osgtest.tests.test_41_jobs.TestGlobusJobRun) Globus GRAM not installed (missing: globus-gram-job-manager-condor)
...
&lt;/pre&gt;

---+++ Example Summaries

=unittest= also prints summaries at the end with the different test counts.
Here is how they will look:

---++++!! Old Output
Case 1: some failures
&lt;pre class=&quot;screen&quot;&gt;
FAILED (failures=5)
&lt;/pre&gt;

Case 2: no failures
&lt;pre class=&quot;screen&quot;&gt;
OK
&lt;/pre&gt;

---++++!! New Output
Case 1: some failures and skips
&lt;pre class=&quot;screen&quot;&gt;
FAILED (failures=5, badSkips=1, okSkips=1)
&lt;/pre&gt;
Case 2a: no failures, but some skips
&lt;pre class=&quot;screen&quot;&gt;
OK (okSkips=1)
&lt;/pre&gt;
Case 2b: no failures, no skips
&lt;pre class=&quot;screen&quot;&gt;
OK
&lt;/pre&gt;

---++ Architecture of Proposed Solution

Pretty much all of the classes in =unittest= need to be subclassed.
In order to maintain backward compatibility, the derived classes should be able to work with the originals where possible.

A test failure in =unittest= is represented by an instance of the built-in Python exception =AssertionError= (though under the alias =TestCase.failureException= ).
Any other exception is considered an =Error=.
Exceptions are caught in =TestCase.run()=.

Making =BadSkip= a subclass of =AssertionError= will allow the existing =TestCase.run()= method to catch it and treat it as a test failure, which is how these things are currently considered.

=OkSkip= will also be a subclass of =AssertionError=.
This is somewhat suboptimal since it is not, technically, an error.
However, there isn&#39;t another exception class we can use that =TestCase.run()= will not either interpret as a failure or an error.
Also, we still want to use an exception to immediately exit the running test function and have the status be recorded in the results.

Currently, tests use =osgtest.library.core.skip()= followed by an explicit =return= to handle a skip.
Thus, their behavior will not be changed by this new exception class.
The problem with _not_ using an exception for =OkSkip= is that we will have to add some other way of notifying =TestCase.run()= that the test was not a success.
Also, if people forget to add a =return= after declaring the skip, we may have multiple skips, a skip and a failure, or a skip and a success, for the same test. 

---++++!! !OkSkip / !BadSkip vs. skip from unittest2

=unittest2=, which is a backport of the new features of =unittest= found in Python 2.7, also has a feature for skipping tests.
The behavior is similar to this design.
One notable addition is that test functions can be _decorated_ with skips, e.g.:&lt;pre class=&quot;file&quot;&gt;
@unittest2.skipIf(mylib.__version__ &lt; (1, 3), &quot;library too old&quot;)
def test_foo(self):
   ...
&lt;/pre&gt;
That won&#39;t be worth adding, since our reasons for skipping a test are fairly dynamic and depend on the current environment.
Also, we might have multiple skip reasons in one test function.

=unittest2= also does not distinguish between an &#39;ok&#39; skip and a &#39;bad&#39; skip.

---+++ unittest Subclasses

The subclass of =unittest.TestCase= (=OSGTestCase=) will contain new assertion functions rasing =OkSkip= or =BadSkip=, patterned after the existing ones in unittest, e.g.: =skip_ok_if=, =skip_bad_unless=.

The =run()= method will be modified to record an =OkSkip= or a =BadSkip= upon receiving the appropriate exception;
since this involves a =unittest.TestResult= instance, =run()= will first check to see if the =TestResult= supports skips. 

If the =TestResult= instance cannot handle skips, then a =BadSkip= is reported as a =Failure= (if it happens while running a test) or an =Error= (if it happens during a test&#39;s =setUp()= method).
This is the current behavior for assertion failures in unittest.
An =OkSkip= will be logged but considered a success.

The subclass of =unittest.TestResult= (=OSGTestResult=) will contain lists of =OkSkips= and =BadSkips=, and methods to append to them.
The signature and behavior of these methods will match the existing methods =addError()= and =addFailure()=.
The =wasSuccessful()= method will be modified to consider =BadSkips= unsuccessful.
A =wasPerfect()= method will be added that will be =True= iff the tests were both successful and had no skips.

The subclass of =OSGTestResult= (=OSGTextTestResult=) will behave like its parent but will also add text output to a stream according to verbosity levels passed in via its constructor.
This mirrors how =unittest._TestResult= works in Python 2.4.

The subclass of =unittest.TextTestRunner= (=OSGTextTestRunner=) creates a test result object that is an instance of =OSGTextTestResult= instead of =TextTestResult=, and adds info on =BadSkips= and =OkSkips= to the test summaries.

Currently, =unittest.TestSuite= does not need to be modified.
To make future additions easier, I will alias it anyway as =OSGTestSuite=.

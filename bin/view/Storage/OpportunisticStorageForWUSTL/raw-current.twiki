---+!! *&lt;noop&gt;%SPACEOUT{ &quot;%TOPIC%&quot; }%*
%TOC%
---++ Purpose
Human Microbiome Project (HMP) at Washington University in St. Louis (WUSTL)  is dedicated to study of core human 
microbiome. The project analyzes how the changes in the human microbiome can be correlated with changes in human health. 
The new technological and bioinformatic tools  are being developed by HMP team in order to advance this research.

---++ Use Case
Basic Local Alignment Search Tool (blastx) is used to find matches (with a set of special criteria) between DNA sequences (each100-character long strings)  and a
&quot;reference database&quot;.  The selection of the datasets depends on the scientific question being asked. The reference database take the form of several files that, depending on the database, will vary from 3 GB to 10 GB.  The goal is to process tens of billions of DNA sequences. The DNA sequences are split into subsets of &quot;input reads&quot; in order to use them as an input for blastx that runs against a given dataset.

The input to every blastx job is about 67KB files (depends on target runtime/platform) and the reference database (varies: e.g., 3 GB to 10 GB but the same for all jobs in a set).  The job usually runs from 15 minutes to an hour.

The output is a list of matches is in the order of 200 KB per job. The requirements for memory for one job vary: majority of jobs are using less then 2.2 GB of RAM and around 3 GB of virtual memory, with the larger input database blastx could use up to 4 GB of RAM and 7.5 GB of virtual memory. 

*NOTE*: (from Brian B) NCBI BLAST has a very aggressive memory caching model which is not well-exposed to the end-user.  It is possible to give it a firm upper limit in virtual memory size if you edit the source code.  In my tests, giving an upper limit of ~1GB (which is what the majority of OSG sites provides) causes about a 10% slowdown in walltime.

---+++ Engage workflow for WUSTL
The first job submitted by Engage on the selected site is staging a database (large text file) and building  references datasets in OSG_DATA shared area. The total size of datasets is 600GB.  Apparently, WUSTL needs just small subset  of these data (10GB) - question to Engage team: Why do all reference datasets need to be staged?

After the reference datasets are successfully built, a pilot job is submitted to a site. Pilot jobs starts user jobs each run from 10 to 1 hours, each runs blastx with input file with subset of DN sequences and subset of the reference datasets. Question to Engage: how do you avoid overloading shared fs when all the jobs start accessing reference data?
 
The requirements:

   1. OSG_DATA allows to store 600GB of data. (question to Engage:  Any other requirement for FS? )
   1. Site should have enough free cpu cycles (question to Engage: what is enough?)
   1. Worker node should allow outbound connection.


---+++ Alternative workflow for WUSTL
   1. Build binary datasets at RENCI from a text file.  The total size of resulting datasets is about 85GB.
   1. According to Brian the binary datasets are relocatable so they can be used at any sites.
   1. Upload all datasets to SE of  the selected sites. 
   1. Start pilot job, download selected dataset from SE, cache it on WN, run jobs
   

---++ Initial Participating Sites
| *Site Name* | *Status *|
|CLEMSON-Palmetto|	functioning|
|FIREFLY|	functioning|
|NEBRASKA_RED|	functioning|
|OSG-RENCI-Engagement|	functioning|
|OSG-UCHC_CBG|	functioning|
|OSG-UFlorida-HPC|	databases will not build, walltime maxes out, other difficulties|
|FNAL_FERMIGRID|	currently debugging|
|FNAL_GPGRID_1|	will attemtp to configure|
|USCMS-FNAL-WC1|            	|
|NYSGRID_CORNELL_NYS1|	could work. Has about 1 TB free|
|OUmissHEP|	not enough CPUs. Has space available, but is misconfigured. OSG_WN_TMP is not writable|
		
---+++ Fermi Grid requirements
Fermilab has a   [[http://fermigrid.fnal.gov/policy.html][strict policy] that is applied to all VOs that are using Fermilab resource. Among other things the following is relevant in WUSTL case:
   1. Fermi Grid has a default quota of 400GB  per VO on OSG_Data 
   1. It is required in case of multi user pilot jobs to use the glexec package to validate the grid credentials of the actual user who has submitted the job
   1. Only voms-proxy initialized credential should be used

In order to applied for permission to use Fermi Grid in non-standard way Engage/WUSTL can submit a request via GOC and assign the ticket to Fermi Site. You might want to request
 
   a. increase of OSG_Data quota to 600GB	(justification should be provided)
   a. . permission to not use gLexec (you will need then to submit pilot jobs with voms-proxy for engage/wustl FQAN). The executables that will be run under pilot jobs should well understood.
 


-- Main.TanyaLevshina - 20 May 2010


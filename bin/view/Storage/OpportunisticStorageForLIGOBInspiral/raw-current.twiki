---+!! *&lt;noop&gt;Opportunistic Storage for LIGO Binary Inspiral*
%TOC%
%BLUE%Requirements:%ENDCOLOR%
   * periodically pre-stage 1 week of data (2 weeks minimal per site)
   * 1 week of data appears to be around O(100GB).
   * a job running on the worker node needs to access to the whole weekly dataset 
   * *Optional*: data should be deleted after processing of the weekly dataset is done.  During testing, we frequently reuse data.
   * RLS catalog needs to be populated after successful transfer.
   * Data should be easily replicated to multiple sites.
   * 100K jobs at 5-15 minutes per job.
   * 25K hours per workflow.

%BLUE%Current Restrictions:%ENDCOLOR%
   * Have to run on site with posix access to the files from the worker nodes, so only sites with hdfs, lustre or panasas may be considered 
   * Currently using the -rfc compliant VOMS proxy.  This is not accepted by current !BeStMan-gateway, but accepted by Globus Gridftp servers.

%BLUE%Immediate Goals:%ENDCOLOR%
   * Document the procedure for kicking off transfers (Britta) %GREEN%DONE%ENDCOLOR%  [[http://www.ligo.caltech.edu/~bdaudert/INSPIRAL/FILE-TRANSFERS/][File transfer instructions]]
   * Investigate the current transfer procedure, see if it could be optimized (Brian &amp; Derek)
   * See if the -rfc requirements can be dropped or use two proxy (change proxy location env.variable to specify in command line). The load balancing provided by SRM could be use for gridftp server selection. (Brian &amp; Derek).
   *  Measure the transfer of weekly dataset from LIGO to Nebraska. The goal is ~ 8 hours. (Britta, Brian, &amp; Derek) 
   *  Write replication script that allowed transfer of data between sites. Provide measurements. (Derek? Brian? Tanya?)
   *  Identify the sites that LIGO could run and verify that they are ready for hosting LIGO data  (Tanya). 19 SEs claim to support LIGO. Only 5 of them have storage area mounted on worker nodes according to BDII. %GREEN%DONE%ENDCOLOR%
   * Create a !GlideInWMS workflow to run in 1-3 days at any particular site that can provide 10K-20K opportunistics hours per day to LIGO (Mats) %GREEN%DONE%ENDCOLOR%
   * Create a standard &quot;transfer test&quot; which we can do between sites to show we understand how to manage transfers. 
   * Discuss second level staging with Pegasus team. %BLUE%In progress%ENDCOLOR%

%BLUE%Britta&#39;s wish list%ENDCOLOR%

   * srm-copy should work with rfc complient proxy
   * srm-copy -mkdir should create directories recursively
   * throttling at BestMan gateways
   * srm-copy error handling should be improved, e.g.
      * Pad passphrase --&gt; one of the gridftp servers not operational
      * EOF error --&gt;  incorrect mount point or port
   * turn  OUHEP_OSG into 64 bit cluster

|*Testing Status*| *Site Name*| *SE_ID*| *Storage Type* | *SURL* | *Test Status* | *Local !Mount* | *SA free space (8/23/10)* |*Ticket*|
|%RED%Fail%ENDCOLOR%|UCSDT2|bsrm-1.t2.ucsd.edu|!BeStMan/HDFS|srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/hadoop/ligo|%GREEN%Success%ENDCOLOR%|/hadoop/ligo|~136TB||
|%GREEN%Success%ENDCOLOR%|CIT_CMS_T2|cit-se.ultralight.org|!BeStMan/HDFS|srm://cit-se.ultralight.org:8443/srm/v2/server?SFN=/mnt/hadoop/osg/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/osg/ligo| ~90TB||
|%GREEN%Success%ENDCOLOR%|Nebraska|red-srm1.unl.edu|!BeStMan/HDFS|srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/user/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/user/ligo|~156TB||
|%GREEN%Success%ENDCOLOR%|Firefly|ff-se.unl.edu|!BeStMan/panasas|srm://ff-se.unl.edu:8443/srm/v2/server?SFN=/panfs/panasas/CMS/data/ligo|%GREEN%Success%ENDCOLOR%|/panfs/panasas/CMS/data/ligo|~40TB||
|%RED%Fail%ENDCOLOR%|TTU-ANTAEUS|sigmorgh.hpcc.ttu.edu|!BeStMan/Lustre|srm://sigmorgh.hpcc.ttu.edu:49443/srm/v2/server?SFN=/lustre/hep/osg/ligo|%GREEN%Success%ENDCOLOR%| /lustre/hep/osg/ligo|~239TB||
|%GREEN%Success%ENDCOLOR%|OUHEP_OSG|ouhep2.nhn.ou.edu|!BeStMan/Lustre|srm://ouhep2.nhn.ou.edu:8443/srm/v2/server?SFN=/raid1/data/griddata/ligo-ihope|%GREEN%Success%ENDCOLOR%|/raid1/data/griddata/ligo-ihope| ~1.5TB/actual 400GB||
|%RED%Omitted%ENDCOLOR%|UFlorida-PG|srmb.ihepa.ufl.edu|!BeStMan/Lustre|srm://srmb.ihepa.ufl.edu:8443/srm/v2/server?SFN=/lustre/raidl/user/ligo/|%RED%Error%ENDCOLOR%|!MountInfo set to none|~1TB||
|%RED%Fail%ENDCOLOR%|!GridUNESP_CENTRAL|se.grid.unesp.br|!BeStMan/?|srm://se.grid.unesp.br:8443/srm/v2/server?SFN=/store/ligo|%GREEN%Success%ENDCOLOR%|/store/ligo|N/A||
|%RED%Fail%ENDCOLOR%|!UMissHEP|umiss005.hep.olemiss.edu|!BeStMan/?|srm://umiss005.hep.olemiss.edu:8443/srm/v2/server?SFN=/osgremote/osg_data/ligo|%GREEN%Success%ENDCOLOR%|/osgremote/osg_data/ligo| ~7TB||
|%RED%Omitted%ENDCOLOR%|NWICG_NDCMS|nwicg.crc.nd.edu|!BeStMan/?|srm://nwicg.crc.nd.edu:49084/srm/v2/server?SFN=/dscratch/osg/bestman|%RED%Error%ENDCOLOR%|!MountInfo is not defined|~0.2TB||
|%GREEN%Success%ENDCOLOR%|NWICG_NotreDame|osg.crc.nd.edu|!BeStMan/?|srm://osg.crc.nd.edu:8443/srm/v2/server?SFN=/pscratch/osg/bestman/ligo-ihope|%GREEN%Success%ENDCOLOR%||~0.2TB||
|%GREEN%Success%ENDCOLOR%|SBGrid-Harvard-East|osg-east.hms.harvard.edu|!BeStMan/?|srm://osg-east.hms.harvard.edu:10443/srm/v2/server?SFN=/osg/storage/data/ligo-ihope|%GREEN%Success%ENDCOLOR%|!MountInfo is not defined|~3TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9571][9571]]|
|%RED%Omitted%ENDCOLOR%|NERSC-PDSF|pdsfsrm.nersc.gov|!BeStMan/?|srm://pdsfsrm.nersc.gov:62443/srm/v2/server?SFN=/srmcache/ligo|%RED%Error%ENDCOLOR%|!MountInfo is not defined|N/A||
|%RED%Omitted%ENDCOLOR%|FNAL_FERMIGRID|fndca1.fnal.gov|dCache|srm://fndca1.fnal.gov:8443/srm/managerv2?SFN=/pnfs/fnal.gov/usr/fermigrid/volatile/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|GLOW|cmssrm.hep.wisc.edu|dCache|srm://cmsdcache03.hep.wisc.edu:8443/srm/managerv2?SFN=/pnfs/hep.wisc.edu/data5/LIGO|%RED%Error%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|BNL-ATLAS|dcsrm.usatlas.bnl.gov|dCache|srm://dcsrm.usatlas.bnl.gov:8443/srm/managerv2?SFN=/pnfs/usatlas.bnl.gov/osg/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|Purdue-RCAC|srm-dcache.rcac.purdue.edu|dCache|srm://srm-dcache.rcac.purdue.edu:8443/srm/managerv2?SFN=/VO/ligo|%RED%Error%ENDCOLOR%||~1TB||
|%RED%Omitted%ENDCOLOR%|SPRACE|osg-se.sprace.org.br|dCache|srm://osg-se.sprace.org.br:8443/srm/managerv2?SFN=/pnfs/sprace.org.br/data|%RED%Error%ENDCOLOR%||~30TB||
                                                                                                                                           

---++Throttling/concurrency testing
   * 5000 files were transferred 
   * 5000 files span approximately 3.5 days of LIGO data, 80000 MB = 78.1 GB
   * Implemented condor throttle 4:  max transfer jobs running simultaneously  
   * Throttling the number of simultaneous srm-copy calls is necessary to avoid excessive memory usage causing submit host crash
   * Increase timeout limit from 1800 secs to 3600 sec to avoid timeout errors
   * Testing different concurrency settings to optimizes file transfers 

|%BLUE%Total run time of transfers (5000 files)%ENDCOLOR%| *Run 1: LDG--&gt;OUHEP*| *Run 2: LDG--&gt;OUHEP*| *Run 1: LDG--&gt;Nebraska* | *Run 2: LDG--&gt;Nebraska* | 
|*Concurrency 1*|8 hrs 56 mins|10 hrs 50 mins|9 hrs 31 mins|9 hrs 18 mins|
|*Concurrency 3*|3 hrs 33 mins|3 hrs 31 mins|3 hrs 15 mins|3 hrs 23 mins|
|*Concurrency 4*|2 hrs 51 mins|4 hrs|2 hrs 39 mins|2 hrs 31 mins|
|*Concurrency 5*|2 hrs 15 mins|2 hrs 33 mins|2 hrs 7 mins|2 hrs 6 mins|
|%BLUE%Runtime range in secs per transfer dag job (100 files)%ENDCOLOR%| *LDG--&gt;OUHEP*| *LDG--&gt;OUHEP*| *LDG--&gt;Nebraska* | *LDG--&gt;Nebraska* |
|*Concurrency 1*|2621 - 2705|2625 - 4077|2510 - 2761|2509 - 2693|
|*Concurrency 3*|910 - 1045|898 - 1151|859 - 886|861 - 977| 
|*Concurrency 4*|675 - 914|672 - 837|632 - 728|635 - 679|
|*Concurrency 5*|575 - 912|578 - 895|508 - 599|507 - 610|


---++md5sumcheck statistics

|%BLUE%in seconds unless otherwise indicated%ENDCOLOR%| * LDG--&gt;Nebraska*| *Nebrasksa--&gt;FF*| *Nebraska--&gt;CIT_CMS_T2* | *Nebraska --&gt; NWICG_NotreDame* |*LDG --&gt; LIGO_CIT*|
|*dagman run time of md5sum job (100 files)*|133|161|321|105|31.46|
|*Average Time in local queue*|11|15|12|0.5|15.85|
|*Average time in remote queue*|303|16438|58565|517|0|
|*Total run + queue (5000 files)*|6.2 hrs|230.8 hrs|818 hrs|8.64 hrs|0.65 hrs|


---++ Tests with !BeStMan2 client vs srm-client-lbnl

Elapsed time for transferring 100 files 16MB each using srm-copy command with the following options:
&lt;pre class=&quot;screen&quot;&gt; 
srm-copy -f file_list -concurrency 5  -3partycopy 
&lt;/pre&gt;

where file_list is the list of sources (gsiftp://..)  and targets (srm://..)

Target server is !BeStMan-gateway on top of HDFS,

|* Client *|* Server *|* Time *|
| bestman 1.3.18 | !BeStMan 1.3.18|9m2.208s|
| bestman 2.0.5 | !BeStMan 1.3.18|3m11.315s|
| bestman 1.3.18 | !BeStMan2 2.0.5 |9m4.223s |
| bestman 2.0.5 | !BeStMan2  2.0.5 | 3m10.185s |

%BLUE%Tanya&#39;s summary:%ENDCOLOR%

&lt;pre class=&quot;screen&quot;&gt;
This is my summary of all the experiments we have done so far.

1. Robert adjusted condor throttling ( otherwise all the 50 jobs started simultaneously making submission node unresponsive). Robert also  increased the RAM allocated for submission node (VM) in order to prevent any memory swapping when four srm-copy commands were running.

2.  Britta ran 16 tests transferring 5,000 file (16MB) from LIGO gridftp server to Nebraska and to OUHEP_OSG. 

3. Both sites have BeStMan2 installed. Nebraska is running BeStMan2/HDFS and OUHEP has BeStMan2/local disk+ nfs.

4. OUHEP is a small Tier-3 site with one gridftp server, Nebraska is CMS - Tier-2 site with 10 gridftp servers

5. LIGO -&gt; OUHEP.  6 out 8 transfers have  finished successfully , 2 tests were terminated by the srm-copy client because of the bug (see #8). Four tests have experienced some gridftp errors that apparently indicate the high load on the grdiftp server on OUHEP but two of them were able to recover after automatic retries performed by srm-copy client and the other two were terminated by srm-copy (see #8).

It took ~  9 hours 30 min to transfer 5,000 files (80GB) with 4 jobs each executing one srm-copy command with one gridftp transfer. It took ~ 2hours 20 min  to transfer 5,000 files (80GB) with 4 jobs each executing one srm-copy command with five gridftp transfer.

6. LIGO -&gt; Nebraska. All the transfers finished successfully without any grdiftp errors.

It took less then 9 hours 30 min to transfer 5,000 files (80GB) with 4 jobs each executing one srm-copy command with one gridftp transfer. It took ~ 2 hours   to transfer 5,000 files (80GB) with 4 jobs each executing one srm-copy command with five gridftp transfer.

7. Most of the tests have been repeated twice and results were pretty consistent (see attached pdf)

8. srm-copy bug: srm-copy stops the transfer as soon as the elapsed time since s start time exceeds the time specified in the --timeout option. So with timeout set to 3600 sec, srm-copy exists after in an hour  regardless of success or failure of transfers. The bug is  fixed by developers but not released yet.

9. BeStMan developers are working on improving &quot;srm-copy -f file -concurrency N&quot;  efficiency. The command will be ready for testing soon (hopefully this week).

10. We are looking on various java options that can be used to decrease memory usage by srm-copy command. The memory is allocated on start up and doesn&#39;t increase during the srm-copy execution.


11. With network bandwidth limit set by LIGO admin it looks like 4 srm-copy commands with concurrency set to 5  is  suitable for LIGO tests. It should take about 26 hours to transfer the weekly data set (1 TB). We should retry all the tests as soon as more efficient srm-copy is available
&lt;/pre&gt;


---++Details of file transfers to different OSG sites

   * 11/05/10 SUBMIT HOST CRASH
      * 4 work-flows submitted at the time, each transferring 5000 files (100 per dag job)
         * Grid_UNESP_CENTRAL, OUHEP_OSG, NWICG_NotreDame, SBGrid-Harvard-East
      * Robert: due to each lbnl srm client using 200MB in RAM for each transfer process
      * Alex Sim: you need to increase the MX value in the srm-copy script for java vm size

   * SBGrid-Harvard-East
      * transferred 3 day data set successfully.

   * NWICG_NotreDame
      * bad pass phrase error cause by source red-gridftp1.unl.edu being non-operational
      * re-run work-flow, 5000 files transferred successfully, closed GOC ticket


   * UCSDT2
      * test work-flow transferring 20 files
      * can&#39;t turn off remote_initialdir with current version of Pegasus
      * remote_initialdir = /hadoop/ligo: md5sum checks fail: no execute permission of pegasus kickstart 
      *  manually removing remote_initialdir from md5sum job submit file: stage_out of data fails, pegasus:transfer sets source directory of output files to $DATA
      * Additionally: $DATA not visible from WNs --&gt; can&#39;t set remote_initialdir to $DATA --&gt; ihope work-flow will fail

   * TTU_ANTAEUS
      * test work-flow transferring 20 files
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job&#39;s remote status is unknown
      * and
         * The job&#39;s remote status is known again

   * OUHEP_OSG
      * test work-flow transferring 20 files succeeds
      * test work-flow transferring 5000 files fails
         * repeated srm-copy fail
         * SRM-CLIENT: Thu Oct 28 15:39:23 PDT 2010 Server did not respond and client is exiting. SrmPrepareToPut
      * Alex Sim says:
&lt;pre class=&quot;screen&quot;&gt;
if the machine is too busy, the connection
will likely take a long time to be established (in part it&#39;s GSI and
another busy network), and if it&#39;s too long, srm-copy might get
timed-out. You can increase the timeout value in that case. It doesn&#39;t
seem that the SRM server is too busy or crashed in this particular case.
&lt;/pre&gt;

      * Resubmitted work-flow with time out 3600s (default is 1800), completes
      * Transferring 3 day data set for ihope testing, I observe repeated time-out/handshake  errors and am unable to transfer the complete data set
      * NOTE: OUHEP only has 400 GB storage space in $DATA which is listed above as the storage location 

   * GridUNESP_CENTRAL
      * test work-flow transferring 20 files succeeds
      * 10/29 - present: site down, MYOSG: CE service is in UNKNOWN status
      * 11/11 submitted rescue dag which succeeds without further fails 
      * Observed repeated time out/handshake errors when transferring 3 day data set, unable to transfer the complete data set
     
   * UMiss_HEP     
      * test work-flow transferring 20 files;
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job&#39;s remote status is unknown
      * and
         * The job&#39;s remote status is known again

---++Running IHOPE with SRM setup on a 1 day data set

   * work-flows ran successfully at 2 OSG sites and the local ITB cluster (LIGO_CIT):
      * Nebraska: 11 days
      * FF: 10 days 18 hours
      * LIGO_CIT: 14 hours 20 minutes
   * work-flows are running at 1 OSG site:
      * NWICG_Notredame: submitted 11/24 13:19:19
   * work-flows failed at 3 OSG sites:
      * SBGrid-Harvard-East:: Missing gwf files, resubmitted data transfer dag 01/07 08:45:28, still running
      * CIT_CMS_T2: submitted 11/19 12:39:56, failed over Xmas break, looks like intermediate data product was corrupted
      * OUHEP_OSG 32 bit cluster, addidtionally OUHEP_OSG will allow no more than 5 LIGO jobs at a time (see e-mail exchange 01/13/11)


|%BLUE%Average (remote) queue times in seconds%ENDCOLOR%|*LIGO_CIT*|*Nebraska*|*Firefly*| *CIT_CMS_T2*| *NWICG_NotreDame*| 
|*lalapps_tmpltbank*|86.88|3559.27|243.75|78749.44||
|*lalapps_inspiral*|227.81|7979.98|1109.85|5364.74||
|%BLUE%Average run times in seconds%ENDCOLOR%|*LIGO_CIT*|*Nebraska*|*Firefly*| *CIT_CMS_T2*| *NWICG_NotreDame*|
|*lalapps_tmpltbank*|1997.55|3134.63|2512.91|2347.66||
|*lalapps_inspiral*|9402.04|23233.21|24695.14|29632.14||

   * [[http://www.ligo.caltech.edu/~bdaudert/INSPIRAL/S6-IHOPE-RUNS/STATS/][Detailed stats]]
  

---++Running IHOPE with SRM setup on a 1 day data set using GLIDEINS

&lt;pre class=&quot;screen&quot;&gt; 
The aim is to run a work-flow spanning a 1 day dataset in less than 1 days.
&lt;/pre&gt;


---+++ Single runs
   * FF   69 hrs
   * FF SRM setup 40.3 hrs
   * LIGO_CIT 13 hrs
   * LIGO_CIT SRM setup 70 hrs
   * NWICG_NotreDame 44.3 hrs (2 runs)
   * SBGrid-Harvard-East test run submitted 01/19, still running
   * USCMS-FNAL-WC1-CE3 test run  submitted01/19, still running
   * CIT_CMS_T2 16 days 
   * USCMS-FNAL-WC1-CE3 33 hrs 20 mins


---+++ Scale tests
   * Ten work-flows submitted simultaneously to FF :
      * Run 1: run times vary 21.2 hrs - 53.8 hrs, one run takes 96.5 hrs
      * Run 2 (Glidein config changes): run times vary 17 hrs - 66hrs, one run takes 102.7 hrs
      * Run 3 (peg 3.0, using ff-gridftp for file transfers): run times vary 69.4 - 82.7 hrs

&lt;pre class=&quot;screen&quot;&gt;
10 work-flows, 1 day each =&gt; total run time target = 240 hrs
Run1:
Total run time: 467.3 hrs

Run 2
Total run time:  474.36 hrs

Run3 (only 8 runs completed --&gt; 196 hrs target run time):
Total run time: 523.92 hrs
&lt;/pre&gt;

   * Ten work-flows submitted simultaneously to Nebraska with bumped priority for LIGO
      * Run times vary from 31.3 hours to 53 hours for 9 out of the ten runs
      * One run takes 78.75 hours

&lt;pre class=&quot;screen&quot;&gt;
10 work-flows, 1 day each =&gt; total run time target = 240 hrs

Total run time: 469 hrs
&lt;/pre&gt;
   * More stats here: 
http://www.ligo.caltech.edu/~bdaudert/INSPIRAL/GLIDEINS/STATS/SCALE-TEST/

---++Running IHOPE with SRM setup on a 1 week data set using GLIDEINS

---+++ General information/Remarks
   * data set
      * Oct 2 - Oct 9 2010,  gps-start - gps-end 970012815-970617615
      * recommended by Alan Weinstein (all detectors running smoothly) 
      * this data set was transferred to LIGO_CIT in 2 days 17 hrs 46 mins
      * currently LIGO_CIT is the only site housing this data set
   * code: master 02-16-11 (tagged version in OSG mode gives dax generation error )
   * remark: ideally we would like to run the work-flow over three detector data (two LIGO, one VIRGO) with plotting and pipedown dags
   * pipedown dags are known to fail in OSG mode
   * pipedown dags may add substantially to overall run time
   * it seems that plotting dags fail when running with VIRGO data
   * OUHEP_OSG is 32 bit cluster
   * CIT_CMS_T2 preempts LIGO, work-flow running extremely slow
   * FF observed memory allocation error

---+++ Completed Runs
   * w/o plotting, w/o pipedown work-flow:  on LIGO_CIT 36 hrs 27 mins, on LDG 21 hrs 50 mins
   * w plotting, w/o pipedown, no VIRGO data on LIGO_CIT 37 hrs, on LDG ...
   * w plotting, w/o pipedown, no VIRGO data on Nebraska: 4 days 19 hours 19 minutes


   

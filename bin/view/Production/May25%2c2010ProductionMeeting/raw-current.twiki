-- Main.DanFraser - 30 Apr 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * Atlas has requested the ability for sites with SE&#39;s but not CE&#39;s to be able to register to the BDII. Tony is starting to look at this from the GIP side.
   * Problem with sites not reporting to the correct Gratia collector is winding down. !FermiGrid operations is planning to see if there are any stragglers. (Marco)
   * GOC monitoring will now alert when 10% of sites have dropped out of the BDII as a high level monitoring mechanism for the CEMON collector. It is also important to monitor/know when a T1 goes offline. Dan to follow up with Rob Q. and understand the status of this. 

---++ Attendees:
   * John, Mats, Xin, Armen, Britta, Rob E., Brian, Suchandra, Burt, Marco, Abhishek, Mine, Dan
 
---++ CMS (Burt)
   * Mediocre weekend, didn&#39;t get the lumi increase as expected
   * Job slots were ~100% filled last week 
   * Job statistics for last week
      * ~79 khours/day
      * 189540 Jobs/day
      * 97% success 
   * Transfer statisics for last week
      * ~48 TB/day 
   * Reported Issue
      * LIGO has more diskspace available now

---++ Atlas (Armen &amp; Xin)

   * General production status
      * LHC operation continues as previous week, mixture of commissioning and physics run. New achievement is 13 bunches per beam. Overall ATLAS collected luminosity ~15 nb-1. Overall ATLAS production, after the reprocessing, on the first half of the week was low, going down to a level of 1-2K on the second half. Waiting for the new defined simulation jobs. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2.3M jobs, with CPU/Walltime ratio of 70%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 427k managed MC production, validation and reprocessing jobs 
         * average 61K jobs per day
         * failed 97K jobs
         * average efficiency:  jobs  - 82%,  walltime - 76%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was about the same as the previous week, 200~400TB/day.  
   * Issues
      * Opportunistic SE usage for D0 : UTA T2 still has authentication problem. 
      * Request to have a standalone and lightweight GIP/CEmon package, to allow USATLAS T3 sites to publish only the SE info to BDII.  

---++ LIGO (Britta, Rob E.)
---+++ Gratia Reports
   * Last week&#39;s total usage: 5 users utilized 38 sites
      * 59792 jobs total (23165 / 36627 = 38.7% success)
      * 590568.1 wall clock hours total (460984.9 / 129583.3 = 78.1% success)
   * This week&#39;s total usage: 5 users utilized 35 sites
      * 59831 jobs total (23170 / 36661 = 38.7% success);
      * 590894.5 wall clock hours total (461017.3 / 129877.2 = 78.0% success);

---+++ LIGO / E@OSG
   * Recent Average Credit (RAC):1,038,302.18178,  Last week: 1,169,942.99571
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0)

---+++LIGO/INSPIRAL
   * LIGO submit host:s6 test work-flow ran on LIGO_CIT  
   * OSG submit host: work-flow fails: library dependency issue
   * started Firefly testing
---++ OSG Operations (Rob Q.)

   * I will be traveling to FNAL during this meeting slot and will not be on the call. If anyone has Operations questions or concerns please contact me via email. 

---+++ Operations Last Week 
   * Last Week&#39;s..
      * [[http://tinyurl.com/2epzezd][Reliability/Availability of GOC Services]]
      * [[http://tinyurl.com/2b6vozc][Reliability/Availability of Security Services]]
   * *BDII*
      * is2.grid.iu.edu was returned to production at 2pm EDT today - no known problems [[https://ticket.grid.iu.edu/goc/viewer?id=8530][Related Ticket]]
         * Reminder: There is now monitoring that to warn us if more than 10% of the resources drop out of either BDII. 
   * !TWiki 
      * GOC set up mirror non-production instance to allow docs group to do testing -- ongoing
   * [[http://osggoc.blogspot.com/2010/05/osg-docdb-outage-resolved.html][OSG DocDB outage resolved by FNAL]]
   * [[http://osggoc.blogspot.com/2010/05/goc-service-update-tuesday-may-25th-at.html][GOC - ITB service updates completed]]

   * Nothing to report

---+++ Operations This Week
   * [[http://myosg.grid.iu.edu/map?all_sites=on&amp;active=on&amp;active_value=1&amp;disable_value=1&amp;gridtype=on&amp;gridtype_1=on][Operations RSV Status Map]]
      * The GOC has taken a renewed effort to use our monitoring tools to promote the health of the OSG with some success.
         *  
   * *GOC Production Service Update May 25th 14:00 UTC*
      * [[http://osggoc.blogspot.com/2010/05/goc-service-update-tuesday-may-25th-at.html][Full Announcement]]
      * NCSD will be tweaked on is1 and is2
      * VMhost will be restarted, short outages [~5 minutes] expected on most services
   * *Ticket Exchange*
      * GGUS - no updates, working properly
      * BNL  - no updates, working properly
      * FNAL - Further tests using FNAL provided script, still no success to report
   * *!TWiki*
      * GOC set up mirror non-production instance to allow docs group to do testing -- ongoing

---++++ Gratia and !ReSS (Represented by Fermigrid Ops)

   * Nothing to report


---++ Engage (Mats, John, Chris)

10 users utilized 34 sites;

12340 jobs total (9277 / 3063 = 75.2% success);

45662.2 wall clock hours total (37585.1 / 8077.1 = 82.3% success);



---++ Integration (Suchandra)
   * Waiting for xrootd changes before moving to VTB
   * Documentation efforts ongoing, pages updated and contacting owners/reviewers now

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.9
      *       80 (3) OSG 1.2.X resources (      18 are 1.2.9)
      *        6 (0) OSG 1.0.X resources (       1 are 1.0.6)
      *        7 (0) OSG 1.0.0 resources
      *        1 (0) OSG 0.8.0 resources



---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)

   * D0
      * Good MC production rate last week: 10.7 Million Events/week; 115,000 wall-hours/day at 87% efficiency. D0 faced internal infrastructure problems in last few days.
      * A D0 collaborator from France using SAGA API (http://www.ogf.org/documents/GFD.144.pdf), facing site failures; multiple tickets open (Notre Dame, Texas Tech, Florida, GUMS team). Abhishek has added Joel to the tickets.
      * Ongoing sites related issues:
         * Nebraska: !GridFTP failures fixed after a restart at site-side.
         * UTA: Robert/FNAL&#39;s retry failed. Abhishek spoke with Mark Sosebee earlier today; it is a VOMS/GUMS FQAN mapping issue. D0 teams are using different FQANs for SE and CE workflows. Will confirm with Joel; and possibly recommend inclusion in OSG distribution&#39;s template.
         * GLUE-X site: Investigation ongoing.

   * Fermilab-VO
      * Estimated projections for all sub-VOs are available for the next 2 calendar years. Already posted for CY 2010; Steve will also send 2011 and 2012 projections to us.

   * SBGrid/NEBioGrid
      * Evaluating !GlideinWMS; using the factory provisioned by UCSD. 
      * SBGrid uses NAT. Condor/GlideinWMS did not directly support this network topology; Dan Bradley from Condor team supplied a quick patch. Has been tested and now deployed by SBGrid. 
   
   * Encouraging VO managers to evaluate potential use of the new script interface. These scripts are expected to help VO&#39;s members to get grid certificate and to import it as a PKCS12 file into web browsers.
      * https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/CertificateGetCmd
      * VOs Group&#39;s Feedback to OSG Security -- A drawback with the new mechanism: It requires an additional overhead of installing OSG stack components using Pacman, unlike the browser based mechanism. End-users are less likely to have access to such centrally-installed software stack. Browser based mechanism is standalone and simpler to operate; it is confusing due to the need to create and remember 3 passwords. 
   
   * Inviting annual stakeholder updates to Plans, Needs, Requirements.
      * Suggested template is [[https://twiki.grid.iu.edu/twiki/pub/Trash/Trash/Trash/Trash/VirtualOrganizations/Stakeholder_PlansNeedsRequirements/OSG_StakeholderVO_Input_to_EB_and_Council_June2010Template.doc][here]].


---++ Security (Mine)

---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * There were some issues reported on Atlas sites this week involving storage filling up at some sites. Several sites were temporarily banned from BNL and data transfers were temporarily halted. Atlas runs a centralized data distribution mechanism but the centralized delete mechanism is not working as efficiently. Work is ongoing to address this issue (Armen)
   * There was a Gratia outage on June 3 that was discovered first thing in the morning. This was due to a RedHat / MySQL problem where MySQL can take longer to restart than RedHat allows. This problem has been fixed. No data was lost. The FNAL monitoring / alarm system worked correctly. (Rob)
   * Scott Teige will be Arvind&#39;s replacement at the GOC. (Rob)
   * It was decided not to upgrade the CEMON Collector since stability has been requested by CMS, and no urgent reasons were found in the release notes to justify an upgrade at this time. (Rob)

---++ Attendees:
   * Armen, Britta, Rob E., Brian, Suchandra, Tony, Rob Q., Mine, Dan
 
---++ CMS (Burt, Tony)
   * Job statistics for last week
      * ~84 khours/day
      * 25248 jobs/day
      * 98% success for cms production
      * 95% success

   * Transfer statisics for last week
      * ~27 TB/day
      * Currently up around 46 TB today

---++ Atlas (Armen &amp; Xin)

   * General production status
      * First half of the week LHC had a technical stop, followed by commissioning. Pretty uneventful week for physics. Data taking was only on Saturday, about 1nb-1 luminosity. Overall collected luminosity for ATLAS so far is about 17nb-1. In coming weeks the physics run portion will be increased to 5days/week. ATLAS production was low on the first half of the week, then picked up to the normal level of about 8K running jobs, mainly simulation jobs.After reprocessing campaign ongoing discussion on tools for more dynamic deletion for the sites running out of space.
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2.5M jobs, with CPU/Walltime ratio of 68%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 612k managed MC production, validation and reprocessing jobs 
         * average 87K jobs per day
         * failed 54K jobs
         * average efficiency:  jobs  - 92%,  walltime - 95%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was 250~550TB/day.  
   * Issues
      * Opportunistic SE usage for D0 : D0 data transfer test succeeds at UTA, next step is to run test jobs using the data. 

---++ LIGO (Britta, Rob E.)
---+++ Gratia Reports
   * Last week&#39;s total usage: 4 users utilized 35 sites
      * 60598 jobs total (22316 / 38282 = 36.8% success)
      * 242896.4 wall clock hours total (189971.8 / 52924.5 = 78.2% success)
   * This week&#39;s total usage: 4 users utilized 36 sites
      *  58214 jobs total (28627 / 29587 = 49.2% success)
      * 324529.4 wall clock hours total (275016.6 / 49512.8 = 84.7% success)
---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 949,356.17174, Last week:966,692.08032
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0) 
---+++LIGO/INSPIRAL
   * Two patches in LIGO code for OSG submission submitted, waiting for approval 
   * Found bug in latest LIGO code, testing patch

---++ OSG Operations (Rob Q.)


---+++ Operations Last Week 
   * Last Week&#39;s..
      * [[https://twiki.grid.iu.edu/bin/view/Operations/Minutes2010May24][Meeting Minutes]] (Note: This is two weeks old due to the holiday)
      * [[http://tinyurl.com/2epzezd][Reliability/Availability of GOC Services]]
      * [[http://tinyurl.com/2g3d2vy][Reliability/Availability of Security Services]]
   * *ITB Update Tuesday, the 1st complete*
      * Routine update -- [[http://osggoc.blogspot.com/2010/06/revised-goc-service-update-tuesday-june.html][detailed notes]]
   * !ReSS underwent a short maintenance on 5/28 when a motherboard was replaced on ress02. This was reported to be successful.  High availability features kept all ReSS features accessible during this time.
   * MySQL security  on June 3rd 7:30 am - 9:00 am caused an outage of all production OSG Gratia collectors when production OSG database failed to restart mysql automatically.  Reporting was not affected, more details in GOC ticket 8675.

---+++ Operations This Week
   * *GOC personnel changes*
      * Welcome Scott Teige to Operations-Infrastructure group! Arvind departing ...
   * [[http://myosg.grid.iu.edu/map?all_sites=on&amp;active=on&amp;active_value=1&amp;disable_value=1&amp;gridtype=on&amp;gridtype_1=on][&lt;strong&gt;Operations RSV Status Map&lt;/strong&gt;]]
   * *Production Update Tuesday, 8th* 
      * Routine update -- [[http://osggoc.blogspot.com/2010/06/revised-goc-service-update-tuesday-june.html][detailed notes]]
         * Per management request, CEMon Collector upgrade has been indefinitely postponed
      * Resource Provider-only VOs like NERSC can now register
      * Implemented /navigator2 - this will replace /navigator and /mytickets in the coming weeks. Check [[https://ticket.grid.iu.edu/goc/navigator2][ITB link]] if interested
      * Reorganized ticket viewer for non-editors and added description update field along with current CC editor
   * *Ticket Exchange*
      * FNAL - Further tests using FNAL provided script, still no success to report yet


---++ Engage (Mats, John, Chris)

7 users utilized 31 sites;

35011 jobs total (32098 / 2913 = 91.7% success);

150956.9 wall clock hours total (114089.8 / 36867.0 = 75.6% success);

We will not be able to make the call this week.


---++ Integration (Suchandra)
   * Currently testing ITB 1.1.22 release
      * Primarily xrootd focused
      * Should be ready for release in the next day or two
   * ITB robot
      * MyProxy integration underway
      * Investigating web reporting infrastructure suggested by Maxim
    
 
---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.9
      *       80 (1) OSG 1.2.X resources (      25 are 1.2.9)
      *        5 (-1) OSG 1.0.X resources (       0 are 1.0.6)
      *        7 (0) OSG 1.0.0 resources
      *        1 (0) OSG 0.8.0 resources
Site Coordination meeting this Thursday 6/10 at 11am central
   * Phone: 510-665-5437, #1212
   * Adobe connect: http://osg.acrobat.com/osgsc100610/
   * Special topic will be data movement in OSG


---++ Virtual Organizations Group (Abhishek)

   * D0
      * Good MC production rate last week: 11 Million Events/week. 108,000 wall hours/day at 86% efficiency. 
      * Sites related issues:
         * Pre-emption/eviction more prominent again: 
            * Purdue RCAC. Ticket: https://ticket.grid.iu.edu/goc/viewer?id=8686
            * Caltech, Florida, MIT.
            * Burt/CMS confirmed that site policy has not changed at these Tier2&#39;s.
            * Abhishek and Dan to discuss further with Joel.
      * Potential new SEs / opportunistic storage
         * Follow up expected with Cornell in coming week.
         * Working with GLUE-X team to enable its UConn SE for D0. 
   
   * Fermilab-VO
      * Many sub-VOs indicating a need to make transition to SL 5.
      
   * GLUE-X 
      * Problem with Glide-in&#39;s.
         * Ticket: https://ticket.grid.iu.edu/goc/viewer?id=8579
         * Burt/GlideinWMS team are helping investigate.
      * GLUE-X hardware may be suited for multi-core/multi-threaded HTPC applications. 
         * Dan and Abhishek following up with Richard to explore the opportunity.   
   
   * OSG-VO (NIH/JHU CHARMM group)
      * Working with BNL !PanDA team to expand CHARMM production from 3 to 25 sites. 
      * Problems in last few weeks with certificates at CERN based !PanDA server; being resolved by BNL team.
   
   * !CompBioGrid and GLUE-X looking to discuss possibility of Campus Grid at Univ of Connecticut.         

---++ Security (Mine)
-- Main.BrittaDaudert - 07 Jun 2010

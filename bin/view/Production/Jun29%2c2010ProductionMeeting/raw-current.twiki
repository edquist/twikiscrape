-- Main.DanFraser - 07 Jun 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * There was a production problem noted last week by some of the VOs that was traced to the GlideinWMS factory running a development release of Condor, in which there was a bug in the detection of GT2 vs GT5 sites. This has been fixed by rolling back to an earlier version, but we need to plan for the future and consider operating a Glide-inWMS factory as a &quot;production&quot; service. (Burt) 
   * There has been significant progress on the problem of adding SEs to the BDII without a CE. The first version will be for Bestman SEs. Testing of the end to end solution is targeted to begin on July 6. Xin suggested that there will be some T3s that may be interested in testing this when the new SE package is available. (Dan)
   * Pakiti server is ready and installed from the source code. Anand is working with Rob to ensure it meets our expectations. (Mine)
   * UTA SE is now being used by D0. Hooray, Hooray. (Abhishek)
   * There will be a Site Administrators meeting on Aug 10-11 at Vanderbilt. (Marco)

---++ Attendees:
   * Mats, Xin, Armen, Britta, Rob E., Derek, Suchandra, Burt, Marco, Abhishek, Rob Q., Dan
 
---++ CMS (Burt)


---++ Atlas (Armen &amp; Xin)

   * General production status
      * LHC is back online with improved proton intensity per bunches. Since weekend managed to double the existing integrated luminosity to about 40nb-1. Overall ATLAS production was up and down during the week and was generally low. Still waiting for new simulation samples.
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2.6M jobs, with CPU/Walltime ratio of 67%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 308k managed MC production, validation and reprocessing jobs 
         * average 44K jobs per day
         * failed 24K jobs
         * average efficiency:  jobs  - 93%,  walltime - 94%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was ~400TB/day, low at ~100TB/day over the weekend.   
   * Issues
      * Opportunistic CE usage for CDF : will set a testbed internally, with all extra rpms installed, to verify they don&#39;t break other VO (ATLAS) jobs. 
      * SAM test issue --- sam bdii under investigation
      * Publish SE only info to BDII --- quick release of GIP from GIP group, CEMon still needs to be done. 

---++ LIGO (Britta, Robert E.)
---+++ Gratia Reports
   * This week&#39;s total usage: 3 users utilized 38 sites
      * 59452 jobs total (28596 / 30856 = 48.1% success)
      * 627850.3 wall clock hours total (436354.5 / 191495.8 = 69.5% success)
   * Last week&#39;s total usage: 5 users utilized 39 sites
      * 61465 jobs total (22162 / 39303 = 36.1% success)
      * 579260.7 wall clock hours total (318382.1 / 260878.6 = 55.0% success)
---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,224,179.24924, Last week: 924,1,157,951.14
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0)

---+++LIGO/INSPIRAL
   * Bug fix tested, waiting for approval to push
   * Trouble shooting USCMS-FNAL-WC1-CE3 fail

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
      * [[http://tinyurl.com/2cpq82y][Reliability/Availability of GOC Services]]
      * [[http://tinyurl.com/28gecm2][Reliability/Availability of Security Services]]
   * *Production Update Tuesday, June 23rd*
      * [[http://osggoc.blogspot.com/2010/06/goc-service-update-tuesday-june-22nd-at.html][Announcement]]
   * *GGUS Production Update*
      * The [[https://gus.fzk.de/][GGUS ticketing system]] will be down Wednesday, June 23rd 2010 from 06:00 - 09:00 UTC for a new GGUS portal release.
         * All ALARM Testing was successful. 
   * CA Distribution Outage Friday for ~30 minutes
      * Due to errors in the CA Package (1.15) released to the GOC
      * When no immediate solution was presented we rolled back to the previous good version (1.13) 

---+++ Operations This Week

   * No Scheduled Release this week as it is a 5th Tuesday 
   * [[http://myosg.grid.iu.edu/map?all_sites=on&amp;active=on&amp;active_value=1&amp;disable_value=1&amp;gridtype=on&amp;gridtype_1=on][&lt;strong&gt;Operations RSV Status Map&lt;/strong&gt;]]
   * *Ticket Exchange*
      * FNAL - No Action This Week as Soichi is on Vacation
   * Investigating ATLAS resources disappearing from BDII
      * [[https://gus.fzk.de/ws/ticket_info.php?ticket=59188][GGUS Ticket]]
      * Initial Testing of the SAM-BDII vs the Top Level WLCG BDII
         * # of Tests - 56315
         * # of Failures Top Level BDII - 74 (0.13%)
         * # of Failures SAM BDII - 219 (0.38%)
   * SE Only Publishing is in the hands of the Software Tools Group and ATLAS for testing
      * GOC Change in BDII that would allow SE publishing is currently set for July 6th

---++++ Gratia and !ReSS (Represented by Fermigrid Ops)

   * A Gratia software upgrade is in the works, an announcement will precede it. This will be more than a week out from today. 


---++ Engage (Mats, John)
Current week&#39;s total usage: 14 users utilized 37 sites;

16653 jobs total (12164 / 4489 = 73.0% success);

98746.8 wall clock hours total (62752.6 / 35994.1 = 63.5% success);

Got a handle on the job failures, production is ramping back up.


---++ Integration (Suchandra)
   * Finishing up ITB testing
      * Should be ready for release by thursday

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.10
      *       78 (1) OSG 1.2.X resources (       8 are 1.2.10)
      *        6 (0) OSG 1.0.X resources (       0 are 1.0.6)
      *        7 (1) OSG 1.0.0 resources
      *        1 (0) OSG 0.8.0 resources
Site Coordination meeting this Thursday 7/8 at 11am central
   * Phone: 510-665-5437, #1212
   * Adobe connect: http://osg.acrobat.com/osgsc100708/
   * Special topic will be planning for August Site Administrators meeting


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)

---+++ D0
   * Monte carlo production ongoing at good rate.
   * 11.6 M Evts/week. 99,000 hours/day at 91% efficiency.
   * Site related issues:
      * Low efficiencies at Purdue RCAC, and UNL clusters.
      * UNL issues related to D0 SAM server; being looked into by D0.
      * UTA SE being used by D0 now. 

---+++ CDF
   * Restarting work on the production expansion plan to use more OSG sites opportunistically. 
   * Need additional packages on SL5 sites. Already deployed at Fermilab resources. 
      * Mine/OSG-Security has pointed out Pakiti&#39;s package query feature. Discussion ongoing.

---+++ SBGrid
   * Continuing work with !GlideinWMS. 
   * Affected by Condor 7.5 bug at UCSD gWMS factory. 
      * Error in interpretation of long string in GLIDEIN_Collector.
      * Reverted back to Condor 7.4.2 soon after.
   * Jaime/Condor team has provided Igor/gWMS with a patched version of 7.5.2 that addresses the GRAM2/GRAM5 misinterpretation.
      * UCSD gWMS factory planning upgrade to Condor 7.5.2.
   * Currently, working to understand Gratia accounting of glide-in jobs.

---+++ GLUE-X
   * Facilitating D0&#39;s use of CE and SE at GLUE-X site UConn-OSG. 
   * Investigated dropped connections issue with Glidein&#39;s. 
   * Univ of Connecticut network team will try to extend timeout for Subnet, and modify Session TTL. 
   * Richard asked for more input on a good setting for TTL. 
      * After input from VOs Group and Fermi-VO, GLUE-X agreed to try 72 hours as the new setting. 


---++ Security (Mine)
   * Pakiti server is ready and installed from the source code. Anand is working with Rob to ensure it meets our expectations.
   * GUMS 1.3 is built and tested from teh source code and works correctly. Now VDT is going through the same experiment of building from the source with our notes
   * CDF has asked to use pakiti server in order to discover existing RPMs in a cluster. There is no security needs here. They merely want to know which sources they can use to submit jobs.
   * Question: Pakiti cannot ensure the queries it sends lands on each one of the worker nodes. Is there a way to configure condor to send jobs in a round-robin fashion so each worker node gets queried at least once? If no technical solution is possible, can we at least define what is expected to be in a standard SL5 worker node? is there is a consensus on this, it will be easier to identify suitable sites.

   * there was a problem with CA distribution last week. WE will have a meeting dedicated to this today.

   * There is a security problem with ROCKS distribution. Ticket is assigned to Barlow from security team.

   * Security team met with Atlas tier 3 site (Susquehanna university) and their site security teams. The layout has been approved and there is no objections from the university.

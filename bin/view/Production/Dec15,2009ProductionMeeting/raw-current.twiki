-- Main.DanFraser - 01 Dec 2009
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * Carry over from last week: In follow up on the WLCG Atlas BDII data synch problem, Burt/Xin have been running Ganglia probes to detect the problem but the probes are currently limited to CE. Burt agreed to make some updates to the probe so that it will specifically monitor SE issues that affect FTS. (Burt, Xin)
   * Burt looking at preemption job thrashing situation at MIT (Bug 7814) -- Brian to forward links (from Joe) that show what is happening in more detail. (Brian, Burt)
   * Xin to ping Michael to see if we can find an additional one or two Atlas sites to support opportunistic storage for DZero, primarily in anticipation of ramp up.
   * Burt to ask at CMS T2 meeting for an additional one or two sites on CMS.
   * LIGO increased job throughput by 48%; now running on 38 sites.
   * GEANT4 now running in production, OSG reports 50% job failure rate, whereas Gratia shows a 100% success rate (probably on the pilots). Abhishek to report the OSG numbers and try and identify where the difference is coming from.
   * CDF portal successfully upgraded to use Glide-in WMS for job submission.
   * There will not be a production call on Dec 29.

---++ Attendees:
   * Xin, Armen, Britta, Brian, Burt, Marco, Abhishek, Mine, Dan
 
---++ CMS (Burt)
   * Computing: 99 khour/day, 89% success. CPU/wallclock at 72%.
   * Storage: Tier 1 transferred .7 PB last week (peak of 141 TB/day).
   * OSG: Nearly all at 1.2 (FNAL and some T3s at 1.0; FNAL migrating one gatekeeper this week to 1.2)
   * LIGO preemption issue @ MIT_CMS: is this being solved behind the scenes?
   * CMS deploying SL5-compatible release on OSG
   * LHC shutdown @ 6 pm CET tomorrow.  At earliest, beams at injection back by 19 Feb.

---++ Atlas (Armen &amp; Xin)

   * General production status
      * During the last week USATLAS production was quite stable at the level of 7-8K running jobs, mainly simulation jobs. Reprocessing sample in preparation to validate the sites.
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1158K jobs, with CPU/Walltime ratio of 88%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 1164K managed MC production, validation and reprocessing jobs 
         * average 166K jobs per day
         * failed 71K jobs
         * average efficiency is very good:  jobs  - 94%,  walltime - 97%
   * Data Transfer statistics for last week
      * BNL T1 transferred ~75 TB/day data last week, with peak at 150 TB/day.  
   * Issues and GOC Tickets
      * GOC ticket 7772: Issues with WLCG BDII periodically loses information about some USATLAS Tier2 sites. Ganglia routine check in place, needs script update to check SE info.
      * GOC ticket 7798: Solved.  

---++ LIGO (Britta)
---+++ Einstein at Home

   *  NEW FORMAT   (?)
This weeks Gratia reports show 4 users utilizing 38 sites compared to last week&#39;s 31 sites. 52218 jobs were completed with 86.2% success rate. This is a 48 % increase form last week. Wall clock hour consumption lies at 423634.2 with 95.1% success rate. Einstein at Home reports 867,960.74835 recent average credits (RAC) which keeps the Caltech Open Science grid team at rank three in the list of top participants this week. The rank based on accumulated credits stays at 11.  
Work is progressing to expand to Fermilab sites and Sprace. Otherwise LIGO is utilizing all the cpu cycles that are available on operational grid resources that support LIGO.
Ten GOC tickets are open for LIGO. For details please see:  https://ticket.grid.iu.edu/goc/list/open .

   * OLD FORMAT

   * Gratia reports:
      * 4 users utilized 38 sites
      * 52218 jobs total (45026 / 7192 = 86.2% success)
      * 423634.2 wall clock hours total (402913.4 / 20720.8 = 95.1% success
      * Last week: 5 users utilized 31 sites
      * 35221 jobs total (29847 / 5374 = 84.7% success)
      * 289669.3 wall clock hours total (260581.5 / 29087.8 = 90.0% success)
 
   * E@H reports
      * Recent Average Credit (RAC): 702,805.33843
      * E@H rank based on RAC: 3 (+0)
      * E@H rank based on accumulated Credits: 11 (+0)
   
   * Robert is working on code changes required to expand to Fermilab sites and Sprace
   * Running up to capacity on all other operational sites
   * Ten GOC tickets are currently open for LIGO.
 
---+++ Binary Inspiral
   * small test work-flow fails
   * ff-grid.unl.edu resolves its own hostname to the internal address, that causes transfer of executable to fail
   * waiting for fix
 
---++ Engage (John, Chris)


---++ Integration (Suchandra)
   * Started ITB cycle
      * Plan to complete before Christmas
      * Minimal changes
   * Progress on ITB/Panda test infrastructure
      * Have basic testing jobs running on UC_ITB site / proof of concept 
      * Working on adding further sites
      * Expanding testing to encompass a simple example job (transfer from se/computation/transfer out to SE)


---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.4
      *       48 OSG 1.2.X resources (      13 are 1.2.4)
      *       15 OSG 1.0.X resources (       7 are 1.0.4)
      *       22 OSG 1.0.0 resources
      *        2 OSG 0.8.0 resources
         * OU_OCHEP_SWT2, tier2-01.ochep.ou.edu , Contact: Horst Severini
         * UIC_PHYSICS mstr1.cluster.phy.uic.edu , Contact: John Wolosuk
   * Site coordination phone meeting Thursday 12/17, 11:00am Central:
      * Resources to get support from OSG or your VO
      * https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/SiteCoordination/SitesCoord091217
      * Phone: 510-665-5437, #1212
      * Adobe connect: http://osg.acrobat.com/osgsc091203/ 

---++ Grid Operations Center (Rob Q.) - vacation

---++ Virtual Organizations Group (Abhishek)

---+++ VOs with Currently High Activity

   * D0 MC reported good production. 
      * 9 M Evts last week. D0 reported 11.4 M Evts; will confirm. 50,000 wall hours at 92% efficiency. 
      * Things running smoothly, but below capacity. Workload still low. 
      * Notable Items:
         * ATLAS: Need for more SEs.
            * D0 can benefit from more opportunistic SEs at ATLAS T2 sites. 
            * Currently 2 SEs: MSU, MWT2-IU.
         * CMS: CE rate of preemption. 
            * Waiting for MIT T2 to apply the fix to CE. 
            * Dan Bradley&#39;s solution: use &lt;u&gt; !LastHeardFrom &lt;/u&gt; instead of the more popular &lt;u&gt; !CurrentTime &lt;/u&gt;.
            * https://ticket.grid.iu.edu/goc/viewer?id=7814
         * CMS: Need for more SEs.
            * D0 can benefit from more opportunistic SEs at CMS T2 sites. 
            * Currently 3 SEs: Purdue, UCSD, UNL.
           
   * SBGrid/NEBioGrid (carried over)
      * https://ticket.grid.iu.edu/goc/viewer?id=7724
      * https://ticket.grid.iu.edu/goc/viewer?id=7781
      * Python 2.4 required; discussion with Alain/OSG-Software; resolved.
      * Convergence needed on WMS: OSG !MatchMaker or !GlideinWMS.
      * Phone meeting coming up, Dec16th. 
         * To be discussed and prioritized: 
            * accounting discrepancy
            * lack of monitoring tools
            * need for more OSG-MM expertise
      * Bookmark: https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/SBGrid_NEBioGrid_OSG     
  
   * GEANT4
      * Biannual EGEE-based exercise now also running on OSG. 
         * Scale is low; large fraction provided by EGEE. 
         * Intial roadblocks resolved; production running.
      * Wasted wall hours fraction
         * 50% in OSG resource-view, 0% (full success) in Geant4-view.
         * Possibly, an exit-code discrepancy due to Pilots.

   * CDF 
      * Successfully upgraded new portal to !GlideinWMS.
      * SL5 evaluation going well. 

---+++ VOs with Limited Activity
      
   * !GridUNESP / DOSAR
      * Bringing up !GridUNESP as community grid VO.
      * Infrastructure being configured with OSG software stack (site side, then VO side).
      * Working on charter for !GridUNESP to start VO application process.   
      
   * Great Plains Network / GPN
      * User from Univ of Arkansas going through Engage VO.
      * Expected to start submitting jobs soon.

   * NYSGrid
      * Interested in GPU cluster with CS dept at SUNY-Buffalo.
      * Exploring !Einstein@Home. Continuing work on Hub; using HUBzero.
   
   * !IceCube
      * Proof of principle completed in Oct&#39;09. Limited data-access model.
      * Work to start on HTTP cache / Squid optimization.
      * Progress slow; !IceCube team on travel to South Pole.

---++ Security (Mine)

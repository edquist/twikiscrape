-- Main.DanFraser - 13 Apr 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * New peak in DZero jobs this week, 13.3 Million Events, 120K hours last week. (Abhishek)
   * Root Cause Analysis of Gratia problem on April 20 affecting the OSG Display is ongoing. Main problem is with a known problem in mySQL.  (Dan)
   * SBGRID overloading issues at FNAL and UNL have been resolved, jobs running normally again (Abhishek, Tony)

---++ Attendees:
   * Mats, Xin, Armen, Britta, Rob E., Brian, Suchandra, Tony, Marco, Abhishek, Rob Q., Mine, Dan
 
---++ CMS (Burt)
   * Technical stop was successfully completed by the end of 4/28/2010, operation was resumed 4/29/2010
      * Record rates at 900 GeV center-of-mass energy
      * Over the weekend, increased the intensity per bunch to a few 1E11 protons per bunch (normal is a few 1E10), took data at record rates (average was 500 Hz with peaks at &gt; 1 kHz after trigger, nominal rate is 200 Hz, higher rates are feasible for short times)
      * Resuming higher intensity collisions at 7 TeV
   * Job statistics for last week
      * 47 khours/day
      * 128,451 Jobs/day
      * 95% success
   * Transfer statistics for last week
      * ~25 TB/day

---++ Atlas (Armen &amp; Xin)

   * General production status
      * Reprocessing of both Data and MC is done without problems, and the ATLAS production was at the average level of 8-9k running jobs. Distribution of reprocessed data between clouds and within the cloud is going on. LHC had a technical stop Mon-Wed. Then preparation for high intensity runs at 900GeV. Sun-Mon stable beams, high intensity at 1E11/bunch at 900GeV. For the week continue commissioning for high intensity run at 7TeV.
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.7M jobs, with CPU/Walltime ratio of 86%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 1M managed MC production, validation and reprocessing jobs 
         * average 148K jobs per day
         * failed 139K jobs
         * average efficiency:  jobs  - 88%,  walltime - 90%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate is 200~350TB/day last week. 
   * Issues
      * Opportunistic SE usage for D0 : asked D0 (Robert) to re-test the site after the mapping is changed. 

---++ LIGO (Britta, Rob E.)
---+++ Gratia Reports
   * Current week&#39;s total usage: 4 users utilized 40 sites
      * 376279 jobs total (125852 / 250427 = 33.4% success)
      * 2649282.3 wall clock hours total (2166518.3 / 482763.9 = 81.8% success)
   * Last week&#39;s total usage: 6 users utilized 36 sites
      * 91368 jobs total (26386 / 64982 = 28.9% success)
      * 598492.8 wall clock hours total (478053.4 / 120439.4 = 79.9% success)

---+++ LIGO / E@H
   * Recent Average Credit (RAC): 1,574,770.42532, Last Week: 1,794,375.88166
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0) 
   * Google ping issue fixed ( needs confirmation from Burt )
   * NFS issue on Nebraska fixed ( needs confirmation from Brian )

   * high production
      * GridUNESP_CENTRAL (up to 1,000 jobs)
      * USCMS-FNAL-WC1-CE3 (up to 4,000 jobs)
      * Purdue-RCAC (400 jobs)

   * low production
      * LIGO_UWM_NEMO (1 job, -99%)
      * UFlorida-HPC (64 jobs, -90%)
      * Firefly (50 jobs, -95%)
      * CIT_CMS_T2 / CIT_CMS_T2B ( 0 jobs, lots of jobs suspended  )
      * UCSDT2 / UCSDT2B ( 4 jobs, open ticket )

   * the graphs below show the distribution of E@OSG jobs on OSG resources.

&lt;a style=&quot;margin: 20px 20px 20px 20px;&quot; href=&quot;%ATTACHURLPATH%/EH.jpg&quot;&gt;&lt;img width=400 height=200 src=&quot;%ATTACHURLPATH%/EH.jpg&quot;/&gt;&lt;/a&gt;
%BR%
&lt;a style=&quot;margin: 20px 20px 20px 20px;&quot; href=&quot;%ATTACHURLPATH%/EH-NO-CMS-TIER1.jpg&quot;&gt;&lt;img width=400 height=200 src=&quot;%ATTACHURLPATH%/EH-NO-CMS-TIER1.jpg&quot;/&gt;&lt;/a&gt;
 
---+++ LIGO / INSPIRAL
   * Waiting for s6code

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Availability metrics for the last week 
      * [[http://tinyurl.com/36djzbq][GOC Services: BDII, MyOSG, RSV Collector, OSG Display]]
      * [[http://tinyurl.com/37aam66][GOC hosted Security services managed by OSG security team]]
         * Ticket 8517 - Problem with yum update process on CA Distribution probe, Anand is looking into it for the security group. Yum errrors may be related to a larger RH issue. 
   * [[http://osggoc.blogspot.com/2010/04/goc-service-update-tuesday-april-27th.html][GOC Production Service update]] -- complete; one-two minute outages experienced due to VM reconfiguration.
      * No bugs reported, no outages detected by monitoring as they were very short. 

---+++ Operations This Week
   * *GOC ITB Service update*: On May 4th 2010
      * Production Release on May 11th [[http://osggoc.blogspot.com/2010/05/goc-service-update-tuesday-may-11th-at.html][Notification]] with change log went out around lunchtime today.
      * WLCG BDII Monitoring will be included. This will just be a display of the results at this point, alarming will be handled at a later point, when we are more familiar with the monitoring results. 
   * *Ongoing - Top Level WLCG BDII Monitoring* 
      * MyOSG GIP-validation view modified to include these results; expect release on May 11th.
   * *Ongoing - Ticket Exchange (TX)*: 
      * *FNAL Remedy* 
         * No Change
         * Tony Tiradani (USCMS) reports problems closing Remedy tickets when GOCTicket is closed under current TX mechanism. Tony has requested priority bump at FNAL. 
         * Fermigrid and GOC will do some testing after this call to see if they can get to the bottom of this issue. 

---++ Engage (Mats, John, Chris)

11 users utilized 28 sites

30037 jobs total (27052 / 2985 = 90.1% success)

65434.6 wall clock hours total (37569.2 / 27865.5 = 57.4% success)


We will not be able to make the call today.


---++ Integration (Suchandra)
   * ITB Robot efforts ongoing
       * Adding certificate checks
       * Testing worker node client checks now
       * Adding space token support to SE tests
   * Testing ITB caches that GOC setup this week
   * VTB testing of OSG 1.2.10 anticipated to start next week depending on when xrootd changes go into vdt cache

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.9
      *       79 (2) OSG 1.2.X resources (       6 are 1.2.9)
      *        6 (0) OSG 1.0.X resources (       1 are 1.0.6)
      *        6 (0) OSG 1.0.0 resources
      *        1 (0) OSG 0.8.0 resources
Site Coordination meeting next Thursday 5/13 at 11am central
   * Phone: 510-665-5437, #1212
   * Adobe connect: http://osg.acrobat.com/osgsc100513/ 


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)

   * D0
      * D0 monte-carlo event production recorded a new annual peak of 13.3 million events per week.
      * Nearly 70,000 wall-hours/day at 80% efficiency.
      * Sites related issues:
         * UTA SE access is still pending; D0 has sent a reminder, waiting to hear back from site contacts.
         * Low efficiency at ce01.cmsaf.mit.edu, umiss001.hep.olemiss.edu.
         * Low efficiency at cit-gatekeeper.ultralight.org; now resolved. 
         * New SE at SPRACE is having file delivery problems; work in progress.
      * Track record of event production averages and peaks:
  &lt;align=left&gt;&lt;a href=&quot;%ATTACHURLPATH%/D0_weekly_MC_events_peak_on_OSG_-__05.03.2010.PNG&quot;&gt;&lt;img src=&quot;%ATTACHURLPATH%/D0_weekly_MC_events_peak_on_OSG_-__05.03.2010.PNG&quot; alt=&quot;D0_weekly_MC_events_peak_on_OSG_-__05.03.2010.PNG&quot; width=&#39;500&#39; height=&#39;370&#39; /&gt;&lt;/a&gt;&lt;/align&gt;    

     
   * !IceCube
      * Abhishek working with !IceCube team (Steve B, Juan Carlos), GLOW (Dan B), and OSG areas (Tanya, Igor S) to ramp-up in coming months. 
      * !IceCube adding a dedicated submit infrastructure node. Using Condor DAGs, !GlideinWMS, Squid.
      * Synopsis of accomplished changes in workflow:
         * Jobs refactored with DAGMan to read from Photonic tables in parallel steps.
         * Multiple jobs in the same glide-in. 
         * Each worker node processes a single photonics bin (subset of the full table). 
         * Multiple jobs can re-use the worker node reducing the traffic of copying the bin.
  
   * SBGrid
      * Problems related to GUMS and VO mapping. 
         * Team members are in two VOs: SBGrid and !NEBioGrid. This poses mapping problems at many sites.
      * Background on overloading issues at FNAL T1 and UNL:   
         * SBGrid jobs were attempting to clean ~/.globus/.gass_cache directory (it is typically on NFS).
         * Every job tried to do this every time. 
            * Now there is a locking mechanism in place, so only one job at a time should attempt to do this, and it should only happen ever 48 hours or so.
         * SBGrid wrapper script was self-replicating with wrong filename, causing any recursive calls to it to fail.
            * Now resolved.
         * SBGrid workflow had &#39;curl&#39; operations to stage data.
            * Now site-localized. Including a 1-3 MB binary file with every job, using condor file staging facilities.

---++ Security (Mine)
   * Created a new certificate package complied with new IGTF layout. GOC set up a new ITB CA cache. We will attend the ITB meeting on Th to see how the tests went. We will list which services are tested in ITB. we will identify if there are software that needs testing but not included in ITB. VOMS is en example, which John Weigand will let us test with his instance
   * GUMS bug. Not a real showstopper but if GUMS disconnects from the db, it cannot reconnect again. it does not affect mapping functionality. BNL has problems with building and testing the code. This is a bigger problem than just this bug. 
   * Java end of life. We have a few security vulnerabilities in Java 5. We cannot get patches because Java 5 reached end of life. We should transition to Java 6. 


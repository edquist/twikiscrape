-- Main.DanFraser - 28 Apr 2009
---++ Attendees:
   * Xin, Britta, Mats, Brian, Suchandra, Burt, Abhishek, Rob Q., Dan
---++ CMS
CMS ran about 154 khours/day last week at 96% success.  CPU/wallclock was 78%.  I noticed this week that our T2 sites in Brazil are starting to make a noticeable overall contribution as well to overall totals (although they&#39;re not currently running any centralized production work).
---++ Atlas
Last week USATLAS had been running ~6000 jobs constantly on all
production sites. 

Gratia reports USATLAS sites ran 1,232,133 jobs, with
CPU/Walltime ratio of 80.1%. 

PanDA production report for all ATLAS sites:
   - completed successfully 954,151 managed MC production,
     validation and reprocessing jobs
   - average ~136,307 jobs per day
   - failed   131,075  jobs
   - average efficiency :
      -- jobs     - 87.9%
      -- walltime - 93.8%

Some notes: 
1. There were some temporary buffer fill-up on BNL tape system. 
   Tape drivers have been doubled since then. 
2. We see good improvements on the condor-g submission with 
  the new condor binary. Plan is in place to do more scalability/load 
  tests with condor team in the near future. 

No particular site issues to report, for people interested, below is a
list of issues reported:

1) Tue Apr 21.
   - US/BNL: 525 jobs failed &quot;too many attempts 11&quot;. Known problem,
     data on tape.
   - US/MWT2_UC_MCDISK: Many user DDM errors, RT 12721 fixed:
     dCache headnode rebooted.
2) Wed Apr 22.
   - US/BNL: SE issues fixed (JAVA VM on the dCache admin node 
      was running out of memory), some FT failures due to a problem 
      with SWT2 proxy, still many FT failures due to files-on-tape. 
3) Thu Apr 23.
   - US/UTA_SWT2 set online: maintenance completed, test jobs
     succeeded. Elog 3186.
4) Fri Apr 24.
   - US/BNL many job faiures due to missing 15.0.0 rel. - it was
     patched/reinstalled by Xin.
   - US/AGLT2_MCDISK permission problem,GGUS 48125 solved (dcache
     restart).
5) Sat Apr 25.
   - US/MWT2_IU/IU_OSG set offline.1500 stageout job failures.RT 12755.
   - US/SLACXRD: FT failed to contact SRM. RT 12759 assigned.
   - US/MWT2_UC: &gt;200 job failed: missing input. RT 12760 assigned.
6) Mon Apr 25.
   - US/BNL high failure rate of repro jobs.DDM-related.GGUS 48185 in
     progress.

---++ LIGO
1) E@OSG Project

E@H Reports:

Recent Average Credit (RAC): 65,185.43295
E@H rank based on RAC: 9
E@H rank based on accumulated Credits: 39

Gratia Report


Current week&#39;s total usage: 5 users utilized 19 sites;
11189 jobs total (10545 / 644 = 94.2% success);
30942.7 wall clock hours total (27868.8 / 3073.9 = 90.1% success);

Previous week&#39;s total usage: 5 users utilized 18 sites;
8513 jobs total (6157 / 2356 = 72.3% success);
41189.9 wall clock hours total (36352.1 / 4837.8 = 88.3% success)


* New code (GT2/GT4 submissions):

             * Recent Average Credit (RAC) down, recovering from  DNS
               server outage at AEI

             * Job success up since switch to GT2 only submission

---++ Engagement
Slow week for Engagement, probably due to the time of the year (end of semesters).


------------------------------------------------------------------------
    | VO             | # of Jobs | Wall Dur. | Cpu / Wall |      Delta
------------------------------------------------------------------------
10  | engage         |     5,973 |     5,450 |       43.5 |        -31



We are not sure yet why the efficiency was so low.

---++ Metrics
1) Installed capacity progress: BDII improvements being tested on ITB; Karthik and Arvind are collaborating on it.  Their working page is here:
 - https://twiki.grid.iu.edu/bin/view/Operations/BdiiInstalledCapacityValidation
 - This deliverable seems like it will make it on time.
 - The deliverable for supporting the storage information in the BDII has gone into the OSG 1.0.1
 - Per our agreement with the WLCG, the first preliminary report goes out May covering April.  It&#39;ll be nice to see in order to understand how our measuring works against theirs.  Haven&#39;t heard much from the WLCG about the report format itself.
2) A lot of this work is currently blocked by the development of OIMv2.  I&#39;d really like to hear the current deadline and &quot;blocking issues&quot; for that.
 - Currently working to adopt the RSV reports to the new OIMv2 schema.
3) Internal Metrics: progressing fine; I&#39;ve talked with most of the area coordinators. I&#39;m missing feedback from Mine, Rob Q, and Rob G; if they&#39;re on this call, I&#39;d like to give them a friendly reminder.

---++ Virtual Organizations Group

Need input from Abhishek here.

---++ Site Support, Integration
Integration:
The osg 1.0.1 update was released on monday.  The release process ran into several problems in the final stages.  There will be a post-mortem meeting this thursday to come up with ways to streamline the release process and the testing process in general.

Sites:

The Sites coordination and integration groups will be working with sites installing the 1.0.1 update through campfire and the mailing lists to try make sure that no problems arise and in order to get feedback on some the documentation and update/installation procedures being given to sites.

---++ Grid Operations Center

---++ Security

Not present.

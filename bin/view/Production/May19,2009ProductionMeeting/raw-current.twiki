-- Main.DanFraser - 18 May 2009
---++ Action Items:
   * Need to explore how to clean up &quot;truth in advertisement&quot; -- making sure manually entered info from sites is up to date (Dan)
   * Need to explore the Nanohub (non-production) issues in more detail (Abhishek, Rob, Dan)

---++ Attendees:
   * Xin, Armen, Britta, Brian, Suchandra, Abhishek, Rob Q., Mine, Chander, Miron, Dan
---++ CMS (Burt)

   * Burt is on vacation; Brian contributed the job statistics and note.
   * We ran an average of 139k hours / day; this is a large increase from last week.
   * Job efficiency: 94%.  CPU/wallclock efficiency was 74%.
   * The Nebraska T2 is now able to access (and assist in grid-enabling) a large resource in Omaha, Nebraska (about 6,000 cores).  This will possibly provide a large amount of opportunistic CPU.

---++ Atlas (Armen &amp; Xin)

   * job statistics for last week. 
      * Gratia report: USATLAS ran 1.1M jobs, with CPU/Walltime ratio of 78.5%, which is slightly lower than last week but basically consistent. 
      * PanDA world-wide production report (real jobs):
         * completed successfully 470,701 managed MC production, validation and reprocessing jobs
         * average  ~67,243 jobs per day
         * failed   95,252  jobs
         * average efficiency: 83.2% for jobs and 93.1% for walltime
   * Sites
         * BNL T1 had a one day maintenance on Monday upgrading dCache pnfs server, production slowed down due to halt of data transfers. Afterward, production ramped up to normal level, and keep at &gt;6000 running jobs the rest of the week.   
   * Panda Server migration to CERN
         * It&#39;s officially announced that the migration of Panda Server to CERN was finished early last week. 


---++ LIGO (Britta)

Current week&#39;s total usage: 4 users utilized 17 sites; &lt;br&gt;
                             5439 jobs total (4091 / 1348 = 75.2% success); &lt;br&gt;
                             30678.0 wall clock hours total (26087.2 / 4590.9 = 85.0% success); &lt;br&gt;
Previous week&#39;s total usage: 2 users utilized 19 sites;&lt;br&gt;
                             18362 jobs total (17066 / 1296 = 92.9% success); &lt;br&gt;
                             52669.9 wall clock hours total (47203.2 / 5466.6 = 89.6% success);&lt;br&gt;


Recent Average Credit (RAC): 114,113.47957 (+3,000)&lt;br&gt;
E@H rank based on RAC: 10 (-1)&lt;br&gt;
E@H rank based on accumulated Credits: 36 (+2)&lt;br&gt;


ligo submit host went down 05/15 -05/17

Robert testing new code that deals with CMS evictions

Robert plans to visit Condor_G team mid to end june

---++ Site Coordination, Integration (Suchandra)
   * VTB cache all set and testing is starting
   * Will be doing initial testing this week
   * VTB/ITB cycle currently in progress
   * Site survey is still being planned by Marco.

---++ Engagement (Mats)
   * Mats on vacation this week

---++ Metrics (Brian)
   * Nothing to report

---++ Virtual Organizations Group (Abhishek)


   * General
      * With OSG Communications, have started to solicit lists of publications from at-large VOs.
      * Positive feedback from D0.

   * D0 
      * D0 MC production is at an average consumption of 100,000 wall hours per day, at 70% job, 90+ % wall, 75% CPU:wall efficiencies. D0 Event production is at 9.2 million events per week. (Peak rate of 12.3 million events per week, in first week of May &#39;09). 
      * High failures were noted at UNL site; now resolved. 
      * Related to OSG, partial bugfix for Globus LSF Manager is now in place. Deployed at OU. URLs - https://oim.grid.iu.edu/gocticket/viewer?id=6489 and http://bugzilla.globus.org/bugzilla/show_bug.cgi?id=6688

   * CDF
      * Have resumed participation in weekly VO forum.
      * Long-term view is to possibly scale up opportunistic usage, to expand production beyond FermiGrid. 
      
   * OSG-TeraGrid Gateway
      * After first successful small-scale submission using Condor-G through the gateway, D0 management is considering whether to use the gateway at a higher scale.
      * No recent movement.

   * ALICE  
      * Taskforce active. Scalability and stress-testing exercise of VO-Box is ongoing on OSG site at NERSC/LBL.
      * Troubleshooting ongoing between NERSC and Condor.  
      
   * !CompBioGrid
      * Site deployment ongoing.
      * Blog - https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/VirtualOrganizations/CompBioGrid_OSG
      
   * VORS deprecation
      * Known dependents on VORS are !FermiGrid/Fermilab-VO, NYSGrid, SBGrid, STAR. 
      * VO Group and GOC have agreed that a short document listing the new functionality/API, is a prerequisite to collect more informed feedback/agreements from the affected VOs. 
      * nanoHUB has conveyed a need to have an XML parser to !MyOSG/OIM metadata.

   * GPN
      * David has expressed interest in attending the weekly VO forums.

   * GROW
      * Bringing up a local site. Currently deploying a small local SE, worth 3TB.
      * Will convey if help from !Trash/Trash/CampusGrids is needed.

   * nanoHUB
      * Taskforce face-to-face meeting at FNAL on May 7 &#39;09.
      * Summary Report - https://twiki.grid.iu.edu/twiki/pub/Trash/Trash/Trash/Trash/VirtualOrganizations/JointTaskForces/nanoHUB-OSG_May2009-F2F-Summary.pdf
      * Wider-scope items within OSG in need of discussion. Suggestions related to positive enhancements to a few OSG procedures and logistics: 
         * More accurate site advertisement of only the supported VOs.
         * GIP design to advertise memory and arch of a heterogeneous site.
         * Measures to publish site policies on preemption, suspension, eviction.
         * Gratia Accounting system to possibly capture application level exit codes.
         * Gratia Accounting system to possibly capture different job streams or categories
         * OSG Metrics system, as an extension to the Gratia feature requests, to possibly generate different graphical views of different exit codes, and for different applications.
      * Pending out-of-taskforce-scope OSG enhancements
         * Multi-gatekeeper handling: Difficulty to evaluate status of a site with multiple gatekeepers on the same cluster. There is a need to advertise gk1 OR gk1 model so that a VO does not erroneously evaluate such a site using gk1 AND gk2 as the usual logic.
         * Interruption in service and impact of change in CA certificate distribution mechanism: CAs in use by nanoHUB are not regular DOECA, thus nanoHUB was affected by the procedural change. A fraction of OSG sites seem not to have implemented the transition.
      
---++ Grid Operations Center (Rob Q.)

   * BDII Maintenances went well last Wednesday and Thursday
   * TWiki Maintenance Tomorrow [[http://osggoc.blogspot.com/2009/05/osg-twiki-maintenance-wednesday-may-20.html][Ops Feed]]
   * OIMv2 Release Coming Very Soon
   * New RSVv3 Tarball Sent to VDT
   * FNAL Trouble Ticket Update Fixed
   * GGUS Trouble Ticket Updated Fixed

---++ Security (Mine)

The operational issues:
   
   * Security drill is over. Grading will start this week and then be sent to eb. It was very successful for the tier1s. next step is to get a drill going for the tier2s and tier3s. 

   * did an audit of DOEgrids CRL downloads. examined which OSG resources are downloading from the DOEgrids service. DOEGrids CA has changed the location of its CRL so this was a good time to see how quickly this information propagates. received a log file form DOEGrids CA service. received log files from VDT and GOC CA distribution services. Comparing the three files: 

10 osg resources hits VDT CA distribution service.

No osg resource hits GOC CA service

48 resource hits the DOEGRids CRL service

still investigating since this is a bizzare case

   * GOC and VDT CA distribution services are asked to obtain a commercial service certificate. VDT decided to stop the service and let their customers get it from IGTF. We need to change the vdt code a bit. Alain and Igor S are looking into what needs to be done.

   * RA process audit is done. Gridadmins can only issue service certs to their assigned FQDNs. Doug is auditing the past requests. I also asked
him to put automated alerts when a grid admin starts issuing certs out of his domain

   * DOEgrids risk assessment is done. contingency planning is almost done. once we agree on that, we start building/implementing the
suggested plans.

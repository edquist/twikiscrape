-- Main.DanFraser - 15 Nov 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * The network problem at the Indiana POP has been fixed. Xin has restored the full Atlas publishing datasets into the BDII and no further problems have been seen. Rob has requested that CERN *not* restore the timeout settings until next week due to the upcoming holiday weekend. 
   * Since problems were discovered last week with OpenLDAP running inside a VM, a new v5 BDII (IS4) running on its own machine will be placed into production next week for testing. (Rob)

---++ Attendees:
   * Xin, Armen, Britta, Suchandra, Burt, Marco, Marcia, Rob Q., Scott T., Mine, Dan
 
---++ CMS (Burt)
   * LHC: Heavy ion running continues
   * First Z boson observation in heavy ion collisions
   * 310 khr/day, 86% success.

---++ Atlas (Armen &amp; Xin)

   * General production status
      * LHC continue to run stable heavy ion collisions. Number of bunches per beam is 121, 113 colliding in ATLAS. Collected luminosity 3150 mb-1. Already interesting physics results coming out of it. ATLAS data reprocessing campaign moving quite nicely, without major problems, will be done by the end of the week. Production at US is stable at the level of ~10k running jobs. MC Geant4 simulation is 80% done. After that MC reprocessing (digi+reco) stage. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.5M jobs, with CPU/Walltime ratio of 83%. 
      * Panda world-wide production report (real jobs): 
         * Failure rate raised on Nov 18~19, due to panda server connection problem with Oracle DB at cern. 
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was 400~500TB/day.
   * Issues
      * CERN/SAM BDIIs: back to full set of information publishing for BNL, so far so good. 

---++ LIGO (Britta, Robert E.)

---+++ Gratia Reports
   * Last week&#39;s total usage: 5 users utilized 33 sites
      * 86048 jobs total (29590 / 56458 = 34.4% success)
      * 478554.7 wall clock hours total (292500.8 / 186053.9 = 61.1% success)
   * This  week&#39;s total usage: 5 users utilized 32 sites
      * 93112 jobs total (33040 / 60072 = 35.5% success)
      * 521261.9 wall clock hours total (330553.0 / 190708.9 = 63.4% success)

---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,238,130.94941, Last week: 1,307,391.95052
   * E@H rank based on RAC: 1 (+1)
   * E@H rank based on accumulated credits: 3

---+++ LIGO / INSPIRAL

   * FILE TRANSFERS
      * Successfully transferred data into 6 OSG SEs
         * Nebraska, FF, CIT_CMS_T2, OUHEP_OSG, GridUNESP_CENTRAL, NWICG_NotreDame, 
      * Testing SBGrid-Harvard-East 
 
   * TESTS WITH SRM SETUP
      * CIT_CMS_T2: currently running on 3 day data set
      * Nebraska: 3 day data set run finished successfully
      * OUHEP_OSG: 32 bit cluster

   * GLIDEINS
      * more testing at FF in progress: 10 simultaneous work-flows are submitted
      * Mats made some configuration changes that should improve performance

---++ Grid Operations Center (Rob Q.)
---+++ Operations Last Week 
   * [[http://tinyurl.com/27fknc6][GOC Services Availability/Reliability]]
   * [[http://tinyurl.com/35zl55c][Security Services Availability/Reliability]]
   * Network Issues on BDII Resolved Tuesday Midday
      * BNL Started Reporting Full Dataset Midday Thursday &lt;br&gt;
     &lt;img src=&quot;https://twiki.grid.iu.edu/twiki/pub/Operations/Minutes2010November22/Picture_1.png&quot; alt=&quot;Picture_1.png&quot; width=&#39;510&#39; height=&#39;282&#39; /&gt;    &lt;br&gt;
     &lt;img src=&quot;https://twiki.grid.iu.edu/twiki/pub/Operations/Minutes2010November22/Picture_2.png&quot; alt=&quot;Picture_2.png&quot; width=&#39;509&#39; height=&#39;317&#39; /&gt;    
   * FNAL Power Outage
      * [[http://osggoc.blogspot.com/2010/11/gratia-and-ress-planned-outage-thursday.html][Scheduled Outage Notification]]
         * Affected USCMS/Fermigrid/Gratia/ReSS Services
      * [[http://osggoc.blogspot.com/2010/11/gratia-outage-18-november-goc-ticket.html][Unscheduled Outage]]
   * [[http://osggoc.blogspot.com/2010/11/goc-service-upgrade-tuesday-november.html][ITB Release]]
      * Critical glibc update
      * Logic for Downtime in OIM Changing 
      * VDT Now Testing TX on Production Queue
      * Cert Request Form Updated to Dynamically Load new VOs
   *  Scheduled power outage took out !ReSS, Gratia, OSG VOMS/VOMRS, mailing lists, and docdb  11/18/10 between 05:30 and 07:30 CST
   *  Unscheduled power outage took out all of the same from 15:00 CST 11/18 to 0700 CST 11/19
   *  Redirect on gratia-osg-prod-reports.opensciencegrid.org/gratia-reporting wasn&#39;t working Fri. 11/19, fixed Sat 11/20

---+++ Operations This Week
   * Short Holiday Week
      * Off Hours Procedures will be followed on Thursday and Friday
   * [[http://osggoc.blogspot.com/2010/11/goc-service-upgrade-tuesday-november.html][Production Release]]
      * !TWiki x509 Authentication 
   * Continue to watch BDII
      * We&#39;ve asked them to keep the longer timeout in affect through the Holiday weekend 

---++ Engage (Mats, John)


---++ Integration (Suchandra)
   * Testing ITB 1.1.28 (OSG 1.2.16) release
      * [[http://vdt.cs.wisc.edu/releases/2.0.0/release-p23.html][Changes in update]]
      * Major change to CA certs script in order to support new format
      * Release next week

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.15
      *       95 (3) OSG 1.2.X resources (      17 are 1.2.15)
      *        4 (0) OSG 1.0.X resources (       0 are 1.0.6)
      *        4 (0) OSG 1.0.0 resources
      *        1 (0) OSG 0.8.0 resources


---++ Virtual Organizations Group (Marcia)

---+++ CDF 

   * still some pbs crashes at KISTI
   * still working on issue of slow performance when copying files back. Gabriele has been working with Rick St. Denis to get gridftp running; worked well. KISTI suggested using iperf, which was much slower. 

---+++ D0
   *  [[https://ticket.grid.iu.edu/goc/viewer?id=9566][Ticket #9566]] open for Cornell: DZero authentication is failing at nys1.cac.cornell.edu

---+++ GEANT 4 
   *  Setting up to do runs on GEANT4; Hoping for release in mid-December. Used opportunistic cyles during testing.

---+++ LSSD
   * Recently finished run on OSG (Phase 2). Image simulation group John Peterson asked to do validation on images between what they do and what we do on the OSG. Essentially 6 images, each made of 3000+ chip images. Evaluation of OSG to do validation was successful.
Gabriele now in discussions re. next phrase. 

---+++ !SBGrid
   * !SBGrid software distribution portal in beta testing for members for main workflow; have run about 12 so far using  20-40K+ hours each (varies).  Good success rates at this point and infrastructure at this level is stable.
   * Continuing issue: user job starts, then instantly fails. Front end resubmits--&gt; thousands of jobs resubmitting then failing. Igor was on call to provide guidance. Peter Doherty says this doesn&#39;t happen often, but is a significant problem when it does. Igor is still waiting for actual data from Ian Stokes-Rees.
   * You can monitor the glidein factory here (for internal ops; not production and not supported.): http://glidein-1.t2.ucsd.edu:8319/glidefactory/monitor/glidein_Production_v3_1/factoryStatus.html 

---++ Security (Mine)
   * We started executing the annual security controls. Each area coordinator will get some questions or tasks to help us perform the controls. The first area was software. You will hear from us more.
   * No new vulnerability or incidents. Started collecting data from pakiti last week to determine site patch statuses. Next week we will send the results to ET to discuss about, if there are, un-patched sites.  

-- Main.DanFraser - 09 Feb 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * Fermi T1 now at Condor 7.4.1; Burt is incrementally increasing the opportunistic jobs limit beyond 5500 (previous limit was 3500). Will keep increasing as long as the system remains stable.
   * Gratia data collection will be delayed for about six hours on Thursday while the database is transferred to new hardware. (Rob)
   * Production service updates at the GOC are being frozen (exceptions considered on a case by case basis) to provide maximum stability as the LHC comes online and computing ramps up. (Rob)
   * Plans are being made to support the IGTF format changes in OpenSSL. The goal is to provide a seamless transition path for OSG services. Since security affects so many different services, an ITB cycle of testing may not be sufficient. The Production Coordinator has requested a test plan that STG, Operations, and Production can review. (Mine)

---++ Attendees:
   * Xin, Armen, Rob E., Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Dan
 
---++ CMS (Burt)
   * Computing: 72 khour/day, 86% success.
   * Storage: T1 not reporting since 2/10.
   * Facilities: power and network issues at the T1 this week
   * Running occasional release validations workflows @ T1
   * T1 now at Condor 7.4.1; opportunistic VOs can ramp up -- I&#39;m raising the running jobs limit as the sytsem remains stable
   * First CMS physics paper released for publication (dN/deta, dN/dpt)

---++ Atlas (Armen &amp; Xin)

   * General production status
      * Reprocessing of the real data (only the collision data) is going on. Today the reprocessing of the simulation data started as well. US production over the week was at the level of 8K running jobs, mixture of reprocessing and simulation.  
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.5M jobs, with CPU/Walltime ratio of 85%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 1.5M managed MC production, validation and reprocessing jobs 
         * average 208K jobs per day
         * failed 131K jobs
         * average efficiency:  jobs  - 92%,  walltime - 96%
   * Data Transfer statistics for last week
      * Transfer rate stays the same as previous weeks. BNL T1 transferred &gt;100 TB/day data last week, with peak at 175 TB/day.  
   * Issues and GOC Tickets
      * Opening more USATLAS T2 sites to D0 VO as opportunistic storage:  Mark from USATLAS and Joel from DZERO are in contact now.

---++ LIGO (Britta, Rob E.)

   * Current week&#39;s total usage: 4 users utilized 36 sites;
      * 64974 jobs total (45347 / 19627 = 69.8% success);
      * 713487.7 wall clock hours total (620271.2 / 93216.5 = 86.9% success);
   * Previous week&#39;s total usage: 5 users utilized 35 sites;
      * 87164 jobs total (64235 / 22929 = 73.7% success);
      * 1150309.1 wall clock hours total (1070486.7 / 79822.4 = 93.1% success);

   * E@H
      * production levels still very low on some major resources:
         * GridUNESP_CENTRAL (down, no ticket - resource not official in production )
         * BNL_ATLAS_1 ( all jobs pending for days , opened ticket )
         * BNL_ATLAS_2 ( all jobs pending, one running, opened ticket )
         * UFlorida-HPC ( -90% )
         * LIGO_UWM_NEMO ( -90% )
         * Purdue-RCAC ( 250 jobs pending, none running, opened ticket )
         * prairiefire ( -90% )

&lt;img align=&quot;left&quot; src=&quot;http://boincstats.com/charts/chart_uk_bo_object_new_users_1789660.gif&quot;/&gt;

&lt;img align=&quot;left&quot; src=&quot;http://t2.unl.edu/gratia/bar_graphs/facility_hours_bar_smry?vo=ligo&quot;/&gt;


---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Last week&#39;s availability metrics
      * [[http://tinyurl.com/yh3t766][GOC Services: BDII, MyOSG, RSV Collector, OSG Display]] 
      * [[http://tinyurl.com/yhbmbzl][GOC hosted Security services managed by OSG security team]]
   * *[[http://osggoc.blogspot.com/2010/02/goc-service-update-tuesday-february-9th.html][Production release Tuesday - Feb 9th 14:00 UTC]]* complete
      * MyOSG 1.15 
      * GOC Ticket 1.14
      * OIM 2.14
         * No service outages
      * RSV Collector 1.06.14 
         * The production RSV collector was down for about two hours during this upgrade; records were collected back from clients when the collector came back up. Nebraska did not come back up after the outage. Nebraska should be particularly watched in future upgrades.
   * *GOC-TX* 
      * (This concerns the project where we are developing an exchange with webservices; our production system is functioning as normal)
      * With USATLAS RT: Ongoing testing - mostly working
      * With FNAL Remedy: No update, GOC waiting on access to FNAL system
      * Ticket Exchange 1.1 Patched
   * *GOC Debian repo*: Anand/Dave Dykstra figured out additional changes required to ensure this works properly; waiting on LIGO testing results before production deployment. We will be releasing 1.11 when it is ready.
   * New CA Distribution 1.12 Today
   * TWiki Google Analytics Added 

---+++ Operations This Week
   * *Gratia Accounting service moving to new hardware on Thursday, Feb 18 2010 09:00 Central*
      * Data collection and DB updates will be suspended to allow a copy of the live DB. Reports will still be available at this time, but obviously will not include data not received. This procedure is expected to take 4-6 hours, during which time,  data will be backlogged by probes and sent when service resumes.
      * On Thursday, Feb 25 2010 (Time??), the migration will occur. Since this will involve a migration of service IPs to the already set-up and operating new systems, the outage for each service is expected to be under 5 minutes barring unforeseen circumstances. Any probes attempting to contact during this time will backlog the data and re-send as before upon resumption of service. In addition, Gratia proposes to retire the unused &quot;GRATIA-ITB-OSG-PERM&quot; service at this time. 
      * The upgrade is expected to result in a significant increase in performance. 
      * The nightly reports should have already gone out before the upgrades, people should only notice if checking their probes that the records will not be sent during the upgrade. 
      * Chris is requesting a degraded service/partial outage option from OIM downtime schedule.
   * *GOC-TX*
      * With USATLAS RT: Further ongoing testing .. No ETA yet on move to new system because of production upgrade freeze
      * With FNAL Remedy: Continue new web service based GOC-FNAL ticket exchange work once GOC gets FNAL system access
   * *GOC Debian repo*: If LIGO testing succeeds, then deploy CA-dist 1.11 to production 
   * *Internal System Improvements*: GOC will continue working on improvement monitoring its own services, etc. through this month

---+++ WLCG Items
   * Retest of the GGUS ALARM Mechanisms Monday morning
   * Some corrupted messages for RSV records to WLCG. ActiveMQ bug opened. 

---++ Engage (Mats, John, Chris)

12 users utilized 37 sites;

50352 jobs total (39070 / 11282 = 77.6% success);

184484.5 wall clock hours total (166587.8 / 17896.7 = 90.3% success);

We will not be able to make the call today due to a conflicting workshop.


---++ Integration (Suchandra)
   * Finishing up ITB testing for OSG 1.2.7
      * CMS testing looks good
      * lcg-utils testing by Marco succeeded
      * Waiting for ATLAS signoff
   * ITB Robot
      * Working with Jose and Maxim to complete job pilots and get statistics
      * Adding gLexec support for FNAL 
   * Documentation
      * Updating documentation based on documentation workshop 

---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.6
      *       70 OSG 1.2.X resources (      13 are 1.2.6)
      *        9 OSG 1.0.X resources (       2 are 1.0.5)
      *       13 OSG 1.0.0 resources
      *        2 OSG 0.8.0 resources
         * OU_OCHEP_SWT2, tier2-01.ochep.ou.edu , Contact: Horst Severini
         * UIC_PHYSICS mstr1.cluster.phy.uic.edu , Contact: John Wolosuk

---++ Virtual Organizations Group (Abhishek)

   * D0 
      * MC production affected by last week&#39;s power outage at FCC/FNAL. 
      * Average 6.5 M Events/week; 35,000 wall-hours/day at 55% efficiency. 
      * Didn&#39;t recover submit-side infrastructure for 1-2 days. 
      * Recovery hopefully complete now. 

   * CDF and Fermi-VO 
      * Affected by FCC/FNAL power outage. No known urgent issues. Recovery hopefully complete now.
   
   * SBGrid
      * Achieved milestone of ~3000 simultaneous jobs. Success rate 30-50%. 
      * Submission rate down again now.
         * Load-testing Squid.
         * Evaluating Panda.
            * Possible advantages: pilots + job prioritization + real-time monitoring + failure analysis.
 
   * !GlueX 
      * In discussion with Richard Jones on status of UConn-OSG.
      * Local CE and SE are now stable and running a steady stream of local jobs.
      * RSV probes are all green.
      * GIP status shows all ok.
      * Some pending issues; to be followed up this week.
         * $OSG_LOCATION/gratia/var/logs directory filled up.
         * Need more jobs from other VOs.
         * Current GIP status on !MyOSG is unclear.
         * Weekly Gratia report for UConn-OSG site incorrectly shows 0 wall hours consumed.

   * !GridUNESP resources operating normally.
   
   * CHARMM molecular dynamics (National Heart, Lung, Blood Institute)
      * Working with Panda team (Maxim P, Jose C) and Tim Miller at NHLBI/NIH to ramp-up production. 
      * Aim is to restart activity in preparation for AHM.
      * Restarting submission; increasing number of targeted sites.
   

---++ Security (Mine)

   * Debian CA package issue is closed.
   * Released a new CA package , IGTF release 1.33 OSG version 1.12. No risks and changes regarding openssl updates. Completely old content layout No changes/ 
   * ITB testing for new  CA package layout. Testing our process that produces the tarball, the rpms and .deb packages. Then we will test on ITB machines. Instead of including GOC, we will ask VDT to set up a test CA repository to test end-end. 
   * T3 documentation is almost over. We will go to the meeting tomorrow. 
   * Pakiti is in test mode. T3 admins like it. We need to ask atlas t3 coordinator to test it. We have to think through how to provide this for T3s.
   * fetch-crl fixes are complete. Now testing to release soon. these are in response to UCSD requests. 
   * glexec tests are going on  by Igor. No progress at all. Sites are not installing glexec. Wisconsin has problems with getting AFS to work with glexec. 
 

-- Main.DanFraser - 07 Jun 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * We continue to operate with minimal changes to allow teams to get their data for iCHEP. (Dan)
   * As noted in last weeks report, there was an issue with a T3 site filling its storage system and effectively creating a DOS attack on the BNL T1. The problem was correctly handled as the T1 simply blacklisted the site. There are still a few questions remaining however and Armen has kindly agreed to look into this problem and report at the next Production meeting.
   * There was a slowdown of one of the ReSS servers over the weekend between 6/11 and 6/14, ReSS service remained available due to high availability features. An auto-correction script was updated which should prevent slowdowns like this in future. (Rob)
   * Gratia machines will delay kernel update pending new Gratia release forthcoming in a couple of weeks, which will partially address issues of the reporting database being behind the collector database during housekeeping. (Rob)
   * The &quot;production&quot; version of Pakiti has been released. The security team will begin work toward deploying for CMS T3 sites. (Mine)

---++ Attendees:
   * Mats, Xin, Armen, Britta, Rob E., Tony, Marco, Abhishek, Rob Q., Dan
 
---++ CMS (Burt)

   * Job statistics for last week
      * ~76 khours/day
      * 29504 jobs/day
      * 97% success
   * Transfer statisics for last week
      * ~504TB/day

---++ Atlas (Armen &amp; Xin)

   * General production status
      * LHC is continuing the commissioning to have stable high intensity (1011 proton/bunch) bunches. ATLAS production was relatively stable over the week at the level of 7K running jobs. At the moment production level is low. Waiting for new samples. Ongoing validation of the new simulation. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2M jobs, with CPU/Walltime ratio of 84%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 506k managed MC production, validation and reprocessing jobs 
         * average 72K jobs per day
         * failed 35K jobs
         * average efficiency:  jobs  - 94%,  walltime - 94%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was 300TB/day.  
   * Issues
      * Opportunistic SE usage for D0 : site should be ready, waiting for test jobs from D0. 


---++ LIGO (Britta, Robert E.)

---+++ Gratia Reports
   * This week&#39;s total usage: 3 users utilized 38 sites
      * 78292 jobs total (33283 / 45009 = 42.5% success)
      * 631193.1 wall clock hours total (533408.5 / 97784.6 = 84.5% success)
   * Last week&#39;s total usage: 4 users utilized 36 sites
      *  58214 jobs total (28627 / 29587 = 49.2% success)
      * 324529.4 wall clock hours total (275016.6 / 49512.8 = 84.7% success)
---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,323,692.63918,  Last week: 949,356.17174
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0)
---+++LIGO/INSPIRAL
   * Three bug reports submitted, waiting for approval 

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Last Week&#39;s..
      * [[http://tinyurl.com/2fxgbg8][Reliability/Availability of GOC Services]]
      * [[http://tinyurl.com/2chulrn][Reliability/Availability of Security Services]]
   * *Production Update Tuesday, 8th -- Complete* 
      * Routine update -- [[http://osggoc.blogspot.com/2010/06/revised-goc-service-update-tuesday-june.html][detailed notes]]
         * Per management request, CEMon Collector upgrade was postponed (indefinitely)
      * Resource Provider-only VOs like NERSC can now register
      * Implemented [[https://ticket.grid.iu.edu/goc/navigator2][/navigator2]] - this will replace /navigator and /mytickets in the coming weeks. 
      * Reorganized ticket viewer for non-editors and added description update field along with current CC editor
   * [[http://osggoc.blogspot.com/2010/06/osg-1210-release-announcement.html][ *OSG 1.2.10 released* ]]
   * [[http://osggoc.blogspot.com/2010/06/power-outage-affecting-vdt-services-on.html][ *VDT Services impacted by planned power outage on June 12th* ]]
       *vdt-version hangs indefinitely during the power outage, S. Timm opened ticket 8735.



---+++ Operations This Week
   * [[http://myosg.grid.iu.edu/map?all_sites=on&amp;active=on&amp;active_value=1&amp;disable_value=1&amp;gridtype=on&amp;gridtype_1=on][&lt;strong&gt;Operations RSV Status Map&lt;/strong&gt;]]
   * *ITB Update Tuesday, 15th* 
      * Watch for notification tomorrow
   * *Ticket Exchange*
      * GOC-TX server outage on Saturday - GOC investigating -- *no GGUS tickets were impacted*
      * GGUS - no updates, working properly
      * BNL - no updates, working properly - some more discussion between GOC Infrastructure and Jason
      * FNAL - Further tests using FNAL provided script, still no success to report yet
   * Slowdown of half of !ReSS service over the weekend between 6/11 and 6/14,  !ReSS service remained available due
       to high availability features.  We have updated auto-correction script which should prevent slowdowns like this in future.
   * !ReSS machines will have kernel update on 6/17/10, HA features will keep !ReSS up.
   * Gratia machines will delay kernel update pending new Gratia release forthcoming in a couple of weeks, which will partially
       address issues of the reporting database being behind the collector database during housekeeping.
      * Fermilab anticipates an 5-10 minute outage, but everything should recover itself, we will send an announcement


---++ Engage (Mats, John)

10 users utilized 30 sites

4118 jobs total (3601 / 517 = 87.4% success)

14863.2 wall clock hours total (10957.4 / 3905.8 = 73.7% success)

Slow week. Nothing to report.



---++ Integration (Suchandra)


---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
Each line has the current number and variation from last week in parenthesis.
You can find a table with current OSG and VDT versions at http://www.mwt2.org/~marco/myosgldr.php
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.10
      *       79 (-1) OSG 1.2.X resources (       4 are 1.2.10)
      *        5 (0) OSG 1.0.X resources (       0 are 1.0.6)
      *        7 (0) OSG 1.0.0 resources
      *        1 (0) OSG 0.8.0 resources


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)

---+++ D0
   * Monte carlo production ongoing at good rate; multiple D0-internal infrastructure problems last week; now resolved. 
   * 10 M Evts/week. 105,000 hours/day at 80% efficiency.
   * Site related issues:
      * UTA/ATLAS SE now added. 
      * Low efficiencies at Purdue RCAC and Florida.

---+++ CDF
   * Restarting work on the production expansion plan to use more OSG sites opportunistically. 
   * CDF needs SL5, along with additional packages. Already deployed at Fermilab resources. 
   * %RED%Important stakeholder request:%ENDCOLOR% Looking for an arrangement with large SL5 sites (ATLAS, CMS) to possibly verify/install these packages.
   * Initial package list: 
&lt;verbatim&gt;
         tcl.i386 tcl-devel.x86_64 tcl-devel.i386 
         compat-libf2c.x86_64 compat-libf2c.i386 
         perl-Crypt-SSLeay lapack.i386 lapack.x86_64 
         libXp-devel.i386 libXpm-devel.i386 libXi-devel.i386 
         libXp-devel.x86_64 libXpm-devel.x86_64 libXi-devel.x86_64 perl-DBI 
         valgrind libstdc++-devel 
&lt;/verbatim&gt;

---+++ OSG-VO/CHARMM Group
   * Work ongoing is to expand CHARMM production from 3 sites to 23 sites.
   * Using !PanDA with active help from BNL team. 
   * In beginning of May, production was halted due to !PanDA server problems. 
      * Certificate issues brought down the central autopilot system. 
      * BNL is tracking the problem at CERN and resolution is expected soon. &lt;u&gt;Note&lt;/u&gt;: BNL !PanDA server is physically located at CERN.

---+++ GLUE-X
   * D0 trying to start using CE and SE at the GLUE-X site UConn-OSG. 
   * A few problems under investigation: 
      * Firewall/NAT likely blocking jobs when large groups of jobs are submitted together by a VO. Ticket: https://ticket.grid.iu.edu/goc/viewer?id=8579
      * D0 jobs blocked; SSL negotiation issue during socket setup. Problem likely rooted in the proxy being double-limited due to NFS-lite configuration. Ticket: https://ticket.grid.iu.edu/goc/viewer?id=8710

---+++ !IceCube
   * Deployment of VO glideinWMS front-end is complete. 

---+++ SBGrid
   * Continuing work with !GlideinWMS. Running 2000-2500 jobs sustained over hours. Looking to go a little higher; will continue to run and see how scalability continues.


---++ Security (Mine)

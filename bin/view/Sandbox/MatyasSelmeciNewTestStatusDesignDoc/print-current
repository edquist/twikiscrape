<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> MatyasSelmeciNewTestStatusDesignDoc &lt; Sandbox &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Sandbox/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Sandbox/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Sandbox/MatyasSelmeciNewTestStatusDesignDoc?_T=16 Feb 2017" type="application/x-wiki" title="edit MatyasSelmeciNewTestStatusDesignDoc" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><meta name="robots" content="noindex,nofollow"> 
<base href="https://twiki.opensciencegrid.org/bin/view/Sandbox/MatyasSelmeciNewTestStatusDesignDoc"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#DDDDDD;
	}
	.patternBookView {
		border-color:#DDDDDD;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">



</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <font color="#ff0000">This now lives at <a href="/bin/view/SoftwareTeam/NewTestStatusDesignDoc" class="twikiLink">NewTestStatusDesignDoc</a></font>
<p />
<h1><a name="New_OSG_Test_Test_Statuses"></a> New OSG-Test Test Statuses </h1>
<p />
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#New_OSG_Test_Test_Statuses"> New OSG-Test Test Statuses</a> <ul>
<li> <a href="?cover=print#Overview"> Overview</a>
</li> <li> <a href="?cover=print#New_Interfaces"> New Interfaces</a> <ul>
<li> <a href="?cover=print#API"> API</a> <ul>
<li> <a href="?cover=print#Assertions"> Assertions</a>
</li> <li> <a href="?cover=print#Helpers"> Helpers</a>
</li></ul> 
</li> <li> <a href="?cover=print#Uses_of_OkSkip_in_Test_Functions"> Uses of OkSkip in Test Functions</a>
</li> <li> <a href="?cover=print#Uses_of_BadSkip_in_Test_Function"> Uses of BadSkip in Test Functions</a>
</li> <li> <a href="?cover=print#Long_Example"> Long Example</a>
</li> <li> <a href="?cover=print#Example_Output"> Example Output</a>
</li> <li> <a href="?cover=print#Example_Summaries"> Example Summaries</a>
</li></ul> 
</li> <li> <a href="?cover=print#Architecture_of_Proposed_Solutio"> Architecture of Proposed Solution</a> <ul>
<li> <a href="?cover=print#unittest_Subclasses"> unittest Subclasses</a>
</li></ul> 
</li></ul> 
</li></ul> 
</div>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Overview"></a> Overview </span></h2>
<p />
We need to be able to separate the following conditions in order to gain insight into the test results:
<p /> <ol>
<li> A test is skipped due to an acceptable condition, e.g. a package that the test depends on is not installed.<br />       This state needs to be distinguished from a successful test, but should not raise any red flags.
</li> <li> A test is skipped due to a previous test or action failing, e.g. a required service could not be started.<br />       In other words, a cascading failure. This needs to be distinguished from the original failure to avoid distracting the developer, but should still raise a red flag.
</li></ol> 
<p />
The existing unittest framework will be updated with two new statuses, tentatively named <code>OkSkip</code> and <code>BadSkip</code>.
This must not cause any backward-incompatible changes, i.e. all existing tests must work without modification.
The tests should be reworked one at a time to make use of the new feature.
<p />
Other requirements are: it must work on both Python 2.4 and 2.6, and should not bring in additional dependencies.
<p />
Also note that we will have to update the nightly test wrapper to handle the new output.
To make full use of the new features, the test aggregator must also be updated.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="New_Interfaces"></a> New Interfaces </span></h2>
<p />
<h3><a name="API"></a> API </h3>
<p />
<h4><a name="Assertions"></a> Assertions </h4>
<p />
Assertions are all instance methods in <code>OSGTestCase</code>, which is the class that all tests should be derived from.
(Currently, all tests are derived from <code>unittest.TestCase</code>).
<p /> <dl>
<dt> <code>skip_ok(msg=None)</code>  </dt><dd> An unconditional ok skip, with optional message.
</dd> <dt> <code>skip_bad(msg=None)</code>  </dt><dd> An unconditional bad skip, with optional message.
</dd> <dt> <code>skip_ok_if(expr, msg=None)</code>  </dt><dd> Ok skip if <code>expr</code> is True, with optional message.
</dd> <dt> <code>skip_bad_if(expr, msg=None)</code>  </dt><dd> Bad skip if <code>expr</code> is True, with optional message.
</dd> <dt> <code>skip_ok_unless(expr, msg=None)</code>  </dt><dd> Ok skip if <code>expr</code> is False, with optional message.
</dd> <dt> <code>skip_bad_unless(expr, msg=None)</code>  </dt><dd> Bad skip if <code>expr</code> is False, with optional message.
</dd></dl> 
<p />
<h4><a name="Helpers"></a> Helpers </h4>
<p />
These are helper functions for the most common use cases of skips.
They are defined in <code>osgtest.library.core</code>.
<p /> <dl>
<dt> <code>skip_ok_unless_installed(*rpms, msg=None)</code>  </dt><dd> Verifies that all RPMs in <code>rpms</code> are installed.<br />       Raises <code>OkSkip</code> with the optional message if any of them are missing.
</dd></dl> 
<p />
<h3><a name="Uses_of_OkSkip_in_Test_Functions"></a> Uses of OkSkip in Test Functions </h3>
<p />
Changing tests to use <code>OkSkip</code> is fairly straightforward.
The following are common replacement patterns where the use of <code>OkSkip</code> would be appropriate:
<p />
<h4><a name="Old_Code"></a>  Old Code </h4>
Case 1:
<pre class="file">
if not core.rpm_is_installed('foo'):
    core.skip("rpm foo not installed")
    return
</pre>
Case 2:
<pre class="file">
if core.missing_rpm('rsv', 'rsv-consumers', 'rsv-core', 'rsv-metrics'):
    return
</pre>
<p />
<h4><a name="New_Code"></a>  New Code </h4>
Case 1:
<pre class="file">
self.skip_ok_unless(core.rpm_is_installed('foo'), "rpm foo not installed")
</pre>
Case 2:
<pre class="file">
core.skip_ok_unless_installed('rsv', 'rsv-consumers', 'rsv-core', 'rsv-metrics',
                           msg='RSV components are missing')
</pre>
<p />
<h3><a name="Uses_of_BadSkip_in_Test_Function"></a> Uses of BadSkip in Test Functions </h3>
One case where a <code>BadSkip</code> would be appropriate is a service that is expected to be running but isn't.
Generally, tests that depend on that service either: <code>core.skip()</code> and <code>return</code> if the service is not reported as running;
or plow through and try to run their tests anyway and just get failures.
<p />
A test of the service now also has to check if the service is <em>supposed</em> to be running, in addition to whether or not it <em>is</em> running.
We can assume that if the service's RPMs are installed then it is supposed to be running, so we can use <code>core.skip_ok_unless_installed()</code>
<p />
Some example changes follow.
<p />
<h4><a name="Old_Code_AN1"></a>  Old Code </h4>
<pre class="file">
def test_03_voms_proxy_info(self):
    if core.missing_rpm('voms-client'):
        return
    if not core.state['voms.got-proxy']:
        core.skip('no proxy')
</pre>
<p />
<h4><a name="New_Code_AN1"></a>  New Code </h4>
<pre class="file">
def test_03_voms_proxy_info(self):
    core.skip_ok_unless_installed('voms-client')
    self.skip_bad_unless(core.state['voms.got-proxy'], 'no proxy')
</pre>
<p />
<p />
<h3><a name="Long_Example"></a> Long Example </h3>
As an example, here is how the tests for HTCondor would be rewritten, including setup, testing, and shutdown.
<p />
<h4><a name="Old_Code_AN2"></a>  Old Code </h4>
<p />
<h5><a name="test_10_condor_py"></a>  test_10_condor.py </h5>
<pre class="file">
class TestStartCondor(unittest.TestCase):

    def test_01_start_condor(self):
        core.config['condor.lockfile'] = '/var/lock/subsys/condor_master'
        core.state['condor.started-service'] = False
        core.state['condor.running-service'] = False

        if core.missing_rpm('condor'):
            return
        if os.path.exists(core.config['condor.lockfile']):
            core.state['condor.running-service'] = True
            core.skip('apparently running')
            return

        command = ('service', 'condor', 'start')
        stdout, _, fail = core.check_system(command, 'Start Condor')
        self.assert_(stdout.find('error') == -1, fail)
        self.assert_(os.path.exists(core.config['condor.lockfile']),
                     'Condor run lock file missing')
        core.state['condor.started-service'] = True
        core.state['condor.running-service'] = True
</pre>
<p />
<h5><a name="test_41_jobs_py"></a>  test_41_jobs.py </h5>
<pre class="file">
class TestGlobusJobRun(unittest.TestCase):
    # ...

    def test_02_condor_job(self):
        if core.missing_rpm('globus-gram-job-manager-condor',
                            'globus-gram-client-tools', 'globus-proxy-utils'):
            return

        command = ('globus-job-run', self.contact_string('condor'), '/bin/echo', 'hello')
        stdout = core.check_system(command, 'globus-job-run on Condor job', user=True)[0]
        self.assertEqual(stdout, 'hello\n',
                         'Incorrect output from globus-job-run on Condor job')
</pre>
<p />
<h5><a name="test_89_condor_py"></a>  test_89_condor.py </h5>
<pre class="file">
class TestStopCondor(unittest.TestCase):

    def test_01_stop_condor(self):
        if core.missing_rpm('condor'):
            return
        if core.state['condor.started-service'] == False:
            core.skip('did not start server')
            return

        command = ('service', 'condor', 'stop')
        stdout, _, fail = core.check_system(command, 'Stop Condor')
        self.assert_(stdout.find('error') == -1, fail)
        self.assert_(not os.path.exists(core.config['condor.lockfile']),
                     'Condor run lock file still present')

        core.state['condor.running-service'] = False
</pre>
<p />
<h4><a name="New_Code_AN2"></a>  New Code </h4>
Changes are highlighted in bold.
<p />
<h5><a name="test_10_condor_py_AN1"></a>  test_10_condor.py </h5>
<pre class="file">
class TestStartCondor(<b>OSGTestCase</b>):

    def test_01_start_condor(self):
        core.config['condor.lockfile'] = '/var/lock/subsys/condor_master'
        core.state['condor.started-service'] = False
        core.state['condor.running-service'] = False

        <b>core.skip_ok_unless_installed('condor')</b>
        if os.path.exists(core.config['condor.lockfile']):
            core.state['condor.running-service'] = True
            <b>self.skip_ok("apparently running")</b>

        command = ('service', 'condor', 'start')
        stdout, _, fail = core.check_system(command, 'Start <b>HT</b>Condor')
        self.assert_(stdout.find('error') == -1, fail)
        self.assert_(os.path.exists(core.config['condor.lockfile']),
                     '<b>HT</b>Condor run lock file missing')
        core.state['condor.started-service'] = True
        core.state['condor.running-service'] = True
</pre>
<p />
Then, a test of the service:
<p />
<h5><a name="test_41_jobs_py_AN1"></a>  test_41_jobs.py </h5>
<pre class="file">
class TestGlobusJobRun(OSGTestCase):
    # ...

   def test_02_condor_job(self):
       <b>core.skip_ok_unless_installed('globus-gram-job-manager-condor', 'globus-gram-client-tools', 'globus-proxy-utils',
                                     msg='Missing Globus components')</b>
       <b>core.skip_ok_unless_installed('condor', msg='Missing HTCondor jobmanager')</b>
   
       <b>self.skip_bad_if(core.state['condor.running-service'], "HTCondor not running")</b>
       command = ('globus-job-run', self.contact_string('condor'), '/bin/echo', 'hello')
       stdout = core.check_system(command, 'globus-job-run on <b>HT</b>Condor job', user=True)[0]
       self.assertEqual(stdout, 'hello\n',
                        'Incorrect output from globus-job-run on <b>HT</b>Condor job')
</pre>
<p />
<h5><a name="test_89_condor_py_AN1"></a>  test_89_condor.py </h5>
<pre class="file">
class TestStopCondor(OSGTestCase):

    def test_01_stop_condor(self):
        <b>core.skip_ok_unless_installed('condor')</b>
        <b>self.skip_ok_unless(core.state['condor.started-service'], 'did not start server')</b>

        command = ('service', 'condor', 'stop')
        stdout, _, fail = core.check_system(command, 'Stop Condor')
        self.assert_(stdout.find('error') == -1, fail)
        self.assert_(not os.path.exists(core.config['condor.lockfile']),
                     'Condor run lock file still present')

        core.state['condor.running-service'] = False
</pre>
<p />
<h3><a name="Example_Output"></a> Example Output </h3>
<p />
Here is what output should look like in a test run with the new features.
<p />
<h4><a name="Current_Output"></a>  Current Output </h4>
For comparison, this is what a failure looks like currently:
<pre class="file">
test_01_fetch_crl (osgtest.tests.test_06_fetch_crl.TestFetchCrl) ... FAIL
...
======================================================================
FAIL: test_01_fetch_crl (osgtest.tests.test_06_fetch_crl.TestFetchCrl)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/lib/python2.6/site-packages/osgtest/tests/test_06_fetch_crl.py", line 11, in test_01_fetch_crl
    stdout, _, fail = core.check_system(command, 'Start fetch-crl')
  File "/usr/lib/python2.6/site-packages/osgtest/library/core.py", line 184, in check_system
    assert status == exit, fail
AssertionError: Start fetch-crl
EXIT STATUS: 1
STANDARD OUTPUT:
ERROR CRL verification failed for JUnet-CA/0 (JUnet-CA)
VERBOSE(0) JUnet-CA/0: CRL has nextUpdate time in the past
STANDARD ERROR: [none]
</pre>
<p />
<h4><a name="New_output"></a>  New output </h4>
<p />
A multi-line error message complete with stack backtrace is overkill for a skip.
Instead, skips will be grouped together (i.e. all BadSkips, then all OkSkips below), one line per item.
<p />
<pre class="file">
test_01_start_condor (osgtest.tests.test_10_condor.TestStartCondor) ... okskip
...
test_02_condor_job (osgtest.tests.test_41_jobs.TestGlobusJobRun) ... BADSKIP
...

======================================================================
BADSKIPS:
----------------------------------------------------------------------
test_02_condor_job (osgtest.tests.test_41_jobs.TestGlobusJobRun) HTCondor not running
...

======================================================================
OKSKIPS:
----------------------------------------------------------------------
test_01_start_condor (osgtest.tests.test_10_condor.TestStartCondor) apparently running
...
</pre>
<p />
<h3><a name="Example_Summaries"></a> Example Summaries </h3>
<p />
<code>unittest</code> also prints summaries at the end with the different test counts.
Here is how they will look:
<p />
<h4><a name="Old_Output"></a>  Old Output </h4>
Case 1: some failures
<pre class="file">
FAILED (failures=5)
</pre>
<p />
Case 2: no failures
<pre class="file">
OK
</pre>
<p />
<h4><a name="New_Output"></a>  New Output </h4>
Case 1: some failures and skips
<pre class="file">
FAILED (failures=5, badSkips=1, okSkips=1)
</pre>
Case 2a: no failures, but some skips
<pre class="file">
OK (okSkips=1)
</pre>
Case 2b: no failures, no skips
<pre class="file">
OK (PERFECT)
</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Architecture_of_Proposed_Solutio"></a> Architecture of Proposed Solution </span></h2>
<p />
Pretty much all of the classes in <code>unittest</code> need to be subclassed.
In order to maintain backward compatibility, the derived classes should be able to work with the originals where possible.
<p />
A test failure in <code>unittest</code> is represented by an instance of the built-in Python exception <code>AssertionError</code> (though under the alias <code>TestCase.failureException</code> ).
Any other exception is considered an <code>Error</code>.
Exceptions are caught in <code>TestCase.run()</code>.
<p />
Making <code>BadSkip</code> a subclass of <code>AssertionError</code> will allow the existing <code>TestCase.run()</code> method to catch it and treat it as a test failure, which is how these things are currently considered.
<p />
<code>OkSkip</code> will also be a subclass of <code>AssertionError</code>.
This is somewhat suboptimal since it is not, technically, an error.
However, there isn't another exception class we can use that <code>TestCase.run()</code> will not either interpret as a failure or an error.
Also, we still want to use an exception to immediately exit the running test function and have the status be recorded in the results.
<p />
Currently, tests use <code>osgtest.library.core.skip()</code> followed by an explicit <code>return</code> to handle a skip.
Thus, their behavior will not be changed by this new exception class.
The problem with <em>not</em> using an exception for <code>OkSkip</code> is that we will have to add some other way of notifying <code>TestCase.run()</code> that the test was not a success.
Also, if people forget to add a <code>return</code> after declaring the skip, we may have multiple skips, a skip and a failure, or a skip and a success, for the same test. 
<p />
<h4><a name="OkSkip_BadSkip_vs_skip_from_unit"></a> OkSkip / BadSkip vs. skip from unittest2 </h4>
<p />
<code>unittest2</code>, which is a backport of the new features of <code>unittest</code> found in Python 2.7, also has a feature for skipping tests.
The behavior is similar to this design.
One notable addition is that test functions can be <em>decorated</em> with skips, e.g.:<pre>
@unittest2.skipIf(mylib.__version__ < (1, 3), "library too old")
def test_foo(self):
   ...
</pre>
That won't be worth adding, since our reasons for skipping a test are fairly dynamic and depend on the current environment.
We might have multiple skip reasons in one test function.
<p />
<code>unittest2</code> also does not distinguish between an 'ok' skip and a 'bad' skip.
<p />
<h3><a name="unittest_Subclasses"></a> unittest Subclasses </h3>
<p />
The subclass of <code>unittest.TestCase</code> (<code>OSGTestCase</code>) will contain new assertion functions rasing <code>OkSkip</code> or <code>BadSkip</code>, patterned after the existing ones in unittest, e.g.: <code>skip_ok_if</code>, <code>skip_bad_unless</code>.
<p />
The <code>run()</code> method will be modified to record an <code>OkSkip</code> or a <code>BadSkip</code> upon receiving the appropriate exception;
since this involves a <code>unittest.TestResult</code> instance, <code>run()</code> will first check to see if the <code>TestResult</code> supports skips. 
<p />
If the <code>TestResult</code> instance cannot handle skips, then a <code>BadSkip</code> is reported as a <code>Failure</code> (if it happens while running a test) or an <code>Error</code> (if it happens during a test's <code>setUp()</code> method).
This is the current behavior for assertion failures in unittest.
An <code>OkSkip</code> will be logged but considered a success.
<p />
The subclass of <code>unittest.TestResult</code> (<code>OSGTestResult</code>) will contain lists of <code>OkSkips</code> and <code>BadSkips</code>, and methods to append to them.
The signature and behavior of these methods will match the existing methods <code>addError()</code> and <code>addFailure()</code>.
The <code>wasSuccessful()</code> method will be modified to consider <code>BadSkips</code> unsuccessful.
A <code>wasPerfect()</code> method will be added that will be <code>True</code> iff the tests were both successful and had no skips.
<p />
The subclass of <code>OSGTestResult</code> (<code>OSGTextTestResult</code>) will behave like its parent but will also add text output to a stream according to verbosity levels passed in via its constructor.
This mirrors how <code>unittest._TestResult</code> works in Python 2.4.
<p />
The subclass of <code>unittest.TextTestRunner</code> (<code>OSGTextTestRunner</code>) creates a test result object that is an instance of <code>OSGTextTestResult</code> instead of <code>TextTestResult</code>, and adds info on <code>BadSkips</code> and <code>OkSkips</code> to the test summaries.
<p />
Currently, <code>unittest.TestSuite</code> does not need to be modified.
To make future additions easier, I will alias it anyway as <code>OSGTestSuite</code>.</div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Sandbox<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span><a href="/bin/view/Main/TWikiGroups" class="twikiLink">TWikiGroups</a> &gt; <a href="/bin/view/Main/GridGroup" class="twikiLink">GridGroup</a> &gt; <a href="/bin/view/Main/MatyasSelmeci" class="twikiLink">MatyasSelmeci</a> &gt; <a href="/bin/view/Sandbox/MatyasSelmeciSandbox" class="twikiLink">MatyasSelmeciSandbox</a><span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>MatyasSelmeciNewTestStatusDesignDoc</span> <br />    
Topic revision: r6 - 14 Dec 2012 - 02:10:44 - <a href="/bin/view/Main/MatyasSelmeci" class="twikiLink">MatyasSelmeci</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
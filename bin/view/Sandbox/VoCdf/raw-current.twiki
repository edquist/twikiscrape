&lt;!--PDFSTART--&gt;
%TOC%

---++ CDF @ OSG

This twiki page is a CDF specific attempt of making sense of the OSG from a Virtual Organizations perspective. Use this information at your own risk!

---++ The Open Science Grid

The Open Science Grid is a distributed, semi-open attempt at unifying computational resources for scientific research into an easily accessible framework, open to all users, maintaining priority of high budget groups while allowing smaller groups to scavenge resources at valuable moments.  So far it has succeeded at none of those goals.

The OSG refers to a collection of sites, loosely connected to each other with a common interface and a common software package, the OSG software release.  Each of these sites is essentially completely independent from all other sites, and hence can have a totally different layout or environment.  There are very few constants throughout the grid, and it is the goal of many site administrators to keep those constants hidden from the users at all costs.  Get used to this.


---+++ Overview: What is a Site?

A grid site is a cluster of computers (or sometimes several clusters of computers) that the admin says is a grid site.  Usually these are all accessed via a single &quot;Gatekeeper&quot;, but this rule can be bent, or flat-out broken.  The only thing that maintains cohesion in a grid site is that they are all under the management of a single group of people. 

There are three kinds of grid sites:

   * Storage sites: These sites are for citing of Storage Elements (SEs), which function as the containers of data on the grid.  You should not need to worry about these.
   * ITBs (Trash/Trash/Integration Test Beds): These are the sites where new releases of the OSG software are tested and validated by various people.  This is where the incorporation of new components is handled.  Generally you can run on these sites, but they are small and not meant for production.
   * Compute sites: For the remainder of this document, this is what I mean by grid sites.  A compute site has a gatekeeper and numerous Computer Elements (CEs), worker nodes on which you can run jobs and generate output.  This is what the OSG was designed for.

Compute sites are the sites you will be working with the most, and are the sites that run your jobs.  Each site gatekeeper can rout your jobs to a cluster of Worker Nodes (WNs), assigning you to open batch slots as they appear.  The method by which you are assigned to a batch slot, commonly referred to as &quot;Matchmaking&quot;, is a process controlled entirely by that site.  A site admin can decide that CDF jobs are to run only if no ATLAS jobs are waiting, or will run immediately.  There is no standard formula here.  In theory, every site has a Site Policy which lists how priorities are assigned to jobs, and what the rules are on who can run.  In practice, site policies obey a strange variant of the Heisenberg Uncertainty Principle, and cannot seem to accurate, thorough, and accessible at the same time without divine intervention.  Often the particulars of how far your priority lags are conspicuously missing.

Once you can submit to a gatekeeper, you have access to a site.  This does not, as we shall see later, mean that you can run anything - just that you can get in.

---+++ Overview: What is a User?

You are a user, someone who wishes to submit jobs to the OSG.  To do this, you must be a member of a Virtual Organization (VO).  Traditionally, a VO is a group that is assembled for a specific purpose as part of a larger scientific project.  For instance, all of the large High Energy Physics projects have a VO, CDF, ATLAS, CMS and D0.  This VO consists of all users that the project is willing to let run on the Grid, and each group decides how it is going to run on the grid.  Other VOs may consist of several small projects put together, or may be organized along the same lines.

Upon joining a VO, each user gets a certificate, which is later used to authenticate that user to an OSG site.  This certificate is VO specific, meaning that one person can have multiple certifictes if they belong to multiple VOs, and even though the username may be the same, they will be mapped to different accounts.  Each user also gets, in theory, told where the people s/he should contact for assistance are located-although in practice this is less common.  Once the user has their certificate, they can, in theory submit to any site that supports their VO and run their jobs.

---+++ Overview: How does all this work?

As you have guessed, there are two sides to the OSG.  Users and sites.  A small diagram of the hierarchy can be seen below:

&lt;img src=&quot;http://osg.ivdgl.org/pub/VO/CDF/OSG1.png&quot; alt=&quot;OSG structure&quot; /&gt;

The basic idea is as follows.  Users write their own jobs, and submit them to sites under the auspices of their VO.  If there is a problem, the user can either contact the VO Support Center (if the question is locally related), or the Grid Operations Center (GOC), if the problem is at the site side.  The VO then maintains a &quot;knowledge pool&quot; of information about how to run on the OSG and what works and does not.  On the site side, sites install OSG software releases as suggested by the OSG management on their sites.  Problems are redirected toward the site admin, who is also in contact with the GOC.

Traditionally, all that the user has to worry about is getting their job to run on a compute element.  To do that you have to be able to submit a job to a site, get the job to run, and then manage the job.  The user than gets to complain to the GOC when things don&#39;t work.


---++  How to Submit

---+++ Where to Submit

The first task is to find a site to submit to.  Usually, most VOs have a &quot;home ground&quot; facility or two, someplace run by their VO that runs production jobs.  This is the easiest place to run because you can get frequent debugging information from your admins.  But if we all stayed in our &quot;home ground&quot;, the OSG would be functionally useless, so we all try to scavenge what we can.

---+++++ How to Find a Grid Site

This should, in theory, be the easiest part.  After all, each VO should be allowed access on each site.  Additionally, each VO should keep a list of sites that it receives preferential treatment at, or that often start jobs quickly.  You should be able to pick off of that list.  Fat chance.  VOs tend not to be that organized.  As a result, you will have to go out there and find a site yourself by picking and choosing.  

The most valuable resources here are the monitoring pages, of which there are several because there are several distinct monitoring systems.

   * [[http://osg-cat.grid.iu.edu/][GridCat]]: The traditional, and best developed, monitoring system, !GridCat contains information about each site, about what jobmanager it uses, what batch slots it has, what disk space is available, and who owns it.  On numberous sites the numbers are skewed.  On others, they are just plain wrong.  However, the gatekeeper information is usually correct.

   * [[http://grid02.uits.indiana.edu:8080/show?page=index.html][Trash.ReleaseDocumentationMonALISA]]: !Trash.ReleaseDocumentationMonALISA is prettier than !GridCat, which is about all it&#39;s got going for it.  You get a much better idea of how many jobs are running, and who they belong to, but there is no other information about the site included.  Since the numbers are sometimes skewed and often just plain wrong, I tend not to put too much store in !Trash.ReleaseDocumentationMonALISA.

   * [[http://osg.ccr.buffalo.edu/grid-dashboard.php][ACDC Grid Dashboard]]: The ACDC Grid Dashboard can be the most powerful of tools, if you know how to use it.  Not only does it monitor the overall status and health of the OSG, but you can also look at per VO performance, and, in theory, the sites you can run at.  However, the information is sometimes confusing, and trying to interpret whether you can run at a site or not is sometimes difficult.

From all of these you should now pick a site that is:

   * Large: The larger a site is, the more likely that small problems have been found and fixed, and that it is currently being at least partially used.  Although it might take longer to start a job, at least you know that some jobs have been started.

   * Owned by you: If your VO has a large stake in it, that usually means that your certificates will not be rejected, and that you can get prompt response to trouble.  Otherwise, running at sites where you are not a stakeholder means that often you don&#39;t know who to call.

   * Not full:  This is the hardest condition to determine, since the numbers reported up to monitoring are seldom accurate.  Experienced users have a better idea of where to run jobs, and !GridCat can be helpful at times like this.


Now that you&#39;ve picked a site, it&#39;s time to submit.


---+++++ Testing the Waters

Once you have a grid site, it&#39;s time to actually see if you can run something there.  If not, you better pick another site.

First, you need a grid certificate and globus set up.  You can test this very easily using a simple command, such as grid-proxy-info:

&lt;verbatim&gt;
bash-2.05b$ grid-proxy-info 
subject  : /DC=gov/DC=fnal/O=Fermilab/OU=Robots/CN=cdf/CN=Matthew O. Norman/USERID=mnorman/CN=proxy
issuer   : /DC=gov/DC=fnal/O=Fermilab/OU=Robots/CN=cdf/CN=Matthew O. Norman/USERID=mnorman
identity : /DC=gov/DC=fnal/O=Fermilab/OU=Robots/CN=cdf/CN=Matthew O. Norman/USERID=mnorman
type     : full legacy globus proxy
strength : 1024 bits
path     : /fbsng/test_condor/CafCondor/tickets/x509_service_proxy.is
timeleft : 163:44:42  (6.8 days)
&lt;/verbatim&gt;

We get our components from the LCG !UIPnP interface, which may or may not be something that you wish to do.  The VDT has its own components for doing this job.

Once you know that you have a valid grid proxy (one with time left on it), it&#39;s time to see if you can enter the site.

The easiest way is to use globus-job-run.  The general format for this is:

&lt;verbatim&gt;
globus-job-run gatekeeper/jobmanager command
&lt;/verbatim&gt;

The gatekeeper is the node that hosts the entrance to the grid pool in question.  The command is something that you wish to run on the command line.  And the jobmanager is what runs it.  For example, if I want to run at Fermigrid at Fermilab, I go to !GridCat and find the site information:

&lt;verbatim&gt;
     Site Name:          FNAL_FERMIGRID
      Gatekeeper Host Name:          fermigrid1.fnal.gov
      Service Type:          CS
      Grid Version:          osg 0.4.0
      Middle-ware Version:          vdt 1.3.9a
      Location (Latitude/Longitude):          IL(43.00/-78.19)
      VO:          fermilab
                  
      SITE INFO Check Date:          Wed May 10 15:52:26 UTC 2006
      Batch System:          condor
&lt;/verbatim&gt;

Now that I know the gatekeeper and the batch system, I can submit a job to the condor batch system normally:

&lt;verbatim&gt;
bash-2.05b$ globus-job-run fermigrid1.fnal.gov/jobmanager-condor /bin/hostname 
fermigrid1.fnal.gov
&lt;/verbatim&gt;

There are several different types of common jobmanagers.

   * jobmanager-condor: One of the most common, this is the batch system run at Fermilab, and seemingly preferred by CMS
   * jobmanager-pbs: Also a common jobmanager, pbs is present on most atlas sites.
   * jobmanager-lcf:
   * jobmanager-sge:
   * jobmanager-fork: This will be covered in depth in the next section


---+++++ How To Know When There&#39;s A Problem

Usually you get told that there&#39;s a problem.  For instance:

&lt;verbatim&gt;
bash-2.05b$ globus-job-run atlas.bu.edu/jobmanager-pbs /bin/hostname 
GRAM Job submission failed because authentication with the remote server failed (error code 7)
&lt;/verbatim&gt;

Unfortunately, even though you get error codes back, often they are overloaded.  Error code 7 is quite popular, but the actual cause can be somewhat indistinct and, depending on what version of globus you happen to be running, somewhat sparse on verbosity.  There is, however, another problem.

So you tried your test job, waited for a long time, found that nothing was happening, and gave up.  It takes too long.  Now you have a problem.  There are two possibilities here.

   * There are no free slots.  If nobody has a free batch slot, your one line, thirty-second job will have to wait in line for a train of seventy-two hour monstrosities to finish running.

   * There has been an error.  In some cases, the system&#39;s failure mode seems to be waiting indefinitely.  This error is virtually indistinguishable from lack of slot.

Because of this problem, and a few others that can crop up, the next step is to run jobs with jobmanager-fork.

jobmanager-fork is present on all the OSG sites that I&#39;ve seen.  However, instead of running on a CE batch system, fork runs jobs directly on the headnode, using up vital system resources.  Using this for anything other than test jobs is forbidden on pain of having graduate students come to your house and beat you to death with iron bars.  Even using it just for test bars will result in Miron giving you a nasty look if you tell him about it.  This is forbidden, verboten, prohibido, not allowed.  Nevertheless, we have to use it because it is essentially the only testing mode we have.

A fork job is simple to run:

&lt;verbatim&gt;
bash-2.05b$ globus-job-run fermigrid1.fnal.gov/jobmanager-fork /bin/hostname 
fermigrid1.fnal.gov
&lt;/verbatim&gt;

It runs faster than a standard grid job because it does not need to undergo matchmaking.  And it provides the most elementary test of site access.  If it gives you an error, then you can&#39;t get in.  If it hangs forever, you can&#39;t get in.  If it works then you have access to the site.  If it works and you still can&#39;t use the jobmanager listed, then you may have to open a GOC ticket.  This usually indicates the presence of a special jobmanager (such as jobmanager-cdf or somesuch) or the necessity of passing RSL arguments.

If you think you should be able to get into a site, but can&#39;t, contact the GOC (See the Getting Help section later in this document).  Else, ignore it and find another site.


---++ Running a Job

Once you have a site, you are ready to begin submitting.  The procedure is simple.  Run a test job.  Modify the test job until it works.  Then run production.  

Of course, like everything, it is more complicated than it appears to be.

---+++ Setting Up

In most OSG sites there should be a script to source that will have a great majority of the environment variables a system will need to run (note that this may not apply to OSG 2.1).  The script is usually:

&lt;verbatim&gt;
$OSG_GRID/setup.sh
&lt;/verbatim&gt;

and allows you to use globus as well as most other pieces of the Grid-enabled operating system.

Additionally, you need to run in the correct directory.  OSG batch systems are not uniform.  Many of them start you in the wrong directory.  CDF has crashed several OSG sites by running in the directory we start in, one that was NFS mounted from the headnode.  Avoid this mistake.

Each worker node should have a $OSG_WN_TMP directory which is local to the machine and that user jobs can run in.  We recommend cd&#39;ing to that directory immediately upon startup.  Do not remain where you are (unless this is where you start).  Information about the space available in that directory is sometimes listed in !GridCat.

More information on local working directories is located [[http://osg.ivdgl.org/twiki/bin/view/Provisioning/LocalStorageRequirements][here]].

---+++ Check the Environment

The first step is to check the environment, and run a test of your job through to make sure it has all the pieces you need.  In theory, the OSG offers a common operating system (Scientific Linux 3) and a common platform on which to run your jobs.  In practice, each site may be wildly different, and you may encounter unexpected difficulties as you bounce from one site to another.  It would be nice if each site had some fact sheet listing what it was capable of and what it was not, but this appears to only be tangentially available.  You will have to submit test jobs and check for yourself.

It is possible that certain functions you may rely upon may not be in the path.  Functions like hostname may not be located in /bin/.  Common functions that your scripts depend upon may not be there.  Assume that when your job fails the first five times that each time is due to a different problem with something that you&#39;ve left out and attempt to rectify it.  


---++ A Guide to Surviving Your Mistakes

Errors happen.  A lot.  The OSG has about as many errors as Windows, far less documentation, and is about as easy to screw up.  Generally there are several types of errors.  There are authentication errors (site cannot authenticate), startup errors (your job cannot start), and errors inside the jobs.  It is really hard to diagnose errors that happen within the job because it is hard to tell who is at fault, and the logs often return incomplete.  My advice is to blame them on your users.

---+++  I Can&#39;t Get In!

---+++++ Globus-job-run Returns Errors

&lt;verbatim&gt;
bash-2.05b$ globus-job-run atlas.bu.edu/jobmanager-fork /bin/hostname 
GRAM Job submission failed because authentication with the remote server failed (error code 7)
&lt;/verbatim&gt;

This means that you can&#39;t get in.  Seriously, this is the catch all error for anyone who fails the authentication procedure.  Chances are that the site admin is just as clueless as you are.  If you think you might be able to get into the site, file a GOC ticket and go out for beer.  It won&#39;t be resolved before you can get back, even if you get drunk and Shanghied to Tijuana.

&lt;verbatim&gt;
bash-2.05b$ globus-job-run osg-login.lonestar.tacc.utexas.edu/jobmanager-fork /bin/hostname 
GRAM Job submission failed because the gatekeeper failed to run the job manager (error code 47)
&lt;/verbatim&gt;

This is a rather confusing error in that I suspect it may have vastly different multiple meanings.  Traditionally it means that your user account does not exist.  Once the authentication procedure is complete, you start running as the user linked to your VO.  If that user does not exist, then this is the error you get and nothing will run.


---+++  My Jobs are Held

There is no error state in Condor.  There is only a Held state (H).  If your jobs as displayed in condor_q go from state Idle (I) to H, then you&#39;ve got a problem.  By running condor_q -l &lt;clusterID&gt; you can see the displayed HoldReason.  There are a great many reasons why your job may be held.  None of them are pretty.

---+++++ GAHP

Errors having to do with GAHP tend to have something to do with your condor configuration.


The following errors seem to occur when running Globus Toolkit 4 (gt4):

&lt;verbatim&gt;
HoldReason = &quot;Failed to initialize GAHP&quot;
&lt;/verbatim&gt;

This usually means that GAHP is missing something necessary, such as Java, etc.  The easiest way to debug this is to run 

&lt;verbatim&gt;
&lt;condor&gt;/sbin/gt4_gahp
&lt;/verbatim&gt;

yourself.  This will usually tell you which environment variable is unset.


---++ Guide to Effective Panic

All right, so you&#39;ve irretrievably broken the system, either your system or the OSG site.  It is now officially time to panic, or, depending on the site you broke, change your name and move to Brazil.  Either way will work.  However, before you buy plane tickets, there are a few ways in which you might be able to fix your problem.

---+++ Authentication Trouble

Authentication problems are the most common problems on the grid, and it&#39;s hard to tell whose fault they are.  There are two different certificates that have to be matched; you have one, the site has the other.  Of course if these two are different, the authentication procedure will fail.

If you are a new VO, chances are that the problem is on the site side and that the site has not added you.  This is a good time to open a GOC ticket.  However, you should also check your own certificates.  Make sure that your certificates are up to date. You should especially check if you suddenly lose access to a site that you had access to before.

For a site you haven&#39;t run on before, you may want to check the Site Policy to see if your VO actually has permissions to run on that site.  Unfortunately, many sites do not have a Site Policy that covers this.  Open a GOC ticket for that one.


---+++ The Grid Operations Center

Because of the decentralized nature of the Grid, there is generally only one way to communicate any complaints and questions that you might have with the site admins or other concerned personnel.  That involves the GOC, through opening a ticket.  To open a ticket, simply send an email to goc@opensciencegrid.org, where it will be read by a real human being (as of this time it is not automated).  The GOC maintins lists of everyone who should be responsible for a ticket that you open, although they are only human and may send your request to the wrong list.

At the moment, simply because there is no better way, everything results in a GOC ticket.  If you can&#39;t find the Site Policy, open a GOC ticket.  If you can&#39;t get in and think that you should be able to, open a GOC ticket.  If you can&#39;t get in and don&#39;t know if you should be able to, open a GOC ticket.  If your jobs don&#39;t run properly, open a GOC ticket.  If you can&#39;t find your car keys, open a GOC ticket.  Because this is essentially your only point of contact, this is how you handle all of your problems.

GOC tickets are generally pointed at two different destinations, the site at which you encounter the problem, or the VO with which you encounter a problem.  Each VO has a VO support center which should handle basic user issues (primarily due to trying to replicate something done by other users).  The VO Support Center has a great deal of know-how and other such practical information that can usually resolve your problems.

However, chances are that if you are reading this document that you may be the VO Support Center, in which case most of your interactions will be with various site VOs.  Site VOs are generally nice people who want you to run on the OSG.  That aside, remember that at the moment there is no requirement for them to answer a GOC ticket, so try not to be rude.  The GOC is good about checking after two weeks or so to see if your ticket has been resolved, but there is no enforcement mechanism.  Don&#39;t piss people off.  It&#39;s not productive.


---++ Sites

Status:
   * Red: Cannot submit
   * Yellow: Can submit but glide-ins cannot run
   * Green : Works

| *Site* | *Gatekeeper* | *Status* | *Comments* |
|BU_ATLAS_Tier2 | atlas.bu.edu | Red | ATLAS site, cannot run until upgraded to 0.4.1 |
|UTA-DPCC | atlas.dpcc.uta.edu | Red | ATLAS site, cannot run until upgraded to 0.4.1 |

---++ Glossary

| *Term* | *Definition* |
| CE | Compute Element: An element capable of running a user job |
| GOC | Grid Operations Center: The help and support center for the entire OSG.  You&#39;ll be hearing from them a lot|
| ITB | Trash/Trash/Integration Test Bed: A site for testing future OSG releases |
| Matchmaking | The process of matching a job to a slot while maintaining site priorities |
| SE | Storage Element: Essentially a big block of disk space |
| VO | Virtual Organization: The group that &quot;buys in&quot; to the OSG, generally for a large project.  Every user runs as a member of a virtual organization, who issues their certificate |
| WN | Worker Node: The hardware that you run on |

&lt;!--PDFSTOP--&gt;

-- Main.MatthewNorman - 16 May 2006
-- Main.FkW - 23 May 2006

      * Set GENPDFADDON_TITLE = Frank&#39;s Guide to the OSG Perplexed
      * Set GENPDFADDON_SUBTITLE = %TOPIC% &amp;#8212; %REVINFO%
      * Set GENPDFADDON_KEYWORDS = %FORMFIELD{&quot;OSG, scientists, running grid jobs&quot;}%
      * Set GENPDFADDON_HEADERTOPIC = Sandbox.GenPDFHeaderFooterTopic
      * Set GENPDFADDON_TITLETOPIC = Sandbox.GenPDFTitleTopic
      * Set GENPDFADDON_LOGOIMAGE = http://www.opensciencegrid.org/images/logos/osg_logo_gs.gif

--&gt;
-- Main.ForrestChristian - 15 Sep 2006

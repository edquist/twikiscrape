<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en_US" lang="en_US">
<head>
<link rel="stylesheet" href="https://twiki.opensciencegrid.org/twiki/pub/TWiki/HeadlinesPlugin/style.css" type="text/css" media="all" />
<title> FakePage &lt; Sandbox &lt; TWiki    </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="icon" href="/twiki/pub/Sandbox/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="shortcut icon" href="/twiki/pub/Sandbox/WebPreferences/favicon.ico    " type="image/x-icon" />
<link rel="alternate" href="https://twiki.opensciencegrid.org/bin/edit/Sandbox/FakePage?_T=16 Feb 2017" type="application/x-wiki" title="edit FakePage" />
<meta name="SCRIPTURLPATH" content="/bin" />
<meta name="SCRIPTSUFFIX" content="" />
<meta name="TEXT_JUMP" content="Jump" />
<meta name="TEXT_SEARCH" content="Search" />
<meta name="TEXT_NUM_TOPICS" content="Number of topics:" />
<meta name="TEXT_MODIFY_SEARCH" content="Modify search" />
<meta name="robots" content="noindex" /><meta name="robots" content="noindex,nofollow"> 
<base href="https://twiki.opensciencegrid.org/bin/view/Sandbox/FakePage"></base>
<!--BEHAVIOURCONTRIB--><script type="text/javascript" src="/twiki/pub/TWiki/BehaviourContrib/behaviour.compressed.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikilib.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiWindow.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiEvent.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiHTML.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiCSS.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiForm.js"></script>
<script type="text/javascript" src="/twiki/pub/TWiki/PatternSkin/pattern.js"></script><style type="text/css" media="all">
@import url('/twiki/pub/TWiki/TWikiTemplates/base.css');
</style><script type="text/javascript" src="/twiki/pub/TWiki/TWikiJavascripts/twikiStyles.js"></script><style type="text/css" media="all">


</style>
<style type="text/css" media="all">
@import url("/twiki/pub/TWiki/TWikiNetSkin/layout.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/style.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/colors.css");
@import url("/twiki/pub/TWiki/TWikiNetSkin/rounded_corners.css");
</style>
<style type="text/css" media="all">
	/* Styles that are set using variables */
	#patternLeftBar .patternWebIndicator,
	.patternBookView .twikiTopRow {
		background-color:#DDDDDD;
	}
	.patternBookView {
		border-color:#DDDDDD;
	}
	.patternPreviewPage #patternMain {
		/* uncomment to set the preview image */
		/*background-image:url("/twiki/pub/TWiki/PreviewBackground/preview2bg.gif    ");*/
	}
	
</style><style type="text/css" media="all">



</style>
<style type="text/css" media="all">
	@import url("/twiki/pub/TWiki/TWikiNetSkin/print.css");
</style><!--GOOGLEANALYTICSPLUGIN--><!-- Google Analytics script -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-69012-21']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body class="patternViewPage patternPrintPage">
<a name="PageTop"></a>
<div id="patternScreen">
<div id="patternPageShadow">
<div id="patternPage">
<div id="patternOuter">
<div id="patternFloatWrap">
<div id="patternMain">
<div id="patternMainContents">
<div class="patternContent"><div class="patternTopic"> <h1><a name="Fake_Page"></a>  <strong><noop>Fake Page</strong> </h1>
<p />
<div class="twikiToc"> <ul>
<li> <a href="?cover=print#Installing_HDFS"> Installing HDFS</a> <ul>
<li> <a href="?cover=print#General_Prerequisites"> General Prerequisites</a>
</li> <li> <a href="?cover=print#RPM_installation"> RPM installation</a> <ul>
<li> <a href="?cover=print#Quick_Install"> Quick Install</a>
</li> <li> <a href="?cover=print#Full_Install"> Full Install</a>
</li> <li> <a href="?cover=print#Installing_with_yum"> Installing with yum</a>
</li></ul> 
</li></ul> 
</li> <li> <a href="?cover=print#Configuring_HDFS"> Configuring HDFS</a> <ul>
<li> <a href="?cover=print#HDFS_Directory_Locations"> HDFS Directory Locations</a>
</li> <li> <a href="?cover=print#Edit_etc_sysconfig_hadoop"> Edit /etc/sysconfig/hadoop</a> <ul>
<li> <a href="?cover=print#Side_topic_Multiple_data_directo"> Side topic: Multiple data directories on a datanode.</a>
</li></ul> 
</li> <li> <a href="?cover=print#Running_Hadoop"> Running Hadoop</a> <ul>
<li> <a href="?cover=print#Side_topic_Client_only_installat"> Side topic: Client-only installation</a>
</li></ul> 
</li> <li> <a href="?cover=print#Mounting_fuse_at_boot_time"> Mounting fuse at boot time</a>
</li></ul> 
</li> <li> <a href="?cover=print#Next_steps"> Next steps</a>
</li></ul> 
</div>
<p />
<h1><a name="Installing_HDFS"></a> Installing HDFS </h1>
<p />
This guide covers installation of the HDFS core components, along with the FUSE mounts.  The current version of HDFS covered in this guide is 0.20.2.
<p />
Once done with this guide, you should have Hadoop installed, configured, and working.  You should be able to navigate the file system through the FUSE mount point.  The next two guides cover the installation of grid components.
<p />
<p />
Conventions used in this document:
<p />
<p />
<font color="#808080">A <i>User Command Line</i> is illustrated by a green box that displays a prompt:</font>
<p />
<pre class="screen">
  [user@client ~]$
</pre>
<p />
<font color="#808080">A <i>Root Command Line</i> is illustrated by a red box that displays the <em>root</em> prompt:</font>
<p />
<pre class="rootscreen">
  [root@client ~]$
</pre>
<p />
<font color="#808080"><i>Lines in a file</i> are illustrated by a yellow box that displays the desired lines in a file:</font>
<pre class="file">
priorities=1
</pre>
<p />
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="General_Prerequisites"></a> General Prerequisites </span></h2>
<p />
Hadoop will run anywhere that Java is supported (including Solaris).  However, these instructions are for <span class="twikiNewLink">RedHat<a href="/bin/edit/Sandbox/RedHat?topicparent=Sandbox.FakePage" rel="nofollow" title="RedHat (this topic does not yet exist; you can create it)">?</a></span> 5 derivants (including Scientific Linux) because of the RPM based installation.  There are RPMs available for 64-bit systems only.
<p />
The HDFS prerequisites are: <ul>
<li> Minimum of 1 headnode (the namenode), although 2 recommended (the namenode and the secondary namenode)
</li> <li> At least one node which will hold data, preferably at least 2.  Most sites will have 20 to 200 datanodes.
</li> <li> The namenode and secondary name node are <strong>not</strong> datanodes.
</li> <li> Working Yum and RPM installation on every system.
</li> <li> Java RPM installed.  This requires the "jdk" RPM available at java.sun.com; Java 1.6.0 or higher is needed; patch level 14 is recommended. <ul>
<li> Java <strong>MUST</strong> be installed prior to following the rest of this document.
</li></ul> 
</li></ul> 
<p />
<strong>Compatibility Note</strong> Note that versions of OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux OpenAFS client, make sure you're running at least OpenAFS 1.4.7.
<p />
<strong>Deprecation Note</strong> There used to be two choices for installation method - RPM-based or Pacman-based.  From feedback we have received from site admins, we are moving forward only with the RPM-based installs.  <span class="twikiNewLink">HadoopInstallDeprecated<a href="/bin/edit/Sandbox/ThePacmanInstallIsDocumentedOnlyForPosterity?topicparent=Sandbox.FakePage" rel="nofollow" title="HadoopInstallDeprecated (this topic does not yet exist; you can create it)">?</a></span>.
<p />
<strong>Note</strong>: The rpm/yum installation will create a 'hadoop' system account and group  (uid,gid &lt; 500) on the host system for running the datanode services.  If you would like to control the uid/gid that is used, then you should create the 'hadoop' user and group manually before installing the rpms.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="RPM_installation"></a> RPM installation </span></h2>
<p />
<h3><a name="Quick_Install"></a> Quick Install </h3>
<p />
Quickstart for the impatient (without fuse).  Follow the following steps on your namenode, secondary namenode, and all the data nodes.
<p />
This assumes you already have the <strong>jdk</strong> 1.6.0 RPM installed on all relevant nodes.
<p />
 <pre class="rootscreen">
rpm -ivh http://newman.ultralight.org/repos/hadoop20/osg-hadoop20-1-2.el5.noarch.rpm
yum install hadoop-0.20-osg
vi /etc/sysconfig/hadoop # Edit appropriately (see below)
service hadoop-firstboot start
chkconfig hadoop on
service hadoop start
</pre>
<p />
<h3><a name="Full_Install"></a> Full Install </h3>
<p />
The Hadoop RPMs require Sun Java jdk 1.6.0 or later.  You can download this from <a href="http://java.sun.com" target="_top">http://java.sun.com</a>.
<p />
The Hadoop init script assumes that you are not running multiple hadoop services (datanode, namenode, secondary namenode) on the same host.
<p />
The fuse interface to HDFS requires the <code>fuse</code> kernel module.  The stock RHEL kernels include the fuse kernel module as of RHEL5.4.  If you are running a custom kernel, then be sure to enable the fuse module with <code>CONFIG_FUSE_FS=m</code>.  Building and installing a fuse kernel module for your custom kernel is beyond the scope of this document.
<p />
<strong>Note:</strong> If you cannot find a fuse kernel module to match your kernel, ATRPMs has a <a href="http://people.atrpms.net/~pcavalcanti/LCG_kernel_modules.html" target="_top">guide for using their RPM spec files</a> in order to generate a module.  That page mostly works, although sections are a bit out dated.  Contact the <a href="mailto&#58;osg&#45;hadoop&#64;opensciencegrid&#46;org">osg-hadoop&#64;opensciencegrid.org</a> list if you need help.
<p />
<h3><a name="Installing_with_yum"></a> Installing with yum </h3>
<p />
A yum repository for installing and upgrading Hadoop is hosted at the VDT.
<p />
To configure your local installation for the yum repository, you should install the osg-hadoop package with the following command:
<p />
<pre class="rootscreen">
[root@client ~]$ rpm -ihv http://vdt.cs.wisc.edu/hadoop/osg-hadoop20-1-2.el5.noarch.rpm
</pre>
<p />
After installing the yum configuration package, you can install the hadoop core with:
<p />
<pre class="rootscreen">
[root@client ~]$ yum install hadoop-0.20-osg
</pre>
<p />
Next, we install the FUSE-related portions:
<p />
<pre class="rootscreen">
[root@client ~]$ yum install hadoop-0.20-fuse
</pre>
<p />
<h1><a name="Configuring_HDFS"></a> Configuring HDFS </h1>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="HDFS_Directory_Locations"></a> HDFS Directory Locations </span></h2>
<p />
The Hadoop RPMs install files into the standard system locations.  The following table highlights some of the more interesting locations, and documents whether you might ever want to edit them.
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table1" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> File Type </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Location </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Needs editing? </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> Log files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> <code>/var/log/hadoop/*</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> PID files </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> <code>/var/run/hadoop/*.pid</code> </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> init scripts </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> <code>/etc/init.d/hadoop</code>, <code>/etc/init.d/hadoop-firstboot</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> init script config file </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> <code>/etc/sysconfig/hadoop</code> </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Yes </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> runtime config files </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> <code>/etc/hadoop/conf/*</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Maybe </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> System binaries </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> <code>/usr/bin/hadoop</code> </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> No </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> JARs </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1 twikiLast"> <code>/usr/lib/hadoop/*</code> </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> No </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Edit_etc_sysconfig_hadoop"></a> Edit /etc/sysconfig/hadoop </span></h2>
<p />
The most common site configuration settings can be changed in <code>/etc/sysconfig/hadoop</code>.  In most cases, this file will be identical on the namenode and datanodes.  The configuration settings are documented in the file itself, but we document some of the most commonly edited ones in the table below:
<p />
<table cellspacing="0" cellpadding="0" border="0" class="twikinetWrapperTable" rules="none">
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableT twikinetWrapperTableTL"></td>
<td class="twikinetWrapperTableT twikinetWrapperTableTR"></td>
</tr>
<tr class="twikinetWrapperTableRow">
<td colspan="2" class="twikinetWrapperTableMain">
<table cellspacing="0" id="table2" cellpadding="0" class="twikiTable" rules="cols" border="1">
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> Option Name </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Needs editing? </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Suggested value </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_NAMENODE </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The host name of your namenode; should match the output 'hostname -s' on the namenode server </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_NAMEPORT </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> 9000 </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_SECONDARY_NAMENODE </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The host name of the secondary namenode; should match the output of 'hostname -s' </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_CHECKPOINT_DIRS </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Comma-separated (<strong>important:</strong> no spaces between commas!) list of directories to store checkpoints on.  The safest configuration is to store 2 checkpoints locally on 2 block devices and 1 checkpoint on a NFS server.  At least 1 checkpoint directory is required. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_CHECKPOINT_PERIOD </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The time, in seconds, between checkpoints.  600 is suggested for small sites </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_REPLICATION_DEFAULT </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Default number of replications.  Suggested: 2 </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_REPLICATION_MIN </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Minimum number of replications; below this, an error will be thrown.  Suggested: 1 or 2. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_REPLICATION_MAX </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> Maximum number of replications.  Suggested: 512 </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_GANGLIA_ADDRESS </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> Hostname or IP of your Ganglia gmetad.  If left empty then hadoop will try to extract the ganglia metad address from /etc/gmond.conf </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_DATADIR </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> The base directory where HDFS temp and management data will be written.  On datanodes this is usually the parent of the first data partition. It is safe to leave this empty for client-only installations. </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_DATA </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Yes </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> A comma-separated list of directories (no spaces!) where the HDFS data blocks will be stored.  The first one is typicall the same as $HADOOP_DATADIR/data.  It is safe to leave this empty for client-only installations. </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_USER </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol"> The username that the hadoop datanode daemons will run under.  The namenode will always run as 'root'.  Suggested: hadoop </td>
		</tr>
		<tr class="twikiTableEven twikiTableRowdataBgSorted1 twikiTableRowdataBg1">
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol0 twikiFirstCol"> HADOOP_NAMENODE_HEAP </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol1"> Maybe </td>
			<td bgcolor="#f2f3f6" valign="top" class="twikiTableCol2 twikiLastCol"> The Java heap size for the namenode; bigger is better, but the node shouldn't swap.  Minimum: 2048m.  Suggested: 8192m </td>
		</tr>
		<tr class="twikiTableOdd twikiTableRowdataBgSorted0 twikiTableRowdataBg0">
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol0 twikiFirstCol twikiLast"> HADOOP_MIN_DATANODE_SIZE </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol1 twikiLast"> Maybe </td>
			<td bgcolor="#ffffff" valign="top" class="twikiTableCol2 twikiLastCol twikiLast"> A value in GB; if the data directory is smaller than this size, HDFS will refuse to start.  Safeguards against starting the datanode daemon on non-datanodes.  Suggested: 300 (this value will vary widely with your datanode size). Set to zero or an empty string to bypass this check. </td>
		</tr></table>
</td>
</tr>
<tr class="twikinetWrapperTableRow">
<td class="twikinetWrapperTableB twikinetWrapperTableBL"></td>
<td class="twikinetWrapperTableB twikinetWrapperTableBR"></td>
</tr>
</table>
<p />
After making changes to the file, you must run <code>service hadoop-firstboot start</code> to propagate the changes to the hadoop configuration files in <code>/etc/hadoop</code>.  <code>hadoop-firstboot</code> must be run every time you make changes to /etc/sysconfig/hadoop.
<p />
<strong>NOTE:</strong> If you just installed Hadoop for the first time, you must log in/out of your shell or source /etc/profile.d/hadoop.sh before your you try playing with the command line tools.
<p />
<strong>Upgrade note:</strong> Configuration files will be saved with a <code>.rpmsave</code> extension if you ever update your hadoop rpms with rpm or yum.  <strong>Make sure to copy your settings from <code>/etc/sysconfig/hadoop.rpmsave</code> to <code>/etc/sysconfig/hadoop</code> if you ever update your hadoop rpms.</strong>  Any manual changes to the hadoop configuration files in <code>/etc/hadoop/</code> should be preserved during an upgrade, but may be overwritten when running <code>hadoop-firstboot</code>.
<p />
<h3><a name="Side_topic_Multiple_data_directo"></a> Side topic: Multiple data directories on a datanode. </h3>
<p />
Hadoop has the ability to store data in multiple directories on a datanode.  This can be useful if you have multiple drives on your datanode and don't want to run them in a raid array, or if you have multiple large storage volumes mounted on your datanode.  To configure a datanode to use multiple directories, you need to enter each directory in the <code>HADOOP_DATA</code> setting in <code>/etc/sysconfig/hadoop</code> as a comma-separated list of directories (no spaces!) and then run <code>service hadoop-firstboot start</code>.  Here is an example of a datanode with 4 storage directories:
<p />
<pre class="file">
HADOOP_DATA=/data1/hadoop/data,/data2/hadoop/data,/data3/hadoop/data,/data4/hadoop/data
</pre>
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Running_Hadoop"></a> Running Hadoop </span></h2>
<p />
The Hadoop rpms install a startup script in <code>/etc/init.d/hadoop</code>.  The same command is used to start hadoop services on a datanode, namenode, or secondary namenode:
<p />
<pre class="rootscreen">
[root@client ~]$ service hadoop start
</pre>
<p />
You will also want to configure hadoop to start at boot time with:
<p />
<pre class="rootscreen">
[root@client ~]$  chkconfig hadoop on
</pre>
<p />
<h3><a name="Side_topic_Client_only_installat"></a> Side topic: Client-only installation </h3>
<p />
Sometimes it is handy to configure a node to be a client, that is, a system that has access to hadoop but will not serve as a datanode or namenode.  The installation and configuration for such a node is the same as above, except that you do not need to start any hadoop services with <code>/etc/init.d/hadoop</code>.  It is still necessary to modify <code>/etc/sysconfig/hadoop</code>, but it is not necessary to specify any datanode directories in <code>HADOOP_DATA</code>.
<p />
<h2 class="twikinetRoundedAttachments"><span class="twikinetHeader"><a name="Mounting_fuse_at_boot_time"></a> Mounting fuse at boot time </span></h2>
<p />
After you have installed the fuse-libs rpms as well as the fuse kernel module (preferably via the fuse RPM), you can mount FUSE by adding the following line to <code>/etc/fstab</code> (Be sure to change the <code>/mnt/hadoop</code> mount point and <code>namenode.host</code> to match your local configuration.  To match the help documents, we recommend using <code>/mnt/hadoop</code> as your mountpoint):
<p />
<pre class="file">
hadoop-fuse-dfs# /mnt/hadoop fuse server=namenode.host,port=9000,rdbuffer=32768,allow_other 0 0
</pre>
<p />
Then run:
<p />
<pre class="rootscreen">
[root@client ~]$ mount /mnt/hadoop
</pre>
<p />
When mounting the HDFS fuse mount, you will see the following harmless warnings printed to the screen:
<p />
<pre class="rootscreen">
# mount /mnt/hadoop
port=32767,server=(
fuse-dfs didn't recognize /mnt/hadoop,-2
fuse-dfs ignoring option allow_other
</pre>
<p />
To start the fuse mount in debug mode, you can run the fuse mount command by hand:
<p />
<pre class="rootscreen">
[root@client ~]$  /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server=compute-13-1,port=9000,rdbuffer=131072,allow_other -d
</pre>
<p />
Debug output will be printed to stderr, which you will probably want to redirect to a file.  Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.
<p />
<h1><a name="Next_steps"></a> Next steps </h1>
<p />
Congratulations!  At this point, you should have a working Hadoop installation.  Please proceed to the validation steps or the next guide, <span class="twikiNewLink">Hadoop and GridFTP<a href="/bin/edit/Sandbox/Hadoop20GridFTP?topicparent=Sandbox.FakePage" rel="nofollow" title="Hadoop and GridFTP (this topic does not yet exist; you can create it)">?</a></span>.
<p />
-- <a href="/bin/view/Main/JeffDost" class="twikiLink">JeffDost</a> - 13 Apr 2011
<p />
<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   <code><b>===============</b></code>
<p />
   Thank you for claiming ownership for this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Sandbox/FirstLast?topicparent=Sandbox.FakePage" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local OWNER          = <span class="twikiNewLink">JeffDost<a href="/bin/edit/Sandbox/JeffDost?topicparent=Sandbox.FakePage" rel="nofollow" title="JeffDost (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (<span class="twikiNewLink">ComputeElement<a href="/bin/edit/Sandbox/ComputeElement?topicparent=Sandbox.FakePage" rel="nofollow" title="ComputeElement (this topic does not yet exist; you can create it)">?</a></span>|General|Trash/Trash/Integration|Monitoring|Operations|Security|Storage|Trash/Tier3|User|VO) <ul>
<li> Local DOC_AREA       =  Storage
</li></ul> 
<p />
   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Developer|Documenter|Scientist|Student|SysAdmin|VOManager) <ul>
<li> Local DOC_ROLE       = <span class="twikiNewLink">SysAdmin<a href="/bin/edit/Sandbox/SysAdmin?topicparent=Sandbox.FakePage" rel="nofollow" title="SysAdmin (this topic does not yet exist; you can create it)">?</a></span>
</li></ul> 
<p />
   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (<span class="twikiNewLink">HowTo<a href="/bin/edit/Sandbox/HowTo?topicparent=Sandbox.FakePage" rel="nofollow" title="HowTo (this topic does not yet exist; you can create it)">?</a></span>|Installation|Knowledge|Navigation|Planning|Training|Troubleshooting) <ul>
<li> Local DOC_TYPE       = Installation
</li></ul> 
<p />
   Please define if this document in general needs to be reviewed before release ( 1 | 0 ) <ul>
<li> Local INCLUDE_REVIEW = 1
</li></ul> 
<p />
   Please define if this document in general needs to be tested before release ( 1 | 0 ) <ul>
<li> Local INCLUDE_TEST   = 1
</li></ul> 
<p />
   change to 1 once the document is ready to be reviewed and back to 0 if that is not the case <ul>
<li> Local REVIEW_READY   = 1
</li></ul> 
<p />
   change to 1 once the document is ready to be tested and back to 0 if that is not the case <ul>
<li> Local TEST_READY     = 0
</li></ul> 
<p />
   change to 1 only if the document has passed the review and the test (if applicable) and is ready for release <ul>
<li> Local RELEASE_READY  = 0
</li></ul> 
<p />
<p />
   DEAR DOCUMENT REVIEWER
   <code><b>==================</b></code>
<p />
   Thank for reviewing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Sandbox/FirstLast?topicparent=Sandbox.FakePage" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local REVIEWER       = 
</li></ul> 
<p />
   Please define the review status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 ) <ul>
<li> Local REVIEW_PASSED  = 2
</li></ul> 
<p />
<p />
   DEAR DOCUMENT TESTER
   <code><b>================</b></code>
<p />
   Thank for testing this document! Please fill in your <span class="twikiNewLink">FirstLast<a href="/bin/edit/Sandbox/FirstLast?topicparent=Sandbox.FakePage" rel="nofollow" title="FirstLast (this topic does not yet exist; you can create it)">?</a></span> name here: <ul>
<li> Local TESTER         = 
</li></ul> 
<p />
   Please define the test status for this document to be in progress ( 2 ), failed ( 0 ) or passed ( 1 ) <ul>
<li> Local TEST_PASSED    = 2
</li></ul> 
############################################################################################################
--></div><!-- /patternTopic-->
<p />
<p />
</div><!-- /patternContent-->
<hr />
This topic: Sandbox<span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span><a href="/bin/view/Sandbox/JeffDostSandbox" class="twikiLink">JeffDostSandbox</a><span class='twikiSeparator'>&nbsp;&gt;&nbsp;</span>FakePage</span> <br />    
Topic revision: r10 - 06 Dec 2016 - 18:13:13 - <a href="/bin/view/Main/KyleGross" class="twikiLink">KyleGross</a>
</div><!-- /patternMainContents-->
</div><!-- /patternMain-->
</div><!-- /patternFloatWrap-->
<div class="clear">&nbsp;</div>
</div><!-- /patternOuter--><div id="patternBottomBar"><div id="patternBottomBarContents"><div id="twikinetBadge"><a href="http://www.twiki.net/"><img src="https://twiki.opensciencegrid.org/twiki/pub/TWiki/TWikiNetSkin/twiki-badge-88x31.gif" alt="TWIKI.NET" width="88" height="31" border="0" /></a></div><!--/twikinetBadge--><div id="patternWebBottomBar"><p>
<font size="-1">
TWiki |
<a href="https://ticket.grid.iu.edu/goc/twiki">Report Bugs</a> |
<a href="https://twiki.grid.iu.edu/bin/view/Operations/IUPrivacyPolicy">Privacy Policy</a>
</p>
<p>
<font size="-2">
<span class="twikiRight"> <a href="http://twiki.org/"><img src="/twiki/pub/TWiki/TWikiLogos/T-logo-80x15.gif" alt="This site is powered by the TWiki collaboration platform" width="80" height="15" title="This site is powered by the TWiki collaboration platform" border="0" /></a></span>Copyright by the contributing authors. All material on this collaboration platform is the property of the contributing authors..
</font>
</p></div><!--/patternWebBottomBar--></div><!-- /patternBottomBarContents--></div><!-- /patternBottomBar-->
</div><!-- /patternPage-->
</div><!-- /patternPageShadow-->
</div><!-- /patternScreen-->
</body></html>
<p />
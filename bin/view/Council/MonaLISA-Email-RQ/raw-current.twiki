---++ Rob Quick&#39;s Email (March 13th, 2008 at 7:15AM Pacific)

Subject: Re: [Fwd: [Fwd: RE: OSG MonALISA Turn Down - GOC Ticket # 4740]]

Date: Thu, 13 Mar 2008 10:15:33 -0400

From: Rob Quick &lt;rquick@iupui.edu&gt;

To: Ruth &lt;ruth@fnal.gov&gt;

CC: Harvey Newman &lt;Harvey.Newman@cern.ch&gt;, Frank Wuerthwein &lt;fkw@fnal.gov&gt;, 
 Paul Avery &lt;avery@phys.ufl.edu&gt;, Iosif Legrand &lt;iosif.legrand@cern.ch&gt;, &quot;McKee, Shawn&quot; &lt;smckee@umich.edu&gt;, 
Joel Butler &lt;butler@fnal.gov&gt;, Howard Gordon &lt;gordon@bnl.gov&gt;, Richard Cavanaugh &lt;Richard.Cavanaugh@cern.ch&gt;, &quot;osg-eb@OPENSCIENCEGRID.ORG&quot; &lt;osg-eb@OPENSCIENCEGRID.ORG&gt;

References: &lt;47D84C95.5040007@cern.ch&gt; &lt;47D84E47.3090904@fnal.gov&gt;

Ruth,

Yes, I will send a stay of the MonALISA turn off date dependent on the  
Council&#39;s decision.

Let me touch on a few points, there are 81 OSG compute elements and 13  
storage elements for a total of 94 registered resources on the OSG.  
MonALISA is only running on CEs, so I won&#39;t include the SEs in the  
next few calculations. 31 resources are reporting to MonALISA as of  
last night, I do not recognize 4 of these resources and 5 reported no  
informaiton. This leaves 22 of 81 resources or ~27%. This graph shows  
DB size and a huge drop off in usage after the 0.6.0 release in March  
of 2007. (http://www.grid.iu.edu/reports/ML_Jan07-Mar08.png)

The effort involved includes maintenance, monitoring, and support of  
MonALISA at the client and server level (aka Trouble Tickets reporting  
problems, inaccuracies, outages, etc.). I agree that MonALISA has  
evolved into be a stable service. But other than a few select  
resources it is largely unused. I would suggest this is a VO level  
service, and not an OSG Operations infrastructure service. If we would  
like this to be an OSG service we need to think about making it a  
mandatory service so we can use it fully. I am not disputing  
MonALISA&#39;s usefulness as a monitoring tool, but until it is more  
widely used, it is not helpful on a OSG-wide operations level.

As I said, I will send a stay of the turn off date this morning  
pending council decision.

Rob

On Mar 12, 2008, at 5:42 PM, Ruth wrote:

&gt; Hi Harvey, Rob Quick
&gt;
&gt; This needs to go through the OSG Council for the longer term  
&gt; decision as I have explained before. It is a matter of effort and  
&gt; priority of course. I am definitely and equially sure we don&#39;t need  
&gt; a crisis right now.
&gt;
&gt; However, with this email I am asking Rob Quick to retract from the  
&gt; recent decision to turn off the current ML service at the OSG  
&gt; operations center. I am, with this email, asking him to  let me know  
&gt; the effort it takes to maintain this service as others ramp up.
&gt;
&gt; Rob Quick - will you send the email retracting the decision or would  
&gt; you like me to?
&gt;
&gt; thank you for the email
&gt;
&gt; Ruth
&gt;
&gt;
&gt;
&gt; Harvey Newman wrote:
&gt;&gt;
&gt;&gt;
&gt;&gt; Dear Ruth,
&gt;&gt;
&gt;&gt; I&#39;ve been informed of this secondhand, but because of the urgency  
&gt;&gt; and import,
&gt;&gt; I am letting you know as OSG Executive Director that the MonALISA  
&gt;&gt; turndown is,
&gt;&gt; as explained in this note, is a strategic mistake of rare  
&gt;&gt; proportions. Also the timing is quite
&gt;&gt; out of synch. with the ramp up to LHC running, and the associated  
&gt;&gt; network
&gt;&gt; developments for managing network resources.
&gt;&gt;
&gt;&gt; As you can see ATLAS is responding to the turn-down immediately, in  
&gt;&gt; recognition
&gt;&gt; of the obvious fact that there is no monitoring system of similar  
&gt;&gt; capability, or
&gt;&gt; indeed in this class. As LHC ramps us, a system with this reach and  
&gt;&gt; capability is
&gt;&gt; required to keep track of what is going on. What it does provide  
&gt;&gt; cannot be replaced
&gt;&gt; by simpler, and in a fundamental way far more basic tools.
&gt;&gt;
&gt;&gt; Unlike many other manual systems, ML can deal with issues  
&gt;&gt; autonomously,
&gt;&gt; or semi-autonomously, and problems do not generate reams of Emails  
&gt;&gt; from
&gt;&gt; users and system managers where the failure is, and what its nature  
&gt;&gt; is. As we&#39;ve
&gt;&gt; said many times, awareness of both end-systems and networks is a  
&gt;&gt; requirement, and
&gt;&gt; this requirement is increasing; not decreasing in any way.
&gt;&gt;
&gt;&gt; As you know, MonALISA underlies EVO, and you may also be aware
&gt;&gt; that ALICE uses it to provide them with a highly capable autonomous
&gt;&gt; infrastructure (and higher level services that they developed  
&gt;&gt; themselves
&gt;&gt; using its services) which supports computing operations
&gt;&gt; with much reduced manpower.
&gt;&gt;
&gt;&gt; The major networks are transitioning to a new dynamic circuit- 
&gt;&gt; oriented
&gt;&gt; paradigm to support the major network flows of the major science
&gt;&gt; programs, most notably the LHC. In the case of the LHC, it is obvious
&gt;&gt; that the LHC experiments will soon exhaust the affordable bandwidth
&gt;&gt; and so managed services are being built, following a plan exposed to
&gt;&gt; the community, including OSG, starting a few years ago. The  
&gt;&gt; infrastucture
&gt;&gt; to provide this has been installed, throughout the US and across the
&gt;&gt; Atlantic by Internet2 and US LHCNet respectively, within the past  
&gt;&gt; year.
&gt;&gt; A partnership of ESnet, GEANT2, the networks just mentioned,
&gt;&gt; Fermilab, BNL and leading NRENs has begun work on a de facto
&gt;&gt; standard way to build and provision inter-domain circuits.
&gt;&gt;
&gt;&gt; The services to be deployed on this infrastructure for LHC require  
&gt;&gt; an end-to-end
&gt;&gt; view and a true real-time monitoring. This is going to be based on  
&gt;&gt; MonALISA
&gt;&gt; as this is the one system of sufficient power and field proven  
&gt;&gt; stability (over
&gt;&gt; 7 years, around the clock) that exists. Applications, including OSG  
&gt;&gt; middleware
&gt;&gt; [sorry to repeat what I told the OSG management a few years ago]  
&gt;&gt; will have
&gt;&gt; to request network resources and communicate with network-resident  
&gt;&gt; services
&gt;&gt; that will manage and control the allocation of bandwidth. Projects  
&gt;&gt; funded by
&gt;&gt; NSF (UltraLight, PLaNetS) are working with US LHCNet on some of these
&gt;&gt; services as well.
&gt;&gt;
&gt;&gt; The APIs are starting to be developed. The communication will be  
&gt;&gt; real-time in that
&gt;&gt; the allocated network resources will be checked as being well-used,  
&gt;&gt; or not,
&gt;&gt; and resources not well-used will be freed to satisfy competing  
&gt;&gt; requests.
&gt;&gt; The configuration of the end-systems in terms of hardware and  
&gt;&gt; software as
&gt;&gt; well as state (load, traffic levels) will be used to diagnose  
&gt;&gt; throughput problems,
&gt;&gt; together with realtime diagnosis of the network itself, to  
&gt;&gt; demystify what is happening
&gt;&gt; to flows that do not perform in a way that matches (to some degree  
&gt;&gt; of accuracy)
&gt;&gt; what was requested.
&gt;&gt;
&gt;&gt; So OSG, using the ML infrastructure, at least had a start in this  
&gt;&gt; correct direction.
&gt;&gt; If it ML monitoring is turned down, then either much greater effort  
&gt;&gt; will have to be invested
&gt;&gt; by OSG to restore (as well as extend and expand on, as above) this  
&gt;&gt; capability, or OSG&#39;s
&gt;&gt; ability to distribute data will be correspondingly reduced, by a  
&gt;&gt; major factor,
&gt;&gt; as the competition for network resources increases with the arrival  
&gt;&gt; of real LHC
&gt;&gt; data. The reason is simply that as a strategic, scarce resource,  
&gt;&gt; Network resources
&gt;&gt; with a guaranteed bandwidth will be assigned to requests where the  
&gt;&gt; requesting application
&gt;&gt; can communicate with the network services, and which are configured  
&gt;&gt; to use the network
&gt;&gt; resources allocated to them at reasonably high efficiency.  
&gt;&gt; Otherwise requests will have
&gt;&gt; go to a catch-all &quot;best effort&quot; class to which a relatively small  
&gt;&gt; amount of bandwidth will
&gt;&gt; be assigned.
&gt;&gt;
&gt;&gt; On a related topic, the above has been presented in some detail at  
&gt;&gt; DOE
&gt;&gt; at the US LHCNet review in mid-January and today at NSF. In the
&gt;&gt; discussion that ensued today, it was clear that OSG needs to re- 
&gt;&gt; engage with
&gt;&gt; these network issues, through the network working group that was  
&gt;&gt; formed
&gt;&gt; a few years ago with the leadership of Don and Shawn, but was not  
&gt;&gt; funded
&gt;&gt; by OSG project management.
&gt;&gt;
&gt;&gt; Best regards
&gt;&gt; Harvey
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; Subject:
&gt;&gt; [Fwd: RE: OSG MonALISA Turn Down - GOC Ticket # 4740]
&gt;&gt; From:
&gt;&gt; Iosif Legrand &lt;Iosif.Legrand@cern.ch&gt;
&gt;&gt; Date:
&gt;&gt; Wed, 12 Mar 2008 21:00:03 +0100
&gt;&gt; To:
&gt;&gt; Harvey Newman &lt;Harvey.Newman@cern.ch&gt;
&gt;&gt; To:
&gt;&gt; Harvey Newman &lt;Harvey.Newman@cern.ch&gt;
&gt;&gt; CC:
&gt;&gt; Iosif Legrand &lt;Iosif.Legrand@cern.ch&gt;
&gt;&gt;
&gt;&gt; Hello Harvey,
&gt;&gt;
&gt;&gt;
&gt;&gt; I received the attached email from the USATLAS , and they want a ML
&gt;&gt; repository to replace the OSG site .
&gt;&gt; I think it will be useful for us to support this... and I hope to  
&gt;&gt; provide a
&gt;&gt; good example for the USCMS.
&gt;&gt; I think  CMS did a big mistake with  the CERN dashboard.
&gt;&gt; In fact the CMS dashboard is totally depended on ML for collecting  
&gt;&gt; all
&gt;&gt; the jobs related information.  As I told you, it is done in a very  
&gt;&gt; inefficient way and
&gt;&gt; is  using now only one  ML service running  at CERN.
&gt;&gt; This ML service collects ~ 200 000 parameters with an average update
&gt;&gt; rate of ~ 1500 parameters per second from all the  CMS jobs running  
&gt;&gt; on all CMS centers
&gt;&gt; This ML service works really fine for more than two years, but this  
&gt;&gt; is not an architecture to monitor the
&gt;&gt; jobs in CMS.
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; Best Regards,
&gt;&gt; ioji
&gt;&gt;
&gt;&gt;
&gt;&gt; -------- Original Message --------
&gt;&gt; Subject:     RE: OSG MonALISA Turn Down - GOC Ticket # 4740
&gt;&gt; Date:     Wed, 12 Mar 2008 13:53:14 -0500 (CDT)
&gt;&gt; From:     Horst Severini &lt;hs@nhn.ou.edu&gt;
&gt;&gt; Reply-To:     Horst Severini &lt;hs@nhn.ou.edu&gt;
&gt;&gt; To:     rwg@hep.uchicago.edu, packardj@rcf.rhic.bnl.gov, smckee@umich.edu
&gt;&gt; CC:     hs@nhn.ou.edu, aglt2-hardware@umich.edu, mernst@bnl.gov, Iosif.Legrand@cern.ch 
&gt;&gt; , Costin.Grigoras@cern.ch
&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt; Hi Ioji and Costin,
&gt;&gt;
&gt;&gt; we (USATLAS) were talking about this OSG ML Repository shutdown  
&gt;&gt; this morning,
&gt;&gt; and we&#39;d like to migrate/replicate the Repository to/at BNL, since  
&gt;&gt; we really like all the useful features of the Repository and don&#39;t  
&gt;&gt; want to lose it.
&gt;&gt;
&gt;&gt; How difficult would that be? Could you help us with the  
&gt;&gt; configuration?
&gt;&gt;
&gt;&gt; Thanks a lot,
&gt;&gt;
&gt;&gt;     Horst
&gt;&gt;
&gt;&gt;


-- Main.KentBlackburn - 01 Apr 2008
